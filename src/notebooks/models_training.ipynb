{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efac910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(\"..\", \"..\"))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8afc8e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "from src.utils.preprocessing_utils import load_dataset\n",
    "\n",
    "from src.prototypes.torch_apk_analysis_model import (\n",
    "    get_best_available_device,\n",
    "    cross_val_train_nn_model,\n",
    "    train_nn_model,\n",
    "    extract_embeddings,\n",
    "    evaluate_model_on_test_set,\n",
    "    NNHyperparams,\n",
    ")\n",
    "\n",
    "from src.prototypes.torch_apk_analysis_model_io import (\n",
    "    save_model_with_metadata,\n",
    "    load_apk_analysis_model_from_version,\n",
    "    load_apk_feature_embedder_from_version,\n",
    "    load_apk_analysis_model_metadata,\n",
    ")\n",
    "\n",
    "from src.prototypes.ml_model import MLHyperparams, train_classical_models_cv\n",
    "from src.prototypes.ml_model_io import save_ml_models_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425a7d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading last preprocessed dataset...\n",
      "Using CUDA device: NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_COLS = [\n",
    "    \"activities_list\",\n",
    "    \"services_list\",\n",
    "    \"receivers_list\",\n",
    "    \"permissions_list\",\n",
    "    \"api_calls_list\",\n",
    "]\n",
    "\n",
    "CHAR_COLS = [\"fuzzy_hash\"]\n",
    "VECTOR_COLS = [\"opcode_counts\"]\n",
    "SCALAR_COLS = [\"file_size\"]\n",
    "VECTOR_DIMS = {\"opcode_counts\": 768}\n",
    "\n",
    "PROJECT_ROOT = Path().cwd().parent.parent\n",
    "PATH_TO_DATASET_DIR = PROJECT_ROOT / \"dataset\"\n",
    "PATH_TO_SAVE_NN_MODEL = PROJECT_ROOT / \"model_artifacts\" / \"nn_models\"\n",
    "PATH_TO_SAVE_ML_MODEL = PROJECT_ROOT / \"model_artifacts\" / \"ml_models\"\n",
    "\n",
    "# Load dataset\n",
    "df, vocab_dict = load_dataset(\n",
    "    PATH_TO_DATASET_DIR,\n",
    "    SEQUENCE_COLS,\n",
    "    CHAR_COLS,\n",
    "    VECTOR_COLS,\n",
    "    SCALAR_COLS,\n",
    "    VECTOR_DIMS,\n",
    "    load_fresh=False,\n",
    "    sample_size=None,\n",
    ")\n",
    "\n",
    "df, df_test = train_test_split(\n",
    "    df, test_size=0.1, random_state=42, stratify=df[\"is_malware\"]\n",
    ")\n",
    "\n",
    "device = get_best_available_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84426bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17974 entries, 2247 to 5263\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   file_size         17974 non-null  int64 \n",
      " 1   fuzzy_hash        17974 non-null  object\n",
      " 2   activities_list   17974 non-null  object\n",
      " 3   services_list     17974 non-null  object\n",
      " 4   receivers_list    17974 non-null  object\n",
      " 5   permissions_list  17974 non-null  object\n",
      " 6   api_calls_list    17974 non-null  object\n",
      " 7   opcode_counts     17974 non-null  object\n",
      " 8   is_malware        17974 non-null  int64 \n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 1.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>fuzzy_hash</th>\n",
       "      <th>activities_list</th>\n",
       "      <th>services_list</th>\n",
       "      <th>receivers_list</th>\n",
       "      <th>permissions_list</th>\n",
       "      <th>api_calls_list</th>\n",
       "      <th>opcode_counts</th>\n",
       "      <th>is_malware</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2247</th>\n",
       "      <td>6909752</td>\n",
       "      <td>[37, 65, 42, 15, 29, 33, 35, 38, 11, 28, 62, 4...</td>\n",
       "      <td>[19048, 19079, 19053, 19081, 19052, 508, 45, 1...</td>\n",
       "      <td>[20, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[2204, 2200, 18, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[69, 27, 6, 25, 3, 19, 22, 61, 14, 9, 24, 1001...</td>\n",
       "      <td>[78, 122, 2465, 119, 106, 7158, 1202, 705, 383...</td>\n",
       "      <td>[0.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5698</th>\n",
       "      <td>9660703</td>\n",
       "      <td>[22, 8, 25, 21, 23, 25, 17, 56, 20, 45, 3, 10,...</td>\n",
       "      <td>[139512, 3799, 34, 139502, 139528, 18, 139522,...</td>\n",
       "      <td>[23, 12423, 12421, 12422, 30, 20, 6, 26, 0, 0,...</td>\n",
       "      <td>[9139, 5, 18, 3, 67, 17, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[15, 39, 11, 26, 78, 4, 2755, 5, 64, 6, 3, 9, ...</td>\n",
       "      <td>[105950, 63567, 187729, 3877, 3934, 3891, 7551...</td>\n",
       "      <td>[628.0, 7051.0, 4476.0, 0.0, 289.0, 550.0, 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381</th>\n",
       "      <td>7014287</td>\n",
       "      <td>[37, 14, 16, 40, 19, 33, 46, 52, 5, 61, 3, 51,...</td>\n",
       "      <td>[73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 8...</td>\n",
       "      <td>[129, 120, 104, 115, 108, 60, 102, 61, 96, 62,...</td>\n",
       "      <td>[62, 45, 55, 58, 54, 63, 57, 56, 53, 61, 49, 5...</td>\n",
       "      <td>[163, 43, 143, 70, 159, 84, 86, 118, 117, 4, 1...</td>\n",
       "      <td>[32224, 32071, 18370, 27218, 23182, 24664, 263...</td>\n",
       "      <td>[933.0, 10440.0, 690.0, 0.0, 304.0, 204.0, 0.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>5863124</td>\n",
       "      <td>[66, 29, 4, 42, 34, 45, 20, 38, 29, 66, 14, 15...</td>\n",
       "      <td>[37, 9, 308, 224756, 315, 224757, 12, 52, 7, 2...</td>\n",
       "      <td>[139, 32, 151, 146, 35, 55, 29, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[29, 156, 39, 31, 100, 26, 40, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[42, 22, 11, 19, 14, 6, 100, 15, 10, 46, 13, 2...</td>\n",
       "      <td>[78, 122, 2465, 119, 106, 7158, 1202, 705, 383...</td>\n",
       "      <td>[0.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18394</th>\n",
       "      <td>8197050</td>\n",
       "      <td>[60, 62, 12, 30, 37, 58, 13, 38, 48, 30, 3, 55...</td>\n",
       "      <td>[6, 10, 4, 586, 44, 3, 42, 41, 6293, 43, 555, ...</td>\n",
       "      <td>[9, 12, 8, 6, 13, 5, 10, 11, 3, 4, 0, 0, 0, 0,...</td>\n",
       "      <td>[94, 11, 5, 10, 13, 7, 8, 4, 12, 43, 22, 6, 21...</td>\n",
       "      <td>[33, 57, 71, 55, 23, 72, 20, 56, 5, 3, 58, 44,...</td>\n",
       "      <td>[2972, 4510, 6430, 5098, 1606, 1889, 3252, 216...</td>\n",
       "      <td>[330.0, 2802.0, 4177.0, 0.0, 1029.0, 788.0, 0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_size                                         fuzzy_hash  \\\n",
       "2247     6909752  [37, 65, 42, 15, 29, 33, 35, 38, 11, 28, 62, 4...   \n",
       "5698     9660703  [22, 8, 25, 21, 23, 25, 17, 56, 20, 45, 3, 10,...   \n",
       "8381     7014287  [37, 14, 16, 40, 19, 33, 46, 52, 5, 61, 3, 51,...   \n",
       "1682     5863124  [66, 29, 4, 42, 34, 45, 20, 38, 29, 66, 14, 15...   \n",
       "18394    8197050  [60, 62, 12, 30, 37, 58, 13, 38, 48, 30, 3, 55...   \n",
       "\n",
       "                                         activities_list  \\\n",
       "2247   [19048, 19079, 19053, 19081, 19052, 508, 45, 1...   \n",
       "5698   [139512, 3799, 34, 139502, 139528, 18, 139522,...   \n",
       "8381   [73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 8...   \n",
       "1682   [37, 9, 308, 224756, 315, 224757, 12, 52, 7, 2...   \n",
       "18394  [6, 10, 4, 586, 44, 3, 42, 41, 6293, 43, 555, ...   \n",
       "\n",
       "                                           services_list  \\\n",
       "2247   [20, 25, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "5698   [23, 12423, 12421, 12422, 30, 20, 6, 26, 0, 0,...   \n",
       "8381   [129, 120, 104, 115, 108, 60, 102, 61, 96, 62,...   \n",
       "1682   [139, 32, 151, 146, 35, 55, 29, 0, 0, 0, 0, 0,...   \n",
       "18394  [9, 12, 8, 6, 13, 5, 10, 11, 3, 4, 0, 0, 0, 0,...   \n",
       "\n",
       "                                          receivers_list  \\\n",
       "2247   [2204, 2200, 18, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "5698   [9139, 5, 18, 3, 67, 17, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "8381   [62, 45, 55, 58, 54, 63, 57, 56, 53, 61, 49, 5...   \n",
       "1682   [29, 156, 39, 31, 100, 26, 40, 0, 0, 0, 0, 0, ...   \n",
       "18394  [94, 11, 5, 10, 13, 7, 8, 4, 12, 43, 22, 6, 21...   \n",
       "\n",
       "                                        permissions_list  \\\n",
       "2247   [69, 27, 6, 25, 3, 19, 22, 61, 14, 9, 24, 1001...   \n",
       "5698   [15, 39, 11, 26, 78, 4, 2755, 5, 64, 6, 3, 9, ...   \n",
       "8381   [163, 43, 143, 70, 159, 84, 86, 118, 117, 4, 1...   \n",
       "1682   [42, 22, 11, 19, 14, 6, 100, 15, 10, 46, 13, 2...   \n",
       "18394  [33, 57, 71, 55, 23, 72, 20, 56, 5, 3, 58, 44,...   \n",
       "\n",
       "                                          api_calls_list  \\\n",
       "2247   [78, 122, 2465, 119, 106, 7158, 1202, 705, 383...   \n",
       "5698   [105950, 63567, 187729, 3877, 3934, 3891, 7551...   \n",
       "8381   [32224, 32071, 18370, 27218, 23182, 24664, 263...   \n",
       "1682   [78, 122, 2465, 119, 106, 7158, 1202, 705, 383...   \n",
       "18394  [2972, 4510, 6430, 5098, 1606, 1889, 3252, 216...   \n",
       "\n",
       "                                           opcode_counts  is_malware  \n",
       "2247   [0.0, 5.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, ...           1  \n",
       "5698   [628.0, 7051.0, 4476.0, 0.0, 289.0, 550.0, 0.0...           1  \n",
       "8381   [933.0, 10440.0, 690.0, 0.0, 304.0, 204.0, 0.0...           1  \n",
       "1682   [0.0, 6.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...           1  \n",
       "18394  [330.0, 2802.0, 4177.0, 0.0, 1029.0, 788.0, 0....           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7faffa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25e1501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_hyperparams = NNHyperparams(\n",
    "    batch_size=64,\n",
    "    max_learning_rate=1e-3,\n",
    "    epochs=20,\n",
    "    early_stopping=True,\n",
    "    patience=5,\n",
    "    optimizer=\"adamw\",\n",
    "    weight_decay=1e-5,\n",
    "    embedding_dim=64,\n",
    "    hidden_dims=[64],\n",
    "    dropout=0.5,\n",
    "    seq_pooling=\"mean\",\n",
    "    n_classes=2,\n",
    "    label_col=\"is_malware\",\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_persistent_workers=True,\n",
    "    grad_scaler_max_norm=1.0,\n",
    ")\n",
    "\n",
    "# nn_hyperparams = NNHyperparams(\n",
    "#     batch_size=16,\n",
    "#     max_learning_rate=5e-3,\n",
    "#     epochs=20,\n",
    "#     early_stopping=True,\n",
    "#     patience=5,\n",
    "#     optimizer=\"adamw\",\n",
    "#     weight_decay=5e-4,\n",
    "#     embedding_dim=256,\n",
    "#     hidden_dims=[256, 16],\n",
    "#     dropout=0.2,\n",
    "#     seq_pooling=\"mean\",\n",
    "#     n_classes=2,\n",
    "#     label_col=\"is_malware\",\n",
    "#     dataloader_num_workers=2,\n",
    "#     dataloader_pin_memory=True,\n",
    "#     dataloader_persistent_workers=True,\n",
    "#     grad_scaler_max_norm=1.0,\n",
    "# )\n",
    "\n",
    "# nn_hyperparams = NNHyperparams(\n",
    "#     batch_size=64,\n",
    "#     max_learning_rate=6e-3,\n",
    "#     epochs=20,\n",
    "#     early_stopping=True,\n",
    "#     patience=5,\n",
    "#     optimizer=\"adamw\",\n",
    "#     weight_decay=8e-4,\n",
    "#     embedding_dim=64,\n",
    "#     hidden_dims=[128],\n",
    "#     dropout=0.5,\n",
    "#     seq_pooling=\"mean\",\n",
    "#     n_classes=2,\n",
    "#     label_col=\"is_malware\",\n",
    "#     dataloader_num_workers=2,\n",
    "#     dataloader_pin_memory=True,\n",
    "#     dataloader_persistent_workers=True,\n",
    "#     grad_scaler_max_norm=1.0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bcf2aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Performing internal train/validation split with ratio: 0.8\n",
      "Training set size: 14379, Validation set size: 3595\n",
      "Training set class distribution: {0: 7200, 1: 7179}\n",
      "Validation set class distribution: {0: 1800, 1: 1795}\n",
      "Using class weights: [0.99854167 1.0014626 ]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/225, Train Loss: 0.7019, LR: 4.00e-05\n",
      "Epoch 1, Batch 46/225, Train Loss: 0.6681, LR: 4.29e-05\n",
      "Epoch 1, Batch 92/225, Train Loss: 0.5862, LR: 5.12e-05\n",
      "Epoch 1, Batch 138/225, Train Loss: 0.5033, LR: 6.49e-05\n",
      "Epoch 1, Batch 184/225, Train Loss: 0.3782, LR: 8.39e-05\n",
      "Epoch 1/20 — Train Loss: 0.5300\n",
      "Epoch 1 — Val Loss: 0.2918, Val Recall: 0.9432 (Acc: 0.8954, P: 0.8607, R: 0.9432, F1: 0.9001, ROC: 0.9543, PR: 0.9500)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/225, Train Loss: 0.2597, LR: 1.05e-04\n",
      "Epoch 2, Batch 46/225, Train Loss: 0.1677, LR: 1.33e-04\n",
      "Epoch 2, Batch 92/225, Train Loss: 0.2564, LR: 1.66e-04\n",
      "Epoch 2, Batch 138/225, Train Loss: 0.0974, LR: 2.02e-04\n",
      "Epoch 2, Batch 184/225, Train Loss: 0.2104, LR: 2.43e-04\n",
      "Epoch 2/20 — Train Loss: 0.1746\n",
      "Epoch 2 — Val Loss: 0.1093, Val Recall: 0.9755 (Acc: 0.9566, P: 0.9399, R: 0.9755, F1: 0.9574, ROC: 0.9938, PR: 0.9940)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/225, Train Loss: 0.0631, LR: 2.81e-04\n",
      "Epoch 3, Batch 46/225, Train Loss: 0.1975, LR: 3.27e-04\n",
      "Epoch 3, Batch 92/225, Train Loss: 0.0318, LR: 3.75e-04\n",
      "Epoch 3, Batch 138/225, Train Loss: 0.0472, LR: 4.25e-04\n",
      "Epoch 3, Batch 184/225, Train Loss: 0.4366, LR: 4.76e-04\n",
      "Epoch 3/20 — Train Loss: 0.0869\n",
      "Epoch 3 — Val Loss: 0.0671, Val Recall: 0.9733 (Acc: 0.9755, P: 0.9776, R: 0.9733, F1: 0.9754, ROC: 0.9972, PR: 0.9973)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/225, Train Loss: 0.0094, LR: 5.22e-04\n",
      "Epoch 4, Batch 46/225, Train Loss: 0.0726, LR: 5.73e-04\n",
      "Epoch 4, Batch 92/225, Train Loss: 0.0396, LR: 6.24e-04\n",
      "Epoch 4, Batch 138/225, Train Loss: 0.1175, LR: 6.73e-04\n",
      "Epoch 4, Batch 184/225, Train Loss: 0.0498, LR: 7.21e-04\n",
      "Epoch 4/20 — Train Loss: 0.0602\n",
      "Epoch 4 — Val Loss: 0.0504, Val Recall: 0.9866 (Acc: 0.9825, P: 0.9785, R: 0.9866, F1: 0.9825, ROC: 0.9984, PR: 0.9984)\n",
      "  New best model (by val_loss) saved at epoch 4.\n",
      "Epoch 5, Batch 0/225, Train Loss: 0.0077, LR: 7.62e-04\n",
      "Epoch 5, Batch 46/225, Train Loss: 0.0154, LR: 8.05e-04\n",
      "Epoch 5, Batch 92/225, Train Loss: 0.0609, LR: 8.44e-04\n",
      "Epoch 5, Batch 138/225, Train Loss: 0.1090, LR: 8.80e-04\n",
      "Epoch 5, Batch 184/225, Train Loss: 0.1706, LR: 9.12e-04\n",
      "Epoch 5/20 — Train Loss: 0.0422\n",
      "Epoch 5 — Val Loss: 0.0477, Val Recall: 0.9777 (Acc: 0.9841, P: 0.9904, R: 0.9777, F1: 0.9840, ROC: 0.9986, PR: 0.9985)\n",
      "  New best model (by val_loss) saved at epoch 5.\n",
      "Epoch 6, Batch 0/225, Train Loss: 0.0108, LR: 9.37e-04\n",
      "Epoch 6, Batch 46/225, Train Loss: 0.0428, LR: 9.60e-04\n",
      "Epoch 6, Batch 92/225, Train Loss: 0.0598, LR: 9.78e-04\n",
      "Epoch 6, Batch 138/225, Train Loss: 0.0033, LR: 9.91e-04\n",
      "Epoch 6, Batch 184/225, Train Loss: 0.0852, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0273\n",
      "Epoch 6 — Val Loss: 0.0450, Val Recall: 0.9900 (Acc: 0.9858, P: 0.9818, R: 0.9900, F1: 0.9859, ROC: 0.9988, PR: 0.9987)\n",
      "  New best model (by val_loss) saved at epoch 6.\n",
      "Epoch 7, Batch 0/225, Train Loss: 0.0208, LR: 1.00e-03\n",
      "Epoch 7, Batch 46/225, Train Loss: 0.0346, LR: 9.99e-04\n",
      "Epoch 7, Batch 92/225, Train Loss: 0.0017, LR: 9.98e-04\n",
      "Epoch 7, Batch 138/225, Train Loss: 0.0049, LR: 9.95e-04\n",
      "Epoch 7, Batch 184/225, Train Loss: 0.0233, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0167\n",
      "Epoch 7 — Val Loss: 0.0463, Val Recall: 0.9772 (Acc: 0.9836, P: 0.9898, R: 0.9772, F1: 0.9835, ROC: 0.9986, PR: 0.9984)\n",
      "Epoch 8, Batch 0/225, Train Loss: 0.0050, LR: 9.87e-04\n",
      "Epoch 8, Batch 46/225, Train Loss: 0.0053, LR: 9.82e-04\n",
      "Epoch 8, Batch 92/225, Train Loss: 0.0083, LR: 9.75e-04\n",
      "Epoch 8, Batch 138/225, Train Loss: 0.0193, LR: 9.67e-04\n",
      "Epoch 8, Batch 184/225, Train Loss: 0.0004, LR: 9.59e-04\n",
      "Epoch 8/20 — Train Loss: 0.0133\n",
      "Epoch 8 — Val Loss: 0.0495, Val Recall: 0.9900 (Acc: 0.9880, P: 0.9861, R: 0.9900, F1: 0.9880, ROC: 0.9979, PR: 0.9977)\n",
      "Epoch 9, Batch 0/225, Train Loss: 0.0108, LR: 9.50e-04\n",
      "Epoch 9, Batch 46/225, Train Loss: 0.0006, LR: 9.40e-04\n",
      "Epoch 9, Batch 92/225, Train Loss: 0.0017, LR: 9.28e-04\n",
      "Epoch 9, Batch 138/225, Train Loss: 0.0162, LR: 9.16e-04\n",
      "Epoch 9, Batch 184/225, Train Loss: 0.0060, LR: 9.03e-04\n",
      "Epoch 9/20 — Train Loss: 0.0068\n",
      "Epoch 9 — Val Loss: 0.0624, Val Recall: 0.9911 (Acc: 0.9844, P: 0.9780, R: 0.9911, F1: 0.9845, ROC: 0.9977, PR: 0.9976)\n",
      "Epoch 10, Batch 0/225, Train Loss: 0.0034, LR: 8.90e-04\n",
      "Epoch 10, Batch 46/225, Train Loss: 0.0001, LR: 8.76e-04\n",
      "Epoch 10, Batch 92/225, Train Loss: 0.0005, LR: 8.60e-04\n",
      "Epoch 10, Batch 138/225, Train Loss: 0.0010, LR: 8.44e-04\n",
      "Epoch 10, Batch 184/225, Train Loss: 0.0013, LR: 8.27e-04\n",
      "Epoch 10/20 — Train Loss: 0.0044\n",
      "Epoch 10 — Val Loss: 0.0559, Val Recall: 0.9855 (Acc: 0.9858, P: 0.9861, R: 0.9855, F1: 0.9858, ROC: 0.9975, PR: 0.9973)\n",
      "Epoch 11, Batch 0/225, Train Loss: 0.0004, LR: 8.11e-04\n",
      "Epoch 11, Batch 46/225, Train Loss: 0.0087, LR: 7.93e-04\n",
      "Epoch 11, Batch 92/225, Train Loss: 0.0004, LR: 7.74e-04\n",
      "Epoch 11, Batch 138/225, Train Loss: 0.0007, LR: 7.54e-04\n",
      "Epoch 11, Batch 184/225, Train Loss: 0.0002, LR: 7.34e-04\n",
      "Epoch 11/20 — Train Loss: 0.0029\n",
      "Epoch 11 — Val Loss: 0.0810, Val Recall: 0.9916 (Acc: 0.9794, P: 0.9679, R: 0.9916, F1: 0.9796, ROC: 0.9973, PR: 0.9972)\n",
      "Early stopping triggered after 11 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 6 with Val Loss: 0.0450\n",
      "\n",
      "Training finished. Total time: 221.77s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 6:\n",
      "  - Accuracy: 0.9858\n",
      "  - Precision: 0.9818\n",
      "  - Recall: 0.9900\n",
      "  - F1: 0.9859\n",
      "  - Roc_auc: 0.9988\n",
      "  - Pr_auc: 0.9987\n",
      "  - Val_loss: 0.0450\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_results, fitted_scalers = train_nn_model(\n",
    "    df=df,\n",
    "    vocab_dict=vocab_dict,\n",
    "    sequence_cols=SEQUENCE_COLS,\n",
    "    scalar_cols=SCALAR_COLS,\n",
    "    char_cols=CHAR_COLS,\n",
    "    vector_cols=VECTOR_COLS,\n",
    "    vector_dims=VECTOR_DIMS,\n",
    "    hyperparams=nn_hyperparams,\n",
    "    scoring_metric=\"recall\",\n",
    "    train_split_ratio=0.8,\n",
    "    device=device,\n",
    "    random_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499b3aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Performing internal train/validation split with ratio: 0.8\n",
      "Training set size: 14379, Validation set size: 3595\n",
      "Training set class distribution: {0: 7200, 1: 7179}\n",
      "Validation set class distribution: {0: 1800, 1: 1795}\n",
      "Using class weights: [0.99854167 1.0014626 ]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/225, Train Loss: 0.6893, LR: 2.40e-04\n",
      "Epoch 1, Batch 46/225, Train Loss: 0.3296, LR: 2.57e-04\n",
      "Epoch 1, Batch 92/225, Train Loss: 0.2261, LR: 3.07e-04\n",
      "Epoch 1, Batch 138/225, Train Loss: 0.2144, LR: 3.90e-04\n",
      "Epoch 1, Batch 184/225, Train Loss: 0.0952, LR: 5.03e-04\n",
      "Epoch 1/20 — Train Loss: 0.2349\n",
      "Epoch 1 — Val Loss: 0.0808, Val Recall: 0.9721 (Acc: 0.9711, P: 0.9700, R: 0.9721, F1: 0.9711, ROC: 0.9960, PR: 0.9960)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/225, Train Loss: 0.0695, LR: 6.30e-04\n",
      "Epoch 2, Batch 46/225, Train Loss: 0.0478, LR: 7.99e-04\n",
      "Epoch 2, Batch 92/225, Train Loss: 0.0383, LR: 9.94e-04\n",
      "Epoch 2, Batch 138/225, Train Loss: 0.0340, LR: 1.21e-03\n",
      "Epoch 2, Batch 184/225, Train Loss: 0.0587, LR: 1.46e-03\n",
      "Epoch 2/20 — Train Loss: 0.0671\n",
      "Epoch 2 — Val Loss: 0.0520, Val Recall: 0.9877 (Acc: 0.9828, P: 0.9779, R: 0.9877, F1: 0.9828, ROC: 0.9983, PR: 0.9982)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/225, Train Loss: 0.0282, LR: 1.69e-03\n",
      "Epoch 3, Batch 46/225, Train Loss: 0.0570, LR: 1.96e-03\n",
      "Epoch 3, Batch 92/225, Train Loss: 0.0140, LR: 2.25e-03\n",
      "Epoch 3, Batch 138/225, Train Loss: 0.0165, LR: 2.55e-03\n",
      "Epoch 3, Batch 184/225, Train Loss: 0.0084, LR: 2.86e-03\n",
      "Epoch 3/20 — Train Loss: 0.0417\n",
      "Epoch 3 — Val Loss: 0.0479, Val Recall: 0.9894 (Acc: 0.9841, P: 0.9791, R: 0.9894, F1: 0.9842, ROC: 0.9985, PR: 0.9985)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/225, Train Loss: 0.0092, LR: 3.13e-03\n",
      "Epoch 4, Batch 46/225, Train Loss: 0.0029, LR: 3.44e-03\n",
      "Epoch 4, Batch 92/225, Train Loss: 0.0038, LR: 3.74e-03\n",
      "Epoch 4, Batch 138/225, Train Loss: 0.0133, LR: 4.04e-03\n",
      "Epoch 4, Batch 184/225, Train Loss: 0.0851, LR: 4.33e-03\n",
      "Epoch 4/20 — Train Loss: 0.0228\n",
      "Epoch 4 — Val Loss: 0.0530, Val Recall: 0.9933 (Acc: 0.9844, P: 0.9759, R: 0.9933, F1: 0.9845, ROC: 0.9982, PR: 0.9981)\n",
      "Epoch 5, Batch 0/225, Train Loss: 0.0016, LR: 4.57e-03\n",
      "Epoch 5, Batch 46/225, Train Loss: 0.0003, LR: 4.83e-03\n",
      "Epoch 5, Batch 92/225, Train Loss: 0.0018, LR: 5.07e-03\n",
      "Epoch 5, Batch 138/225, Train Loss: 0.0070, LR: 5.28e-03\n",
      "Epoch 5, Batch 184/225, Train Loss: 0.0001, LR: 5.47e-03\n",
      "Epoch 5/20 — Train Loss: 0.0101\n",
      "Epoch 5 — Val Loss: 0.0646, Val Recall: 0.9844 (Acc: 0.9822, P: 0.9800, R: 0.9844, F1: 0.9822, ROC: 0.9979, PR: 0.9979)\n",
      "Epoch 6, Batch 0/225, Train Loss: 0.0002, LR: 5.62e-03\n",
      "Epoch 6, Batch 46/225, Train Loss: 0.0001, LR: 5.76e-03\n",
      "Epoch 6, Batch 92/225, Train Loss: 0.0001, LR: 5.87e-03\n",
      "Epoch 6, Batch 138/225, Train Loss: 0.0448, LR: 5.94e-03\n",
      "Epoch 6, Batch 184/225, Train Loss: 0.0004, LR: 5.99e-03\n",
      "Epoch 6/20 — Train Loss: 0.0092\n",
      "Epoch 6 — Val Loss: 0.0693, Val Recall: 0.9850 (Acc: 0.9822, P: 0.9795, R: 0.9850, F1: 0.9822, ROC: 0.9977, PR: 0.9976)\n",
      "Epoch 7, Batch 0/225, Train Loss: 0.0001, LR: 6.00e-03\n",
      "Epoch 7, Batch 46/225, Train Loss: 0.0000, LR: 6.00e-03\n",
      "Epoch 7, Batch 92/225, Train Loss: 0.0000, LR: 5.99e-03\n",
      "Epoch 7, Batch 138/225, Train Loss: 0.0001, LR: 5.97e-03\n",
      "Epoch 7, Batch 184/225, Train Loss: 0.0000, LR: 5.95e-03\n",
      "Epoch 7/20 — Train Loss: 0.0036\n",
      "Epoch 7 — Val Loss: 0.0837, Val Recall: 0.9811 (Acc: 0.9819, P: 0.9827, R: 0.9811, F1: 0.9819, ROC: 0.9966, PR: 0.9964)\n",
      "Epoch 8, Batch 0/225, Train Loss: 0.0000, LR: 5.92e-03\n",
      "Epoch 8, Batch 46/225, Train Loss: 0.0006, LR: 5.89e-03\n",
      "Epoch 8, Batch 92/225, Train Loss: 0.0000, LR: 5.85e-03\n",
      "Epoch 8, Batch 138/225, Train Loss: 0.0003, LR: 5.80e-03\n",
      "Epoch 8, Batch 184/225, Train Loss: 0.0001, LR: 5.75e-03\n",
      "Epoch 8/20 — Train Loss: 0.0016\n",
      "Epoch 8 — Val Loss: 0.1003, Val Recall: 0.9833 (Acc: 0.9836, P: 0.9838, R: 0.9833, F1: 0.9836, ROC: 0.9964, PR: 0.9963)\n",
      "Early stopping triggered after 8 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 3 with Val Loss: 0.0479\n",
      "\n",
      "Training finished. Total time: 165.05s. Model size: 1034891.51 KB\n",
      "Best model (by val_loss) obtained at epoch 3:\n",
      "  - Accuracy: 0.9841\n",
      "  - Precision: 0.9791\n",
      "  - Recall: 0.9894\n",
      "  - F1: 0.9842\n",
      "  - Roc_auc: 0.9985\n",
      "  - Pr_auc: 0.9985\n",
      "  - Val_loss: 0.0479\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_results, fitted_scalers = train_nn_model(\n",
    "    df=df,\n",
    "    vocab_dict=vocab_dict,\n",
    "    sequence_cols=SEQUENCE_COLS,\n",
    "    scalar_cols=SCALAR_COLS,\n",
    "    char_cols=CHAR_COLS,\n",
    "    vector_cols=VECTOR_COLS,\n",
    "    vector_dims=VECTOR_DIMS,\n",
    "    hyperparams=nn_hyperparams,\n",
    "    scoring_metric=\"recall\",\n",
    "    train_split_ratio=0.8,\n",
    "    device=device,\n",
    "    random_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4f3f280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cross-Validation Training ---\n",
      "Using device: cuda\n",
      "Primary scoring metric for best model selection: RECALL\n",
      "Number of folds: 2, Number of repetitions: 5\n",
      "\n",
      "--- Repetition 1/5, Fold 1/2 (Overall Fold 1) ---\n",
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Using explicitly provided training and validation DataFrames.\n",
      "Training set size: 8987, Validation set size: 8987\n",
      "Training set class distribution: {0: 4500, 1: 4487}\n",
      "Validation set class distribution: {0: 4500, 1: 4487}\n",
      "Using class weights: [0.99855556 1.00144863]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/141, Train Loss: 0.7227, LR: 4.00e-05\n",
      "Epoch 1, Batch 29/141, Train Loss: 0.6691, LR: 4.30e-05\n",
      "Epoch 1, Batch 58/141, Train Loss: 0.6395, LR: 5.15e-05\n",
      "Epoch 1, Batch 87/141, Train Loss: 0.5995, LR: 6.55e-05\n",
      "Epoch 1, Batch 116/141, Train Loss: 0.4867, LR: 8.47e-05\n",
      "Epoch 1/20 — Train Loss: 0.5996\n",
      "Epoch 1 — Val Loss: 0.4058, Val Recall: 0.9019 (Acc: 0.8758, P: 0.8569, R: 0.9019, F1: 0.8788, ROC: 0.9423, PR: 0.9382)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/141, Train Loss: 0.4015, LR: 1.05e-04\n",
      "Epoch 2, Batch 29/141, Train Loss: 0.3801, LR: 1.34e-04\n",
      "Epoch 2, Batch 58/141, Train Loss: 0.2521, LR: 1.67e-04\n",
      "Epoch 2, Batch 87/141, Train Loss: 0.1508, LR: 2.04e-04\n",
      "Epoch 2, Batch 116/141, Train Loss: 0.1540, LR: 2.44e-04\n",
      "Epoch 2/20 — Train Loss: 0.2482\n",
      "Epoch 2 — Val Loss: 0.1381, Val Recall: 0.9574 (Acc: 0.9595, P: 0.9613, R: 0.9574, F1: 0.9594, ROC: 0.9901, PR: 0.9844)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/141, Train Loss: 0.0960, LR: 2.82e-04\n",
      "Epoch 3, Batch 29/141, Train Loss: 0.1011, LR: 3.28e-04\n",
      "Epoch 3, Batch 58/141, Train Loss: 0.2521, LR: 3.77e-04\n",
      "Epoch 3, Batch 87/141, Train Loss: 0.1715, LR: 4.27e-04\n",
      "Epoch 3, Batch 116/141, Train Loss: 0.0871, LR: 4.78e-04\n",
      "Epoch 3/20 — Train Loss: 0.1084\n",
      "Epoch 3 — Val Loss: 0.0808, Val Recall: 0.9797 (Acc: 0.9753, P: 0.9711, R: 0.9797, F1: 0.9754, ROC: 0.9956, PR: 0.9937)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/141, Train Loss: 0.0523, LR: 5.23e-04\n",
      "Epoch 4, Batch 29/141, Train Loss: 0.1104, LR: 5.74e-04\n",
      "Epoch 4, Batch 58/141, Train Loss: 0.0330, LR: 6.25e-04\n",
      "Epoch 4, Batch 87/141, Train Loss: 0.1852, LR: 6.75e-04\n",
      "Epoch 4, Batch 116/141, Train Loss: 0.0912, LR: 7.23e-04\n",
      "Epoch 4/20 — Train Loss: 0.0744\n",
      "Epoch 4 — Val Loss: 0.0645, Val Recall: 0.9795 (Acc: 0.9810, P: 0.9823, R: 0.9795, F1: 0.9809, ROC: 0.9973, PR: 0.9961)\n",
      "  New best model (by val_loss) saved at epoch 4.\n",
      "Epoch 5, Batch 0/141, Train Loss: 0.0222, LR: 7.63e-04\n",
      "Epoch 5, Batch 29/141, Train Loss: 0.0507, LR: 8.06e-04\n",
      "Epoch 5, Batch 58/141, Train Loss: 0.0229, LR: 8.46e-04\n",
      "Epoch 5, Batch 87/141, Train Loss: 0.0248, LR: 8.82e-04\n",
      "Epoch 5, Batch 116/141, Train Loss: 0.1267, LR: 9.14e-04\n",
      "Epoch 5/20 — Train Loss: 0.0517\n",
      "Epoch 5 — Val Loss: 0.0594, Val Recall: 0.9886 (Acc: 0.9840, P: 0.9795, R: 0.9886, F1: 0.9840, ROC: 0.9978, PR: 0.9973)\n",
      "  New best model (by val_loss) saved at epoch 5.\n",
      "Epoch 6, Batch 0/141, Train Loss: 0.0230, LR: 9.37e-04\n",
      "Epoch 6, Batch 29/141, Train Loss: 0.0310, LR: 9.60e-04\n",
      "Epoch 6, Batch 58/141, Train Loss: 0.0215, LR: 9.78e-04\n",
      "Epoch 6, Batch 87/141, Train Loss: 0.0147, LR: 9.91e-04\n",
      "Epoch 6, Batch 116/141, Train Loss: 0.0339, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0342\n",
      "Epoch 6 — Val Loss: 0.0596, Val Recall: 0.9791 (Acc: 0.9832, P: 0.9872, R: 0.9791, F1: 0.9831, ROC: 0.9978, PR: 0.9973)\n",
      "Epoch 7, Batch 0/141, Train Loss: 0.0449, LR: 1.00e-03\n",
      "Epoch 7, Batch 29/141, Train Loss: 0.0064, LR: 9.99e-04\n",
      "Epoch 7, Batch 58/141, Train Loss: 0.0689, LR: 9.98e-04\n",
      "Epoch 7, Batch 87/141, Train Loss: 0.0616, LR: 9.95e-04\n",
      "Epoch 7, Batch 116/141, Train Loss: 0.0411, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0243\n",
      "Epoch 7 — Val Loss: 0.0570, Val Recall: 0.9791 (Acc: 0.9835, P: 0.9879, R: 0.9791, F1: 0.9834, ROC: 0.9980, PR: 0.9977)\n",
      "  New best model (by val_loss) saved at epoch 7.\n",
      "Epoch 8, Batch 0/141, Train Loss: 0.0080, LR: 9.87e-04\n",
      "Epoch 8, Batch 29/141, Train Loss: 0.0132, LR: 9.81e-04\n",
      "Epoch 8, Batch 58/141, Train Loss: 0.0057, LR: 9.75e-04\n",
      "Epoch 8, Batch 87/141, Train Loss: 0.0083, LR: 9.67e-04\n",
      "Epoch 8, Batch 116/141, Train Loss: 0.0268, LR: 9.58e-04\n",
      "Epoch 8/20 — Train Loss: 0.0155\n",
      "Epoch 8 — Val Loss: 0.0627, Val Recall: 0.9860 (Acc: 0.9841, P: 0.9822, R: 0.9860, F1: 0.9841, ROC: 0.9979, PR: 0.9977)\n",
      "Epoch 9, Batch 0/141, Train Loss: 0.0057, LR: 9.50e-04\n",
      "Epoch 9, Batch 29/141, Train Loss: 0.0040, LR: 9.39e-04\n",
      "Epoch 9, Batch 58/141, Train Loss: 0.0049, LR: 9.28e-04\n",
      "Epoch 9, Batch 87/141, Train Loss: 0.0017, LR: 9.15e-04\n",
      "Epoch 9, Batch 116/141, Train Loss: 0.0067, LR: 9.02e-04\n",
      "Epoch 9/20 — Train Loss: 0.0108\n",
      "Epoch 9 — Val Loss: 0.0680, Val Recall: 0.9828 (Acc: 0.9842, P: 0.9855, R: 0.9828, F1: 0.9842, ROC: 0.9976, PR: 0.9974)\n",
      "Epoch 10, Batch 0/141, Train Loss: 0.0260, LR: 8.90e-04\n",
      "Epoch 10, Batch 29/141, Train Loss: 0.0008, LR: 8.75e-04\n",
      "Epoch 10, Batch 58/141, Train Loss: 0.0150, LR: 8.59e-04\n",
      "Epoch 10, Batch 87/141, Train Loss: 0.0012, LR: 8.43e-04\n",
      "Epoch 10, Batch 116/141, Train Loss: 0.0042, LR: 8.26e-04\n",
      "Epoch 10/20 — Train Loss: 0.0066\n",
      "Epoch 10 — Val Loss: 0.0721, Val Recall: 0.9848 (Acc: 0.9845, P: 0.9842, R: 0.9848, F1: 0.9845, ROC: 0.9977, PR: 0.9976)\n",
      "Epoch 11, Batch 0/141, Train Loss: 0.0009, LR: 8.10e-04\n",
      "Epoch 11, Batch 29/141, Train Loss: 0.0008, LR: 7.92e-04\n",
      "Epoch 11, Batch 58/141, Train Loss: 0.0054, LR: 7.73e-04\n",
      "Epoch 11, Batch 87/141, Train Loss: 0.0021, LR: 7.53e-04\n",
      "Epoch 11, Batch 116/141, Train Loss: 0.0005, LR: 7.33e-04\n",
      "Epoch 11/20 — Train Loss: 0.0037\n",
      "Epoch 11 — Val Loss: 0.0739, Val Recall: 0.9819 (Acc: 0.9833, P: 0.9846, R: 0.9819, F1: 0.9833, ROC: 0.9976, PR: 0.9975)\n",
      "Epoch 12, Batch 0/141, Train Loss: 0.0015, LR: 7.16e-04\n",
      "Epoch 12, Batch 29/141, Train Loss: 0.0003, LR: 6.94e-04\n",
      "Epoch 12, Batch 58/141, Train Loss: 0.0016, LR: 6.73e-04\n",
      "Epoch 12, Batch 87/141, Train Loss: 0.0028, LR: 6.51e-04\n",
      "Epoch 12, Batch 116/141, Train Loss: 0.0169, LR: 6.29e-04\n",
      "Epoch 12/20 — Train Loss: 0.0036\n",
      "Epoch 12 — Val Loss: 0.0765, Val Recall: 0.9813 (Acc: 0.9835, P: 0.9857, R: 0.9813, F1: 0.9835, ROC: 0.9977, PR: 0.9976)\n",
      "Early stopping triggered after 12 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 7 with Val Loss: 0.0570\n",
      "\n",
      "Training finished. Total time: 162.28s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 7:\n",
      "  - Accuracy: 0.9835\n",
      "  - Precision: 0.9879\n",
      "  - Recall: 0.9791\n",
      "  - F1: 0.9834\n",
      "  - Roc_auc: 0.9980\n",
      "  - Pr_auc: 0.9977\n",
      "  - Val_loss: 0.0570\n",
      "Fold 1 finished. Val Recall: 0.9791 (from best model by val_loss within fold)\n",
      "  *** New overall best model found based on RECALL across folds! Score: 0.9791 ***\n",
      "\n",
      "--- Repetition 1/5, Fold 2/2 (Overall Fold 2) ---\n",
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Using explicitly provided training and validation DataFrames.\n",
      "Training set size: 8987, Validation set size: 8987\n",
      "Training set class distribution: {0: 4500, 1: 4487}\n",
      "Validation set class distribution: {0: 4500, 1: 4487}\n",
      "Using class weights: [0.99855556 1.00144863]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/141, Train Loss: 0.7135, LR: 4.00e-05\n",
      "Epoch 1, Batch 29/141, Train Loss: 0.6722, LR: 4.30e-05\n",
      "Epoch 1, Batch 58/141, Train Loss: 0.6513, LR: 5.15e-05\n",
      "Epoch 1, Batch 87/141, Train Loss: 0.6021, LR: 6.55e-05\n",
      "Epoch 1, Batch 116/141, Train Loss: 0.5426, LR: 8.47e-05\n",
      "Epoch 1/20 — Train Loss: 0.6103\n",
      "Epoch 1 — Val Loss: 0.4190, Val Recall: 0.8897 (Acc: 0.8660, P: 0.8492, R: 0.8897, F1: 0.8690, ROC: 0.9413, PR: 0.9331)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/141, Train Loss: 0.4599, LR: 1.05e-04\n",
      "Epoch 2, Batch 29/141, Train Loss: 0.3044, LR: 1.34e-04\n",
      "Epoch 2, Batch 58/141, Train Loss: 0.2503, LR: 1.67e-04\n",
      "Epoch 2, Batch 87/141, Train Loss: 0.2507, LR: 2.04e-04\n",
      "Epoch 2, Batch 116/141, Train Loss: 0.1654, LR: 2.44e-04\n",
      "Epoch 2/20 — Train Loss: 0.2506\n",
      "Epoch 2 — Val Loss: 0.1343, Val Recall: 0.9632 (Acc: 0.9488, P: 0.9361, R: 0.9632, F1: 0.9495, ROC: 0.9897, PR: 0.9889)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/141, Train Loss: 0.0920, LR: 2.82e-04\n",
      "Epoch 3, Batch 29/141, Train Loss: 0.1992, LR: 3.28e-04\n",
      "Epoch 3, Batch 58/141, Train Loss: 0.1699, LR: 3.77e-04\n",
      "Epoch 3, Batch 87/141, Train Loss: 0.0859, LR: 4.27e-04\n",
      "Epoch 3, Batch 116/141, Train Loss: 0.0415, LR: 4.78e-04\n",
      "Epoch 3/20 — Train Loss: 0.1064\n",
      "Epoch 3 — Val Loss: 0.0936, Val Recall: 0.9744 (Acc: 0.9682, P: 0.9624, R: 0.9744, F1: 0.9683, ROC: 0.9935, PR: 0.9920)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/141, Train Loss: 0.0441, LR: 5.23e-04\n",
      "Epoch 4, Batch 29/141, Train Loss: 0.0807, LR: 5.74e-04\n",
      "Epoch 4, Batch 58/141, Train Loss: 0.1559, LR: 6.25e-04\n",
      "Epoch 4, Batch 87/141, Train Loss: 0.1309, LR: 6.75e-04\n",
      "Epoch 4, Batch 116/141, Train Loss: 0.0472, LR: 7.23e-04\n",
      "Epoch 4/20 — Train Loss: 0.0666\n",
      "Epoch 4 — Val Loss: 0.0770, Val Recall: 0.9840 (Acc: 0.9746, P: 0.9659, R: 0.9840, F1: 0.9748, ROC: 0.9959, PR: 0.9951)\n",
      "  New best model (by val_loss) saved at epoch 4.\n",
      "Epoch 5, Batch 0/141, Train Loss: 0.0425, LR: 7.63e-04\n",
      "Epoch 5, Batch 29/141, Train Loss: 0.0440, LR: 8.06e-04\n",
      "Epoch 5, Batch 58/141, Train Loss: 0.0665, LR: 8.46e-04\n",
      "Epoch 5, Batch 87/141, Train Loss: 0.0693, LR: 8.82e-04\n",
      "Epoch 5, Batch 116/141, Train Loss: 0.0609, LR: 9.14e-04\n",
      "Epoch 5/20 — Train Loss: 0.0458\n",
      "Epoch 5 — Val Loss: 0.0741, Val Recall: 0.9869 (Acc: 0.9769, P: 0.9674, R: 0.9869, F1: 0.9771, ROC: 0.9965, PR: 0.9961)\n",
      "  New best model (by val_loss) saved at epoch 5.\n",
      "Epoch 6, Batch 0/141, Train Loss: 0.0133, LR: 9.37e-04\n",
      "Epoch 6, Batch 29/141, Train Loss: 0.0123, LR: 9.60e-04\n",
      "Epoch 6, Batch 58/141, Train Loss: 0.0652, LR: 9.78e-04\n",
      "Epoch 6, Batch 87/141, Train Loss: 0.0115, LR: 9.91e-04\n",
      "Epoch 6, Batch 116/141, Train Loss: 0.0059, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0305\n",
      "Epoch 6 — Val Loss: 0.0626, Val Recall: 0.9782 (Acc: 0.9793, P: 0.9803, R: 0.9782, F1: 0.9793, ROC: 0.9970, PR: 0.9966)\n",
      "  New best model (by val_loss) saved at epoch 6.\n",
      "Epoch 7, Batch 0/141, Train Loss: 0.0125, LR: 1.00e-03\n",
      "Epoch 7, Batch 29/141, Train Loss: 0.0095, LR: 9.99e-04\n",
      "Epoch 7, Batch 58/141, Train Loss: 0.0174, LR: 9.98e-04\n",
      "Epoch 7, Batch 87/141, Train Loss: 0.0068, LR: 9.95e-04\n",
      "Epoch 7, Batch 116/141, Train Loss: 0.0047, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0168\n",
      "Epoch 7 — Val Loss: 0.0720, Val Recall: 0.9842 (Acc: 0.9794, P: 0.9748, R: 0.9842, F1: 0.9795, ROC: 0.9967, PR: 0.9962)\n",
      "Epoch 8, Batch 0/141, Train Loss: 0.0049, LR: 9.87e-04\n",
      "Epoch 8, Batch 29/141, Train Loss: 0.0330, LR: 9.81e-04\n",
      "Epoch 8, Batch 58/141, Train Loss: 0.0278, LR: 9.75e-04\n",
      "Epoch 8, Batch 87/141, Train Loss: 0.0602, LR: 9.67e-04\n",
      "Epoch 8, Batch 116/141, Train Loss: 0.0144, LR: 9.58e-04\n",
      "Epoch 8/20 — Train Loss: 0.0124\n",
      "Epoch 8 — Val Loss: 0.0701, Val Recall: 0.9802 (Acc: 0.9791, P: 0.9780, R: 0.9802, F1: 0.9791, ROC: 0.9969, PR: 0.9967)\n",
      "Epoch 9, Batch 0/141, Train Loss: 0.0028, LR: 9.50e-04\n",
      "Epoch 9, Batch 29/141, Train Loss: 0.0026, LR: 9.39e-04\n",
      "Epoch 9, Batch 58/141, Train Loss: 0.0028, LR: 9.28e-04\n",
      "Epoch 9, Batch 87/141, Train Loss: 0.0150, LR: 9.15e-04\n",
      "Epoch 9, Batch 116/141, Train Loss: 0.0139, LR: 9.02e-04\n",
      "Epoch 9/20 — Train Loss: 0.0086\n",
      "Epoch 9 — Val Loss: 0.0749, Val Recall: 0.9819 (Acc: 0.9786, P: 0.9754, R: 0.9819, F1: 0.9787, ROC: 0.9968, PR: 0.9966)\n",
      "Epoch 10, Batch 0/141, Train Loss: 0.0017, LR: 8.90e-04\n",
      "Epoch 10, Batch 29/141, Train Loss: 0.0017, LR: 8.75e-04\n",
      "Epoch 10, Batch 58/141, Train Loss: 0.0030, LR: 8.59e-04\n",
      "Epoch 10, Batch 87/141, Train Loss: 0.0029, LR: 8.43e-04\n",
      "Epoch 10, Batch 116/141, Train Loss: 0.0078, LR: 8.26e-04\n",
      "Epoch 10/20 — Train Loss: 0.0046\n",
      "Epoch 10 — Val Loss: 0.0804, Val Recall: 0.9813 (Acc: 0.9794, P: 0.9776, R: 0.9813, F1: 0.9794, ROC: 0.9966, PR: 0.9964)\n",
      "Epoch 11, Batch 0/141, Train Loss: 0.0050, LR: 8.10e-04\n",
      "Epoch 11, Batch 29/141, Train Loss: 0.0013, LR: 7.92e-04\n",
      "Epoch 11, Batch 58/141, Train Loss: 0.0034, LR: 7.73e-04\n",
      "Epoch 11, Batch 87/141, Train Loss: 0.0013, LR: 7.53e-04\n",
      "Epoch 11, Batch 116/141, Train Loss: 0.0006, LR: 7.33e-04\n",
      "Epoch 11/20 — Train Loss: 0.0034\n",
      "Epoch 11 — Val Loss: 0.0845, Val Recall: 0.9799 (Acc: 0.9795, P: 0.9791, R: 0.9799, F1: 0.9795, ROC: 0.9966, PR: 0.9964)\n",
      "Early stopping triggered after 11 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 6 with Val Loss: 0.0626\n",
      "\n",
      "Training finished. Total time: 152.38s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 6:\n",
      "  - Accuracy: 0.9793\n",
      "  - Precision: 0.9803\n",
      "  - Recall: 0.9782\n",
      "  - F1: 0.9793\n",
      "  - Roc_auc: 0.9970\n",
      "  - Pr_auc: 0.9966\n",
      "  - Val_loss: 0.0626\n",
      "Fold 2 finished. Val Recall: 0.9782 (from best model by val_loss within fold)\n",
      "\n",
      "--- Repetition 2/5, Fold 1/2 (Overall Fold 3) ---\n",
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Using explicitly provided training and validation DataFrames.\n",
      "Training set size: 8987, Validation set size: 8987\n",
      "Training set class distribution: {0: 4500, 1: 4487}\n",
      "Validation set class distribution: {0: 4500, 1: 4487}\n",
      "Using class weights: [0.99855556 1.00144863]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/141, Train Loss: 0.7170, LR: 4.00e-05\n",
      "Epoch 1, Batch 29/141, Train Loss: 0.6719, LR: 4.30e-05\n",
      "Epoch 1, Batch 58/141, Train Loss: 0.6237, LR: 5.15e-05\n",
      "Epoch 1, Batch 87/141, Train Loss: 0.5855, LR: 6.55e-05\n",
      "Epoch 1, Batch 116/141, Train Loss: 0.4986, LR: 8.47e-05\n",
      "Epoch 1/20 — Train Loss: 0.5952\n",
      "Epoch 1 — Val Loss: 0.3979, Val Recall: 0.9006 (Acc: 0.8602, P: 0.8330, R: 0.9006, F1: 0.8655, ROC: 0.9352, PR: 0.9264)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/141, Train Loss: 0.4462, LR: 1.05e-04\n",
      "Epoch 2, Batch 29/141, Train Loss: 0.3866, LR: 1.34e-04\n",
      "Epoch 2, Batch 58/141, Train Loss: 0.1957, LR: 1.67e-04\n",
      "Epoch 2, Batch 87/141, Train Loss: 0.1944, LR: 2.04e-04\n",
      "Epoch 2, Batch 116/141, Train Loss: 0.1266, LR: 2.44e-04\n",
      "Epoch 2/20 — Train Loss: 0.2406\n",
      "Epoch 2 — Val Loss: 0.1370, Val Recall: 0.9735 (Acc: 0.9549, P: 0.9385, R: 0.9735, F1: 0.9557, ROC: 0.9909, PR: 0.9891)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/141, Train Loss: 0.1064, LR: 2.82e-04\n",
      "Epoch 3, Batch 29/141, Train Loss: 0.1128, LR: 3.28e-04\n",
      "Epoch 3, Batch 58/141, Train Loss: 0.0584, LR: 3.77e-04\n",
      "Epoch 3, Batch 87/141, Train Loss: 0.0769, LR: 4.27e-04\n",
      "Epoch 3, Batch 116/141, Train Loss: 0.0808, LR: 4.78e-04\n",
      "Epoch 3/20 — Train Loss: 0.1006\n",
      "Epoch 3 — Val Loss: 0.1076, Val Recall: 0.9679 (Acc: 0.9705, P: 0.9729, R: 0.9679, F1: 0.9704, ROC: 0.9945, PR: 0.9939)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/141, Train Loss: 0.0816, LR: 5.23e-04\n",
      "Epoch 4, Batch 29/141, Train Loss: 0.0326, LR: 5.74e-04\n",
      "Epoch 4, Batch 58/141, Train Loss: 0.0776, LR: 6.25e-04\n",
      "Epoch 4, Batch 87/141, Train Loss: 0.0070, LR: 6.75e-04\n",
      "Epoch 4, Batch 116/141, Train Loss: 0.0420, LR: 7.23e-04\n",
      "Epoch 4/20 — Train Loss: 0.0661\n",
      "Epoch 4 — Val Loss: 0.1005, Val Recall: 0.9786 (Acc: 0.9750, P: 0.9715, R: 0.9786, F1: 0.9750, ROC: 0.9960, PR: 0.9957)\n",
      "  New best model (by val_loss) saved at epoch 4.\n",
      "Epoch 5, Batch 0/141, Train Loss: 0.0623, LR: 7.63e-04\n",
      "Epoch 5, Batch 29/141, Train Loss: 0.0477, LR: 8.06e-04\n",
      "Epoch 5, Batch 58/141, Train Loss: 0.0944, LR: 8.46e-04\n",
      "Epoch 5, Batch 87/141, Train Loss: 0.1650, LR: 8.82e-04\n",
      "Epoch 5, Batch 116/141, Train Loss: 0.0291, LR: 9.14e-04\n",
      "Epoch 5/20 — Train Loss: 0.0453\n",
      "Epoch 5 — Val Loss: 0.0966, Val Recall: 0.9750 (Acc: 0.9793, P: 0.9834, R: 0.9750, F1: 0.9792, ROC: 0.9965, PR: 0.9960)\n",
      "  New best model (by val_loss) saved at epoch 5.\n",
      "Epoch 6, Batch 0/141, Train Loss: 0.0180, LR: 9.37e-04\n",
      "Epoch 6, Batch 29/141, Train Loss: 0.0770, LR: 9.60e-04\n",
      "Epoch 6, Batch 58/141, Train Loss: 0.0160, LR: 9.78e-04\n",
      "Epoch 6, Batch 87/141, Train Loss: 0.0621, LR: 9.91e-04\n",
      "Epoch 6, Batch 116/141, Train Loss: 0.0419, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0326\n",
      "Epoch 6 — Val Loss: 0.1003, Val Recall: 0.9802 (Acc: 0.9800, P: 0.9797, R: 0.9802, F1: 0.9799, ROC: 0.9970, PR: 0.9970)\n",
      "Epoch 7, Batch 0/141, Train Loss: 0.0355, LR: 1.00e-03\n",
      "Epoch 7, Batch 29/141, Train Loss: 0.0055, LR: 9.99e-04\n",
      "Epoch 7, Batch 58/141, Train Loss: 0.0101, LR: 9.98e-04\n",
      "Epoch 7, Batch 87/141, Train Loss: 0.0054, LR: 9.95e-04\n",
      "Epoch 7, Batch 116/141, Train Loss: 0.0411, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0219\n",
      "Epoch 7 — Val Loss: 0.1054, Val Recall: 0.9757 (Acc: 0.9804, P: 0.9849, R: 0.9757, F1: 0.9803, ROC: 0.9969, PR: 0.9970)\n",
      "Epoch 8, Batch 0/141, Train Loss: 0.0128, LR: 9.87e-04\n",
      "Epoch 8, Batch 29/141, Train Loss: 0.0072, LR: 9.81e-04\n",
      "Epoch 8, Batch 58/141, Train Loss: 0.0014, LR: 9.75e-04\n",
      "Epoch 8, Batch 87/141, Train Loss: 0.0082, LR: 9.67e-04\n",
      "Epoch 8, Batch 116/141, Train Loss: 0.0027, LR: 9.58e-04\n",
      "Epoch 8/20 — Train Loss: 0.0135\n",
      "Epoch 8 — Val Loss: 0.1210, Val Recall: 0.9804 (Acc: 0.9803, P: 0.9802, R: 0.9804, F1: 0.9803, ROC: 0.9969, PR: 0.9969)\n",
      "Epoch 9, Batch 0/141, Train Loss: 0.0543, LR: 9.50e-04\n",
      "Epoch 9, Batch 29/141, Train Loss: 0.0119, LR: 9.39e-04\n",
      "Epoch 9, Batch 58/141, Train Loss: 0.0075, LR: 9.28e-04\n",
      "Epoch 9, Batch 87/141, Train Loss: 0.0239, LR: 9.15e-04\n",
      "Epoch 9, Batch 116/141, Train Loss: 0.0165, LR: 9.02e-04\n",
      "Epoch 9/20 — Train Loss: 0.0099\n",
      "Epoch 9 — Val Loss: 0.1093, Val Recall: 0.9813 (Acc: 0.9806, P: 0.9800, R: 0.9813, F1: 0.9806, ROC: 0.9967, PR: 0.9968)\n",
      "Epoch 10, Batch 0/141, Train Loss: 0.0064, LR: 8.90e-04\n",
      "Epoch 10, Batch 29/141, Train Loss: 0.0025, LR: 8.75e-04\n",
      "Epoch 10, Batch 58/141, Train Loss: 0.0015, LR: 8.59e-04\n",
      "Epoch 10, Batch 87/141, Train Loss: 0.0025, LR: 8.43e-04\n",
      "Epoch 10, Batch 116/141, Train Loss: 0.0015, LR: 8.26e-04\n",
      "Epoch 10/20 — Train Loss: 0.0056\n",
      "Epoch 10 — Val Loss: 0.1225, Val Recall: 0.9786 (Acc: 0.9815, P: 0.9843, R: 0.9786, F1: 0.9814, ROC: 0.9968, PR: 0.9971)\n",
      "Early stopping triggered after 10 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 5 with Val Loss: 0.0966\n",
      "\n",
      "Training finished. Total time: 137.28s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 5:\n",
      "  - Accuracy: 0.9793\n",
      "  - Precision: 0.9834\n",
      "  - Recall: 0.9750\n",
      "  - F1: 0.9792\n",
      "  - Roc_auc: 0.9965\n",
      "  - Pr_auc: 0.9960\n",
      "  - Val_loss: 0.0966\n",
      "Fold 3 finished. Val Recall: 0.9750 (from best model by val_loss within fold)\n",
      "\n",
      "--- Repetition 2/5, Fold 2/2 (Overall Fold 4) ---\n",
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Using explicitly provided training and validation DataFrames.\n",
      "Training set size: 8987, Validation set size: 8987\n",
      "Training set class distribution: {0: 4500, 1: 4487}\n",
      "Validation set class distribution: {0: 4500, 1: 4487}\n",
      "Using class weights: [0.99855556 1.00144863]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/141, Train Loss: 0.7173, LR: 4.00e-05\n",
      "Epoch 1, Batch 29/141, Train Loss: 0.6771, LR: 4.30e-05\n",
      "Epoch 1, Batch 58/141, Train Loss: 0.6559, LR: 5.15e-05\n",
      "Epoch 1, Batch 87/141, Train Loss: 0.5918, LR: 6.55e-05\n",
      "Epoch 1, Batch 116/141, Train Loss: 0.5166, LR: 8.47e-05\n",
      "Epoch 1/20 — Train Loss: 0.6000\n",
      "Epoch 1 — Val Loss: 0.3917, Val Recall: 0.8761 (Acc: 0.8717, P: 0.8682, R: 0.8761, F1: 0.8721, ROC: 0.9438, PR: 0.9393)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/141, Train Loss: 0.3818, LR: 1.05e-04\n",
      "Epoch 2, Batch 29/141, Train Loss: 0.3523, LR: 1.34e-04\n",
      "Epoch 2, Batch 58/141, Train Loss: 0.2719, LR: 1.67e-04\n",
      "Epoch 2, Batch 87/141, Train Loss: 0.2526, LR: 2.04e-04\n",
      "Epoch 2, Batch 116/141, Train Loss: 0.1365, LR: 2.44e-04\n",
      "Epoch 2/20 — Train Loss: 0.2461\n",
      "Epoch 2 — Val Loss: 0.1299, Val Recall: 0.9661 (Acc: 0.9574, P: 0.9494, R: 0.9661, F1: 0.9577, ROC: 0.9903, PR: 0.9879)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/141, Train Loss: 0.1612, LR: 2.82e-04\n",
      "Epoch 3, Batch 29/141, Train Loss: 0.1234, LR: 3.28e-04\n",
      "Epoch 3, Batch 58/141, Train Loss: 0.1036, LR: 3.77e-04\n",
      "Epoch 3, Batch 87/141, Train Loss: 0.1198, LR: 4.27e-04\n",
      "Epoch 3, Batch 116/141, Train Loss: 0.1440, LR: 4.78e-04\n",
      "Epoch 3/20 — Train Loss: 0.1068\n",
      "Epoch 3 — Val Loss: 0.0864, Val Recall: 0.9746 (Acc: 0.9717, P: 0.9690, R: 0.9746, F1: 0.9718, ROC: 0.9953, PR: 0.9947)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/141, Train Loss: 0.0778, LR: 5.23e-04\n",
      "Epoch 4, Batch 29/141, Train Loss: 0.0631, LR: 5.74e-04\n",
      "Epoch 4, Batch 58/141, Train Loss: 0.0379, LR: 6.25e-04\n",
      "Epoch 4, Batch 87/141, Train Loss: 0.0704, LR: 6.75e-04\n",
      "Epoch 4, Batch 116/141, Train Loss: 0.0434, LR: 7.23e-04\n",
      "Epoch 4/20 — Train Loss: 0.0714\n",
      "Epoch 4 — Val Loss: 0.0702, Val Recall: 0.9840 (Acc: 0.9770, P: 0.9703, R: 0.9840, F1: 0.9771, ROC: 0.9966, PR: 0.9963)\n",
      "  New best model (by val_loss) saved at epoch 4.\n",
      "Epoch 5, Batch 0/141, Train Loss: 0.0066, LR: 7.63e-04\n",
      "Epoch 5, Batch 29/141, Train Loss: 0.1149, LR: 8.06e-04\n",
      "Epoch 5, Batch 58/141, Train Loss: 0.0942, LR: 8.46e-04\n",
      "Epoch 5, Batch 87/141, Train Loss: 0.0226, LR: 8.82e-04\n",
      "Epoch 5, Batch 116/141, Train Loss: 0.0321, LR: 9.14e-04\n",
      "Epoch 5/20 — Train Loss: 0.0487\n",
      "Epoch 5 — Val Loss: 0.0705, Val Recall: 0.9869 (Acc: 0.9797, P: 0.9730, R: 0.9869, F1: 0.9799, ROC: 0.9973, PR: 0.9972)\n",
      "Epoch 6, Batch 0/141, Train Loss: 0.0081, LR: 9.37e-04\n",
      "Epoch 6, Batch 29/141, Train Loss: 0.0142, LR: 9.60e-04\n",
      "Epoch 6, Batch 58/141, Train Loss: 0.0179, LR: 9.78e-04\n",
      "Epoch 6, Batch 87/141, Train Loss: 0.0094, LR: 9.91e-04\n",
      "Epoch 6, Batch 116/141, Train Loss: 0.0816, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0312\n",
      "Epoch 6 — Val Loss: 0.0581, Val Recall: 0.9846 (Acc: 0.9824, P: 0.9803, R: 0.9846, F1: 0.9824, ROC: 0.9977, PR: 0.9978)\n",
      "  New best model (by val_loss) saved at epoch 6.\n",
      "Epoch 7, Batch 0/141, Train Loss: 0.0118, LR: 1.00e-03\n",
      "Epoch 7, Batch 29/141, Train Loss: 0.0147, LR: 9.99e-04\n",
      "Epoch 7, Batch 58/141, Train Loss: 0.0203, LR: 9.98e-04\n",
      "Epoch 7, Batch 87/141, Train Loss: 0.0620, LR: 9.95e-04\n",
      "Epoch 7, Batch 116/141, Train Loss: 0.0049, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0204\n",
      "Epoch 7 — Val Loss: 0.0665, Val Recall: 0.9724 (Acc: 0.9794, P: 0.9862, R: 0.9724, F1: 0.9792, ROC: 0.9974, PR: 0.9974)\n",
      "Epoch 8, Batch 0/141, Train Loss: 0.0028, LR: 9.87e-04\n",
      "Epoch 8, Batch 29/141, Train Loss: 0.0031, LR: 9.81e-04\n",
      "Epoch 8, Batch 58/141, Train Loss: 0.0059, LR: 9.75e-04\n",
      "Epoch 8, Batch 87/141, Train Loss: 0.0044, LR: 9.67e-04\n",
      "Epoch 8, Batch 116/141, Train Loss: 0.0094, LR: 9.58e-04\n",
      "Epoch 8/20 — Train Loss: 0.0128\n",
      "Epoch 8 — Val Loss: 0.0637, Val Recall: 0.9846 (Acc: 0.9820, P: 0.9794, R: 0.9846, F1: 0.9820, ROC: 0.9977, PR: 0.9979)\n",
      "Epoch 9, Batch 0/141, Train Loss: 0.0090, LR: 9.50e-04\n",
      "Epoch 9, Batch 29/141, Train Loss: 0.0179, LR: 9.39e-04\n",
      "Epoch 9, Batch 58/141, Train Loss: 0.0050, LR: 9.28e-04\n",
      "Epoch 9, Batch 87/141, Train Loss: 0.0011, LR: 9.15e-04\n",
      "Epoch 9, Batch 116/141, Train Loss: 0.0053, LR: 9.02e-04\n",
      "Epoch 9/20 — Train Loss: 0.0071\n",
      "Epoch 9 — Val Loss: 0.0763, Val Recall: 0.9795 (Acc: 0.9814, P: 0.9832, R: 0.9795, F1: 0.9814, ROC: 0.9976, PR: 0.9976)\n",
      "Epoch 10, Batch 0/141, Train Loss: 0.0221, LR: 8.90e-04\n",
      "Epoch 10, Batch 29/141, Train Loss: 0.0016, LR: 8.75e-04\n",
      "Epoch 10, Batch 58/141, Train Loss: 0.0015, LR: 8.59e-04\n",
      "Epoch 10, Batch 87/141, Train Loss: 0.0304, LR: 8.43e-04\n",
      "Epoch 10, Batch 116/141, Train Loss: 0.0022, LR: 8.26e-04\n",
      "Epoch 10/20 — Train Loss: 0.0053\n",
      "Epoch 10 — Val Loss: 0.0794, Val Recall: 0.9848 (Acc: 0.9811, P: 0.9774, R: 0.9848, F1: 0.9811, ROC: 0.9971, PR: 0.9970)\n",
      "Epoch 11, Batch 0/141, Train Loss: 0.0122, LR: 8.10e-04\n",
      "Epoch 11, Batch 29/141, Train Loss: 0.0049, LR: 7.92e-04\n",
      "Epoch 11, Batch 58/141, Train Loss: 0.0003, LR: 7.73e-04\n",
      "Epoch 11, Batch 87/141, Train Loss: 0.0062, LR: 7.53e-04\n",
      "Epoch 11, Batch 116/141, Train Loss: 0.0011, LR: 7.33e-04\n",
      "Epoch 11/20 — Train Loss: 0.0034\n",
      "Epoch 11 — Val Loss: 0.0824, Val Recall: 0.9819 (Acc: 0.9815, P: 0.9811, R: 0.9819, F1: 0.9815, ROC: 0.9972, PR: 0.9972)\n",
      "Early stopping triggered after 11 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 6 with Val Loss: 0.0581\n",
      "\n",
      "Training finished. Total time: 149.92s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 6:\n",
      "  - Accuracy: 0.9824\n",
      "  - Precision: 0.9803\n",
      "  - Recall: 0.9846\n",
      "  - F1: 0.9824\n",
      "  - Roc_auc: 0.9977\n",
      "  - Pr_auc: 0.9978\n",
      "  - Val_loss: 0.0581\n",
      "Fold 4 finished. Val Recall: 0.9846 (from best model by val_loss within fold)\n",
      "  *** New overall best model found based on RECALL across folds! Score: 0.9846 ***\n",
      "\n",
      "--- Repetition 3/5, Fold 1/2 (Overall Fold 5) ---\n",
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Using explicitly provided training and validation DataFrames.\n",
      "Training set size: 8987, Validation set size: 8987\n",
      "Training set class distribution: {0: 4500, 1: 4487}\n",
      "Validation set class distribution: {0: 4500, 1: 4487}\n",
      "Using class weights: [0.99855556 1.00144863]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/141, Train Loss: 0.7067, LR: 4.00e-05\n",
      "Epoch 1, Batch 29/141, Train Loss: 0.6532, LR: 4.30e-05\n",
      "Epoch 1, Batch 58/141, Train Loss: 0.6453, LR: 5.15e-05\n",
      "Epoch 1, Batch 87/141, Train Loss: 0.5677, LR: 6.55e-05\n",
      "Epoch 1, Batch 116/141, Train Loss: 0.5065, LR: 8.47e-05\n",
      "Epoch 1/20 — Train Loss: 0.5856\n",
      "Epoch 1 — Val Loss: 0.3884, Val Recall: 0.8647 (Acc: 0.8526, P: 0.8438, R: 0.8647, F1: 0.8542, ROC: 0.9332, PR: 0.9269)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/141, Train Loss: 0.4370, LR: 1.05e-04\n",
      "Epoch 2, Batch 29/141, Train Loss: 0.3737, LR: 1.34e-04\n",
      "Epoch 2, Batch 58/141, Train Loss: 0.2217, LR: 1.67e-04\n",
      "Epoch 2, Batch 87/141, Train Loss: 0.1293, LR: 2.04e-04\n",
      "Epoch 2, Batch 116/141, Train Loss: 0.1485, LR: 2.44e-04\n",
      "Epoch 2/20 — Train Loss: 0.2418\n",
      "Epoch 2 — Val Loss: 0.1347, Val Recall: 0.9646 (Acc: 0.9553, P: 0.9468, R: 0.9646, F1: 0.9556, ROC: 0.9890, PR: 0.9855)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/141, Train Loss: 0.1272, LR: 2.82e-04\n",
      "Epoch 3, Batch 29/141, Train Loss: 0.1166, LR: 3.28e-04\n",
      "Epoch 3, Batch 58/141, Train Loss: 0.1469, LR: 3.77e-04\n",
      "Epoch 3, Batch 87/141, Train Loss: 0.1106, LR: 4.27e-04\n",
      "Epoch 3, Batch 116/141, Train Loss: 0.0830, LR: 4.78e-04\n",
      "Epoch 3/20 — Train Loss: 0.1090\n",
      "Epoch 3 — Val Loss: 0.0981, Val Recall: 0.9815 (Acc: 0.9655, P: 0.9510, R: 0.9815, F1: 0.9660, ROC: 0.9944, PR: 0.9927)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/141, Train Loss: 0.1321, LR: 5.23e-04\n",
      "Epoch 4, Batch 29/141, Train Loss: 0.0642, LR: 5.74e-04\n",
      "Epoch 4, Batch 58/141, Train Loss: 0.0782, LR: 6.25e-04\n",
      "Epoch 4, Batch 87/141, Train Loss: 0.0474, LR: 6.75e-04\n",
      "Epoch 4, Batch 116/141, Train Loss: 0.0878, LR: 7.23e-04\n",
      "Epoch 4/20 — Train Loss: 0.0708\n",
      "Epoch 4 — Val Loss: 0.0769, Val Recall: 0.9753 (Acc: 0.9756, P: 0.9759, R: 0.9753, F1: 0.9756, ROC: 0.9958, PR: 0.9954)\n",
      "  New best model (by val_loss) saved at epoch 4.\n",
      "Epoch 5, Batch 0/141, Train Loss: 0.0229, LR: 7.63e-04\n",
      "Epoch 5, Batch 29/141, Train Loss: 0.1488, LR: 8.06e-04\n",
      "Epoch 5, Batch 58/141, Train Loss: 0.0571, LR: 8.46e-04\n",
      "Epoch 5, Batch 87/141, Train Loss: 0.0131, LR: 8.82e-04\n",
      "Epoch 5, Batch 116/141, Train Loss: 0.0122, LR: 9.14e-04\n",
      "Epoch 5/20 — Train Loss: 0.0494\n",
      "Epoch 5 — Val Loss: 0.0684, Val Recall: 0.9828 (Acc: 0.9794, P: 0.9761, R: 0.9828, F1: 0.9795, ROC: 0.9968, PR: 0.9971)\n",
      "  New best model (by val_loss) saved at epoch 5.\n",
      "Epoch 6, Batch 0/141, Train Loss: 0.0963, LR: 9.37e-04\n",
      "Epoch 6, Batch 29/141, Train Loss: 0.0533, LR: 9.60e-04\n",
      "Epoch 6, Batch 58/141, Train Loss: 0.0164, LR: 9.78e-04\n",
      "Epoch 6, Batch 87/141, Train Loss: 0.0122, LR: 9.91e-04\n",
      "Epoch 6, Batch 116/141, Train Loss: 0.0857, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0348\n",
      "Epoch 6 — Val Loss: 0.0805, Val Recall: 0.9857 (Acc: 0.9765, P: 0.9678, R: 0.9857, F1: 0.9767, ROC: 0.9967, PR: 0.9971)\n",
      "Epoch 7, Batch 0/141, Train Loss: 0.0086, LR: 1.00e-03\n",
      "Epoch 7, Batch 29/141, Train Loss: 0.0044, LR: 9.99e-04\n",
      "Epoch 7, Batch 58/141, Train Loss: 0.1000, LR: 9.98e-04\n",
      "Epoch 7, Batch 87/141, Train Loss: 0.0087, LR: 9.95e-04\n",
      "Epoch 7, Batch 116/141, Train Loss: 0.0210, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0219\n",
      "Epoch 7 — Val Loss: 0.0803, Val Recall: 0.9848 (Acc: 0.9806, P: 0.9766, R: 0.9848, F1: 0.9807, ROC: 0.9970, PR: 0.9974)\n",
      "Epoch 8, Batch 0/141, Train Loss: 0.0531, LR: 9.87e-04\n",
      "Epoch 8, Batch 29/141, Train Loss: 0.0044, LR: 9.81e-04\n",
      "Epoch 8, Batch 58/141, Train Loss: 0.0033, LR: 9.75e-04\n",
      "Epoch 8, Batch 87/141, Train Loss: 0.0060, LR: 9.67e-04\n",
      "Epoch 8, Batch 116/141, Train Loss: 0.0070, LR: 9.58e-04\n",
      "Epoch 8/20 — Train Loss: 0.0153\n",
      "Epoch 8 — Val Loss: 0.0777, Val Recall: 0.9835 (Acc: 0.9803, P: 0.9772, R: 0.9835, F1: 0.9803, ROC: 0.9971, PR: 0.9974)\n",
      "Epoch 9, Batch 0/141, Train Loss: 0.0308, LR: 9.50e-04\n",
      "Epoch 9, Batch 29/141, Train Loss: 0.0017, LR: 9.39e-04\n",
      "Epoch 9, Batch 58/141, Train Loss: 0.0046, LR: 9.28e-04\n",
      "Epoch 9, Batch 87/141, Train Loss: 0.0054, LR: 9.15e-04\n",
      "Epoch 9, Batch 116/141, Train Loss: 0.0019, LR: 9.02e-04\n",
      "Epoch 9/20 — Train Loss: 0.0090\n",
      "Epoch 9 — Val Loss: 0.0937, Val Recall: 0.9860 (Acc: 0.9801, P: 0.9744, R: 0.9860, F1: 0.9802, ROC: 0.9966, PR: 0.9969)\n",
      "Epoch 10, Batch 0/141, Train Loss: 0.0262, LR: 8.90e-04\n",
      "Epoch 10, Batch 29/141, Train Loss: 0.0149, LR: 8.75e-04\n",
      "Epoch 10, Batch 58/141, Train Loss: 0.0093, LR: 8.59e-04\n",
      "Epoch 10, Batch 87/141, Train Loss: 0.0148, LR: 8.43e-04\n",
      "Epoch 10, Batch 116/141, Train Loss: 0.0011, LR: 8.26e-04\n",
      "Epoch 10/20 — Train Loss: 0.0067\n",
      "Epoch 10 — Val Loss: 0.0929, Val Recall: 0.9853 (Acc: 0.9802, P: 0.9753, R: 0.9853, F1: 0.9803, ROC: 0.9969, PR: 0.9973)\n",
      "Early stopping triggered after 10 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 5 with Val Loss: 0.0684\n",
      "\n",
      "Training finished. Total time: 137.50s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 5:\n",
      "  - Accuracy: 0.9794\n",
      "  - Precision: 0.9761\n",
      "  - Recall: 0.9828\n",
      "  - F1: 0.9795\n",
      "  - Roc_auc: 0.9968\n",
      "  - Pr_auc: 0.9971\n",
      "  - Val_loss: 0.0684\n",
      "Fold 5 finished. Val Recall: 0.9828 (from best model by val_loss within fold)\n",
      "\n",
      "--- Repetition 3/5, Fold 2/2 (Overall Fold 6) ---\n",
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Using explicitly provided training and validation DataFrames.\n",
      "Training set size: 8987, Validation set size: 8987\n",
      "Training set class distribution: {0: 4500, 1: 4487}\n",
      "Validation set class distribution: {0: 4500, 1: 4487}\n",
      "Using class weights: [0.99855556 1.00144863]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/141, Train Loss: 0.7123, LR: 4.00e-05\n",
      "Epoch 1, Batch 29/141, Train Loss: 0.6669, LR: 4.30e-05\n",
      "Epoch 1, Batch 58/141, Train Loss: 0.6567, LR: 5.15e-05\n",
      "Epoch 1, Batch 87/141, Train Loss: 0.6055, LR: 6.55e-05\n",
      "Epoch 1, Batch 116/141, Train Loss: 0.5300, LR: 8.47e-05\n",
      "Epoch 1/20 — Train Loss: 0.6012\n",
      "Epoch 1 — Val Loss: 0.4077, Val Recall: 0.8883 (Acc: 0.8735, P: 0.8624, R: 0.8883, F1: 0.8752, ROC: 0.9472, PR: 0.9463)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/141, Train Loss: 0.4468, LR: 1.05e-04\n",
      "Epoch 2, Batch 29/141, Train Loss: 0.4191, LR: 1.34e-04\n",
      "Epoch 2, Batch 58/141, Train Loss: 0.2015, LR: 1.67e-04\n",
      "Epoch 2, Batch 87/141, Train Loss: 0.2337, LR: 2.04e-04\n",
      "Epoch 2, Batch 116/141, Train Loss: 0.1573, LR: 2.44e-04\n",
      "Epoch 2/20 — Train Loss: 0.2604\n",
      "Epoch 2 — Val Loss: 0.1363, Val Recall: 0.9481 (Acc: 0.9573, P: 0.9657, R: 0.9481, F1: 0.9568, ROC: 0.9915, PR: 0.9898)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/141, Train Loss: 0.1408, LR: 2.82e-04\n",
      "Epoch 3, Batch 29/141, Train Loss: 0.0894, LR: 3.28e-04\n",
      "Epoch 3, Batch 58/141, Train Loss: 0.0888, LR: 3.77e-04\n",
      "Epoch 3, Batch 87/141, Train Loss: 0.0865, LR: 4.27e-04\n",
      "Epoch 3, Batch 116/141, Train Loss: 0.1122, LR: 4.78e-04\n",
      "Epoch 3/20 — Train Loss: 0.1036\n",
      "Epoch 3 — Val Loss: 0.0818, Val Recall: 0.9746 (Acc: 0.9752, P: 0.9757, R: 0.9746, F1: 0.9751, ROC: 0.9957, PR: 0.9944)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/141, Train Loss: 0.0857, LR: 5.23e-04\n",
      "Epoch 4, Batch 29/141, Train Loss: 0.0738, LR: 5.74e-04\n",
      "Epoch 4, Batch 58/141, Train Loss: 0.0254, LR: 6.25e-04\n",
      "Epoch 4, Batch 87/141, Train Loss: 0.0099, LR: 6.75e-04\n",
      "Epoch 4, Batch 116/141, Train Loss: 0.0385, LR: 7.23e-04\n",
      "Epoch 4/20 — Train Loss: 0.0670\n",
      "Epoch 4 — Val Loss: 0.1065, Val Recall: 0.9412 (Acc: 0.9648, P: 0.9878, R: 0.9412, F1: 0.9639, ROC: 0.9957, PR: 0.9944)\n",
      "Epoch 5, Batch 0/141, Train Loss: 0.0687, LR: 7.63e-04\n",
      "Epoch 5, Batch 29/141, Train Loss: 0.0164, LR: 8.06e-04\n",
      "Epoch 5, Batch 58/141, Train Loss: 0.0771, LR: 8.46e-04\n",
      "Epoch 5, Batch 87/141, Train Loss: 0.0101, LR: 8.82e-04\n",
      "Epoch 5, Batch 116/141, Train Loss: 0.0666, LR: 9.14e-04\n",
      "Epoch 5/20 — Train Loss: 0.0490\n",
      "Epoch 5 — Val Loss: 0.0638, Val Recall: 0.9762 (Acc: 0.9803, P: 0.9843, R: 0.9762, F1: 0.9802, ROC: 0.9976, PR: 0.9973)\n",
      "  New best model (by val_loss) saved at epoch 5.\n",
      "Epoch 6, Batch 0/141, Train Loss: 0.0332, LR: 9.37e-04\n",
      "Epoch 6, Batch 29/141, Train Loss: 0.0238, LR: 9.60e-04\n",
      "Epoch 6, Batch 58/141, Train Loss: 0.0034, LR: 9.78e-04\n",
      "Epoch 6, Batch 87/141, Train Loss: 0.0346, LR: 9.91e-04\n",
      "Epoch 6, Batch 116/141, Train Loss: 0.0394, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0332\n",
      "Epoch 6 — Val Loss: 0.0604, Val Recall: 0.9779 (Acc: 0.9809, P: 0.9836, R: 0.9779, F1: 0.9808, ROC: 0.9979, PR: 0.9977)\n",
      "  New best model (by val_loss) saved at epoch 6.\n",
      "Epoch 7, Batch 0/141, Train Loss: 0.0276, LR: 1.00e-03\n",
      "Epoch 7, Batch 29/141, Train Loss: 0.0278, LR: 9.99e-04\n",
      "Epoch 7, Batch 58/141, Train Loss: 0.0017, LR: 9.98e-04\n",
      "Epoch 7, Batch 87/141, Train Loss: 0.0031, LR: 9.95e-04\n",
      "Epoch 7, Batch 116/141, Train Loss: 0.0109, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0201\n",
      "Epoch 7 — Val Loss: 0.0657, Val Recall: 0.9770 (Acc: 0.9810, P: 0.9847, R: 0.9770, F1: 0.9809, ROC: 0.9979, PR: 0.9979)\n",
      "Epoch 8, Batch 0/141, Train Loss: 0.0311, LR: 9.87e-04\n",
      "Epoch 8, Batch 29/141, Train Loss: 0.0153, LR: 9.81e-04\n",
      "Epoch 8, Batch 58/141, Train Loss: 0.0143, LR: 9.75e-04\n",
      "Epoch 8, Batch 87/141, Train Loss: 0.0479, LR: 9.67e-04\n",
      "Epoch 8, Batch 116/141, Train Loss: 0.0538, LR: 9.58e-04\n",
      "Epoch 8/20 — Train Loss: 0.0125\n",
      "Epoch 8 — Val Loss: 0.0699, Val Recall: 0.9748 (Acc: 0.9818, P: 0.9885, R: 0.9748, F1: 0.9816, ROC: 0.9981, PR: 0.9980)\n",
      "Epoch 9, Batch 0/141, Train Loss: 0.0298, LR: 9.50e-04\n",
      "Epoch 9, Batch 29/141, Train Loss: 0.0003, LR: 9.39e-04\n",
      "Epoch 9, Batch 58/141, Train Loss: 0.0064, LR: 9.28e-04\n",
      "Epoch 9, Batch 87/141, Train Loss: 0.0008, LR: 9.15e-04\n",
      "Epoch 9, Batch 116/141, Train Loss: 0.0130, LR: 9.02e-04\n",
      "Epoch 9/20 — Train Loss: 0.0079\n",
      "Epoch 9 — Val Loss: 0.0743, Val Recall: 0.9815 (Acc: 0.9824, P: 0.9833, R: 0.9815, F1: 0.9824, ROC: 0.9979, PR: 0.9978)\n",
      "Epoch 10, Batch 0/141, Train Loss: 0.0044, LR: 8.90e-04\n",
      "Epoch 10, Batch 29/141, Train Loss: 0.0016, LR: 8.75e-04\n",
      "Epoch 10, Batch 58/141, Train Loss: 0.0040, LR: 8.59e-04\n",
      "Epoch 10, Batch 87/141, Train Loss: 0.0004, LR: 8.43e-04\n",
      "Epoch 10, Batch 116/141, Train Loss: 0.0014, LR: 8.26e-04\n",
      "Epoch 10/20 — Train Loss: 0.0046\n",
      "Epoch 10 — Val Loss: 0.0818, Val Recall: 0.9791 (Acc: 0.9824, P: 0.9856, R: 0.9791, F1: 0.9823, ROC: 0.9979, PR: 0.9978)\n",
      "Epoch 11, Batch 0/141, Train Loss: 0.0035, LR: 8.10e-04\n",
      "Epoch 11, Batch 29/141, Train Loss: 0.0007, LR: 7.92e-04\n",
      "Epoch 11, Batch 58/141, Train Loss: 0.0043, LR: 7.73e-04\n",
      "Epoch 11, Batch 87/141, Train Loss: 0.0016, LR: 7.53e-04\n",
      "Epoch 11, Batch 116/141, Train Loss: 0.0004, LR: 7.33e-04\n",
      "Epoch 11/20 — Train Loss: 0.0029\n",
      "Epoch 11 — Val Loss: 0.0851, Val Recall: 0.9782 (Acc: 0.9818, P: 0.9852, R: 0.9782, F1: 0.9817, ROC: 0.9977, PR: 0.9976)\n",
      "Early stopping triggered after 11 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 6 with Val Loss: 0.0604\n",
      "\n",
      "Training finished. Total time: 149.95s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 6:\n",
      "  - Accuracy: 0.9809\n",
      "  - Precision: 0.9836\n",
      "  - Recall: 0.9779\n",
      "  - F1: 0.9808\n",
      "  - Roc_auc: 0.9979\n",
      "  - Pr_auc: 0.9977\n",
      "  - Val_loss: 0.0604\n",
      "Fold 6 finished. Val Recall: 0.9779 (from best model by val_loss within fold)\n",
      "\n",
      "--- Repetition 4/5, Fold 1/2 (Overall Fold 7) ---\n",
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Using explicitly provided training and validation DataFrames.\n",
      "Training set size: 8987, Validation set size: 8987\n",
      "Training set class distribution: {0: 4500, 1: 4487}\n",
      "Validation set class distribution: {0: 4500, 1: 4487}\n",
      "Using class weights: [0.99855556 1.00144863]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/141, Train Loss: 0.7003, LR: 4.00e-05\n",
      "Epoch 1, Batch 29/141, Train Loss: 0.6661, LR: 4.30e-05\n",
      "Epoch 1, Batch 58/141, Train Loss: 0.6228, LR: 5.15e-05\n",
      "Epoch 1, Batch 87/141, Train Loss: 0.6182, LR: 6.55e-05\n",
      "Epoch 1, Batch 116/141, Train Loss: 0.5102, LR: 8.47e-05\n",
      "Epoch 1/20 — Train Loss: 0.6091\n",
      "Epoch 1 — Val Loss: 0.4193, Val Recall: 0.8629 (Acc: 0.8752, P: 0.8842, R: 0.8629, F1: 0.8734, ROC: 0.9504, PR: 0.9465)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/141, Train Loss: 0.5358, LR: 1.05e-04\n",
      "Epoch 2, Batch 29/141, Train Loss: 0.3450, LR: 1.34e-04\n",
      "Epoch 2, Batch 58/141, Train Loss: 0.3327, LR: 1.67e-04\n",
      "Epoch 2, Batch 87/141, Train Loss: 0.1844, LR: 2.04e-04\n",
      "Epoch 2, Batch 116/141, Train Loss: 0.1564, LR: 2.44e-04\n",
      "Epoch 2/20 — Train Loss: 0.2574\n",
      "Epoch 2 — Val Loss: 0.1239, Val Recall: 0.9710 (Acc: 0.9612, P: 0.9521, R: 0.9710, F1: 0.9615, ROC: 0.9915, PR: 0.9878)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/141, Train Loss: 0.1008, LR: 2.82e-04\n",
      "Epoch 3, Batch 29/141, Train Loss: 0.0765, LR: 3.28e-04\n",
      "Epoch 3, Batch 58/141, Train Loss: 0.1513, LR: 3.77e-04\n",
      "Epoch 3, Batch 87/141, Train Loss: 0.0876, LR: 4.27e-04\n",
      "Epoch 3, Batch 116/141, Train Loss: 0.1818, LR: 4.78e-04\n",
      "Epoch 3/20 — Train Loss: 0.1077\n",
      "Epoch 3 — Val Loss: 0.0961, Val Recall: 0.9777 (Acc: 0.9724, P: 0.9674, R: 0.9777, F1: 0.9725, ROC: 0.9957, PR: 0.9941)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/141, Train Loss: 0.0585, LR: 5.23e-04\n",
      "Epoch 4, Batch 29/141, Train Loss: 0.1084, LR: 5.74e-04\n",
      "Epoch 4, Batch 58/141, Train Loss: 0.1157, LR: 6.25e-04\n",
      "Epoch 4, Batch 87/141, Train Loss: 0.2015, LR: 6.75e-04\n",
      "Epoch 4, Batch 116/141, Train Loss: 0.0313, LR: 7.23e-04\n",
      "Epoch 4/20 — Train Loss: 0.0672\n",
      "Epoch 4 — Val Loss: 0.0885, Val Recall: 0.9786 (Acc: 0.9766, P: 0.9747, R: 0.9786, F1: 0.9766, ROC: 0.9969, PR: 0.9959)\n",
      "  New best model (by val_loss) saved at epoch 4.\n",
      "Epoch 5, Batch 0/141, Train Loss: 0.0737, LR: 7.63e-04\n",
      "Epoch 5, Batch 29/141, Train Loss: 0.0103, LR: 8.06e-04\n",
      "Epoch 5, Batch 58/141, Train Loss: 0.0242, LR: 8.46e-04\n",
      "Epoch 5, Batch 87/141, Train Loss: 0.0677, LR: 8.82e-04\n",
      "Epoch 5, Batch 116/141, Train Loss: 0.0248, LR: 9.14e-04\n",
      "Epoch 5/20 — Train Loss: 0.0488\n",
      "Epoch 5 — Val Loss: 0.0829, Val Recall: 0.9721 (Acc: 0.9785, P: 0.9847, R: 0.9721, F1: 0.9784, ROC: 0.9972, PR: 0.9965)\n",
      "  New best model (by val_loss) saved at epoch 5.\n",
      "Epoch 6, Batch 0/141, Train Loss: 0.0245, LR: 9.37e-04\n",
      "Epoch 6, Batch 29/141, Train Loss: 0.0404, LR: 9.60e-04\n",
      "Epoch 6, Batch 58/141, Train Loss: 0.0297, LR: 9.78e-04\n",
      "Epoch 6, Batch 87/141, Train Loss: 0.0059, LR: 9.91e-04\n",
      "Epoch 6, Batch 116/141, Train Loss: 0.0220, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0360\n",
      "Epoch 6 — Val Loss: 0.0757, Val Recall: 0.9784 (Acc: 0.9818, P: 0.9850, R: 0.9784, F1: 0.9817, ROC: 0.9980, PR: 0.9978)\n",
      "  New best model (by val_loss) saved at epoch 6.\n",
      "Epoch 7, Batch 0/141, Train Loss: 0.0100, LR: 1.00e-03\n",
      "Epoch 7, Batch 29/141, Train Loss: 0.0083, LR: 9.99e-04\n",
      "Epoch 7, Batch 58/141, Train Loss: 0.0304, LR: 9.98e-04\n",
      "Epoch 7, Batch 87/141, Train Loss: 0.0167, LR: 9.95e-04\n",
      "Epoch 7, Batch 116/141, Train Loss: 0.0128, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0240\n",
      "Epoch 7 — Val Loss: 0.0781, Val Recall: 0.9804 (Acc: 0.9832, P: 0.9859, R: 0.9804, F1: 0.9831, ROC: 0.9980, PR: 0.9979)\n",
      "Epoch 8, Batch 0/141, Train Loss: 0.0024, LR: 9.87e-04\n",
      "Epoch 8, Batch 29/141, Train Loss: 0.0229, LR: 9.81e-04\n",
      "Epoch 8, Batch 58/141, Train Loss: 0.0402, LR: 9.75e-04\n",
      "Epoch 8, Batch 87/141, Train Loss: 0.0038, LR: 9.67e-04\n",
      "Epoch 8, Batch 116/141, Train Loss: 0.0054, LR: 9.58e-04\n",
      "Epoch 8/20 — Train Loss: 0.0146\n",
      "Epoch 8 — Val Loss: 0.0893, Val Recall: 0.9804 (Acc: 0.9832, P: 0.9859, R: 0.9804, F1: 0.9831, ROC: 0.9979, PR: 0.9979)\n",
      "Epoch 9, Batch 0/141, Train Loss: 0.0025, LR: 9.50e-04\n",
      "Epoch 9, Batch 29/141, Train Loss: 0.0026, LR: 9.39e-04\n",
      "Epoch 9, Batch 58/141, Train Loss: 0.0102, LR: 9.28e-04\n",
      "Epoch 9, Batch 87/141, Train Loss: 0.0176, LR: 9.15e-04\n",
      "Epoch 9, Batch 116/141, Train Loss: 0.0017, LR: 9.02e-04\n",
      "Epoch 9/20 — Train Loss: 0.0097\n",
      "Epoch 9 — Val Loss: 0.0885, Val Recall: 0.9788 (Acc: 0.9831, P: 0.9872, R: 0.9788, F1: 0.9830, ROC: 0.9978, PR: 0.9978)\n",
      "Epoch 10, Batch 0/141, Train Loss: 0.0035, LR: 8.90e-04\n",
      "Epoch 10, Batch 29/141, Train Loss: 0.0044, LR: 8.75e-04\n",
      "Epoch 10, Batch 58/141, Train Loss: 0.0073, LR: 8.59e-04\n",
      "Epoch 10, Batch 87/141, Train Loss: 0.0059, LR: 8.43e-04\n",
      "Epoch 10, Batch 116/141, Train Loss: 0.0024, LR: 8.26e-04\n",
      "Epoch 10/20 — Train Loss: 0.0093\n",
      "Epoch 10 — Val Loss: 0.0875, Val Recall: 0.9802 (Acc: 0.9821, P: 0.9839, R: 0.9802, F1: 0.9820, ROC: 0.9978, PR: 0.9977)\n",
      "Epoch 11, Batch 0/141, Train Loss: 0.0011, LR: 8.10e-04\n",
      "Epoch 11, Batch 29/141, Train Loss: 0.0014, LR: 7.92e-04\n",
      "Epoch 11, Batch 58/141, Train Loss: 0.0036, LR: 7.73e-04\n",
      "Epoch 11, Batch 87/141, Train Loss: 0.0018, LR: 7.53e-04\n",
      "Epoch 11, Batch 116/141, Train Loss: 0.0018, LR: 7.33e-04\n",
      "Epoch 11/20 — Train Loss: 0.0058\n",
      "Epoch 11 — Val Loss: 0.0931, Val Recall: 0.9846 (Acc: 0.9819, P: 0.9792, R: 0.9846, F1: 0.9819, ROC: 0.9976, PR: 0.9977)\n",
      "Early stopping triggered after 11 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 6 with Val Loss: 0.0757\n",
      "\n",
      "Training finished. Total time: 150.16s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 6:\n",
      "  - Accuracy: 0.9818\n",
      "  - Precision: 0.9850\n",
      "  - Recall: 0.9784\n",
      "  - F1: 0.9817\n",
      "  - Roc_auc: 0.9980\n",
      "  - Pr_auc: 0.9978\n",
      "  - Val_loss: 0.0757\n",
      "Fold 7 finished. Val Recall: 0.9784 (from best model by val_loss within fold)\n",
      "\n",
      "--- Repetition 4/5, Fold 2/2 (Overall Fold 8) ---\n",
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Using explicitly provided training and validation DataFrames.\n",
      "Training set size: 8987, Validation set size: 8987\n",
      "Training set class distribution: {0: 4500, 1: 4487}\n",
      "Validation set class distribution: {0: 4500, 1: 4487}\n",
      "Using class weights: [0.99855556 1.00144863]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/141, Train Loss: 0.6711, LR: 4.00e-05\n",
      "Epoch 1, Batch 29/141, Train Loss: 0.6568, LR: 4.30e-05\n",
      "Epoch 1, Batch 58/141, Train Loss: 0.6009, LR: 5.15e-05\n",
      "Epoch 1, Batch 87/141, Train Loss: 0.5678, LR: 6.55e-05\n",
      "Epoch 1, Batch 116/141, Train Loss: 0.4580, LR: 8.47e-05\n",
      "Epoch 1/20 — Train Loss: 0.5716\n",
      "Epoch 1 — Val Loss: 0.3855, Val Recall: 0.9082 (Acc: 0.8720, P: 0.8467, R: 0.9082, F1: 0.8763, ROC: 0.9380, PR: 0.9331)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/141, Train Loss: 0.3942, LR: 1.05e-04\n",
      "Epoch 2, Batch 29/141, Train Loss: 0.2760, LR: 1.34e-04\n",
      "Epoch 2, Batch 58/141, Train Loss: 0.2763, LR: 1.67e-04\n",
      "Epoch 2, Batch 87/141, Train Loss: 0.1957, LR: 2.04e-04\n",
      "Epoch 2, Batch 116/141, Train Loss: 0.1322, LR: 2.44e-04\n",
      "Epoch 2/20 — Train Loss: 0.2364\n",
      "Epoch 2 — Val Loss: 0.1431, Val Recall: 0.9728 (Acc: 0.9477, P: 0.9262, R: 0.9728, F1: 0.9489, ROC: 0.9896, PR: 0.9862)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/141, Train Loss: 0.1339, LR: 2.82e-04\n",
      "Epoch 3, Batch 29/141, Train Loss: 0.1637, LR: 3.28e-04\n",
      "Epoch 3, Batch 58/141, Train Loss: 0.0900, LR: 3.77e-04\n",
      "Epoch 3, Batch 87/141, Train Loss: 0.0526, LR: 4.27e-04\n",
      "Epoch 3, Batch 116/141, Train Loss: 0.1170, LR: 4.78e-04\n",
      "Epoch 3/20 — Train Loss: 0.1021\n",
      "Epoch 3 — Val Loss: 0.0940, Val Recall: 0.9659 (Acc: 0.9696, P: 0.9731, R: 0.9659, F1: 0.9695, ROC: 0.9943, PR: 0.9933)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/141, Train Loss: 0.0532, LR: 5.23e-04\n",
      "Epoch 4, Batch 29/141, Train Loss: 0.0634, LR: 5.74e-04\n",
      "Epoch 4, Batch 58/141, Train Loss: 0.0367, LR: 6.25e-04\n",
      "Epoch 4, Batch 87/141, Train Loss: 0.0204, LR: 6.75e-04\n",
      "Epoch 4, Batch 116/141, Train Loss: 0.0908, LR: 7.23e-04\n",
      "Epoch 4/20 — Train Loss: 0.0663\n",
      "Epoch 4 — Val Loss: 0.0837, Val Recall: 0.9721 (Acc: 0.9733, P: 0.9743, R: 0.9721, F1: 0.9732, ROC: 0.9951, PR: 0.9950)\n",
      "  New best model (by val_loss) saved at epoch 4.\n",
      "Epoch 5, Batch 0/141, Train Loss: 0.0324, LR: 7.63e-04\n",
      "Epoch 5, Batch 29/141, Train Loss: 0.0374, LR: 8.06e-04\n",
      "Epoch 5, Batch 58/141, Train Loss: 0.0399, LR: 8.46e-04\n",
      "Epoch 5, Batch 87/141, Train Loss: 0.0306, LR: 8.82e-04\n",
      "Epoch 5, Batch 116/141, Train Loss: 0.0123, LR: 9.14e-04\n",
      "Epoch 5/20 — Train Loss: 0.0481\n",
      "Epoch 5 — Val Loss: 0.0663, Val Recall: 0.9724 (Acc: 0.9779, P: 0.9831, R: 0.9724, F1: 0.9777, ROC: 0.9971, PR: 0.9975)\n",
      "  New best model (by val_loss) saved at epoch 5.\n",
      "Epoch 6, Batch 0/141, Train Loss: 0.0123, LR: 9.37e-04\n",
      "Epoch 6, Batch 29/141, Train Loss: 0.0118, LR: 9.60e-04\n",
      "Epoch 6, Batch 58/141, Train Loss: 0.0129, LR: 9.78e-04\n",
      "Epoch 6, Batch 87/141, Train Loss: 0.0327, LR: 9.91e-04\n",
      "Epoch 6, Batch 116/141, Train Loss: 0.0286, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0286\n",
      "Epoch 6 — Val Loss: 0.0699, Val Recall: 0.9804 (Acc: 0.9806, P: 0.9808, R: 0.9804, F1: 0.9806, ROC: 0.9973, PR: 0.9977)\n",
      "Epoch 7, Batch 0/141, Train Loss: 0.0123, LR: 1.00e-03\n",
      "Epoch 7, Batch 29/141, Train Loss: 0.0093, LR: 9.99e-04\n",
      "Epoch 7, Batch 58/141, Train Loss: 0.0132, LR: 9.98e-04\n",
      "Epoch 7, Batch 87/141, Train Loss: 0.0049, LR: 9.95e-04\n",
      "Epoch 7, Batch 116/141, Train Loss: 0.0222, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0197\n",
      "Epoch 7 — Val Loss: 0.0673, Val Recall: 0.9813 (Acc: 0.9809, P: 0.9804, R: 0.9813, F1: 0.9808, ROC: 0.9974, PR: 0.9977)\n",
      "Epoch 8, Batch 0/141, Train Loss: 0.0273, LR: 9.87e-04\n",
      "Epoch 8, Batch 29/141, Train Loss: 0.0014, LR: 9.81e-04\n",
      "Epoch 8, Batch 58/141, Train Loss: 0.0038, LR: 9.75e-04\n",
      "Epoch 8, Batch 87/141, Train Loss: 0.0013, LR: 9.67e-04\n",
      "Epoch 8, Batch 116/141, Train Loss: 0.0286, LR: 9.58e-04\n",
      "Epoch 8/20 — Train Loss: 0.0111\n",
      "Epoch 8 — Val Loss: 0.0813, Val Recall: 0.9741 (Acc: 0.9782, P: 0.9820, R: 0.9741, F1: 0.9781, ROC: 0.9969, PR: 0.9970)\n",
      "Epoch 9, Batch 0/141, Train Loss: 0.0010, LR: 9.50e-04\n",
      "Epoch 9, Batch 29/141, Train Loss: 0.0190, LR: 9.39e-04\n",
      "Epoch 9, Batch 58/141, Train Loss: 0.0051, LR: 9.28e-04\n",
      "Epoch 9, Batch 87/141, Train Loss: 0.0028, LR: 9.15e-04\n",
      "Epoch 9, Batch 116/141, Train Loss: 0.0429, LR: 9.02e-04\n",
      "Epoch 9/20 — Train Loss: 0.0076\n",
      "Epoch 9 — Val Loss: 0.0808, Val Recall: 0.9819 (Acc: 0.9802, P: 0.9785, R: 0.9819, F1: 0.9802, ROC: 0.9972, PR: 0.9975)\n",
      "Epoch 10, Batch 0/141, Train Loss: 0.0006, LR: 8.90e-04\n",
      "Epoch 10, Batch 29/141, Train Loss: 0.0056, LR: 8.75e-04\n",
      "Epoch 10, Batch 58/141, Train Loss: 0.0114, LR: 8.59e-04\n",
      "Epoch 10, Batch 87/141, Train Loss: 0.0006, LR: 8.43e-04\n",
      "Epoch 10, Batch 116/141, Train Loss: 0.0004, LR: 8.26e-04\n",
      "Epoch 10/20 — Train Loss: 0.0045\n",
      "Epoch 10 — Val Loss: 0.0889, Val Recall: 0.9840 (Acc: 0.9802, P: 0.9766, R: 0.9840, F1: 0.9802, ROC: 0.9969, PR: 0.9972)\n",
      "Early stopping triggered after 10 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 5 with Val Loss: 0.0663\n",
      "\n",
      "Training finished. Total time: 136.07s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 5:\n",
      "  - Accuracy: 0.9779\n",
      "  - Precision: 0.9831\n",
      "  - Recall: 0.9724\n",
      "  - F1: 0.9777\n",
      "  - Roc_auc: 0.9971\n",
      "  - Pr_auc: 0.9975\n",
      "  - Val_loss: 0.0663\n",
      "Fold 8 finished. Val Recall: 0.9724 (from best model by val_loss within fold)\n",
      "\n",
      "--- Repetition 5/5, Fold 1/2 (Overall Fold 9) ---\n",
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Using explicitly provided training and validation DataFrames.\n",
      "Training set size: 8987, Validation set size: 8987\n",
      "Training set class distribution: {0: 4500, 1: 4487}\n",
      "Validation set class distribution: {0: 4500, 1: 4487}\n",
      "Using class weights: [0.99855556 1.00144863]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/141, Train Loss: 0.6914, LR: 4.00e-05\n",
      "Epoch 1, Batch 29/141, Train Loss: 0.6652, LR: 4.30e-05\n",
      "Epoch 1, Batch 58/141, Train Loss: 0.6534, LR: 5.15e-05\n",
      "Epoch 1, Batch 87/141, Train Loss: 0.5877, LR: 6.55e-05\n",
      "Epoch 1, Batch 116/141, Train Loss: 0.5167, LR: 8.47e-05\n",
      "Epoch 1/20 — Train Loss: 0.6023\n",
      "Epoch 1 — Val Loss: 0.3981, Val Recall: 0.8681 (Acc: 0.8628, P: 0.8587, R: 0.8681, F1: 0.8633, ROC: 0.9380, PR: 0.9328)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/141, Train Loss: 0.4320, LR: 1.05e-04\n",
      "Epoch 2, Batch 29/141, Train Loss: 0.3729, LR: 1.34e-04\n",
      "Epoch 2, Batch 58/141, Train Loss: 0.2907, LR: 1.67e-04\n",
      "Epoch 2, Batch 87/141, Train Loss: 0.1465, LR: 2.04e-04\n",
      "Epoch 2, Batch 116/141, Train Loss: 0.1821, LR: 2.44e-04\n",
      "Epoch 2/20 — Train Loss: 0.2424\n",
      "Epoch 2 — Val Loss: 0.1338, Val Recall: 0.9684 (Acc: 0.9538, P: 0.9409, R: 0.9684, F1: 0.9544, ROC: 0.9890, PR: 0.9892)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/141, Train Loss: 0.2108, LR: 2.82e-04\n",
      "Epoch 3, Batch 29/141, Train Loss: 0.1083, LR: 3.28e-04\n",
      "Epoch 3, Batch 58/141, Train Loss: 0.1384, LR: 3.77e-04\n",
      "Epoch 3, Batch 87/141, Train Loss: 0.0462, LR: 4.27e-04\n",
      "Epoch 3, Batch 116/141, Train Loss: 0.0581, LR: 4.78e-04\n",
      "Epoch 3/20 — Train Loss: 0.1023\n",
      "Epoch 3 — Val Loss: 0.0859, Val Recall: 0.9659 (Acc: 0.9706, P: 0.9750, R: 0.9659, F1: 0.9704, ROC: 0.9949, PR: 0.9941)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/141, Train Loss: 0.1681, LR: 5.23e-04\n",
      "Epoch 4, Batch 29/141, Train Loss: 0.0491, LR: 5.74e-04\n",
      "Epoch 4, Batch 58/141, Train Loss: 0.1320, LR: 6.25e-04\n",
      "Epoch 4, Batch 87/141, Train Loss: 0.1110, LR: 6.75e-04\n",
      "Epoch 4, Batch 116/141, Train Loss: 0.0187, LR: 7.23e-04\n",
      "Epoch 4/20 — Train Loss: 0.0666\n",
      "Epoch 4 — Val Loss: 0.0757, Val Recall: 0.9733 (Acc: 0.9766, P: 0.9798, R: 0.9733, F1: 0.9765, ROC: 0.9957, PR: 0.9948)\n",
      "  New best model (by val_loss) saved at epoch 4.\n",
      "Epoch 5, Batch 0/141, Train Loss: 0.0216, LR: 7.63e-04\n",
      "Epoch 5, Batch 29/141, Train Loss: 0.0144, LR: 8.06e-04\n",
      "Epoch 5, Batch 58/141, Train Loss: 0.0188, LR: 8.46e-04\n",
      "Epoch 5, Batch 87/141, Train Loss: 0.0130, LR: 8.82e-04\n",
      "Epoch 5, Batch 116/141, Train Loss: 0.0853, LR: 9.14e-04\n",
      "Epoch 5/20 — Train Loss: 0.0439\n",
      "Epoch 5 — Val Loss: 0.0744, Val Recall: 0.9759 (Acc: 0.9789, P: 0.9816, R: 0.9759, F1: 0.9788, ROC: 0.9962, PR: 0.9957)\n",
      "  New best model (by val_loss) saved at epoch 5.\n",
      "Epoch 6, Batch 0/141, Train Loss: 0.0331, LR: 9.37e-04\n",
      "Epoch 6, Batch 29/141, Train Loss: 0.0145, LR: 9.60e-04\n",
      "Epoch 6, Batch 58/141, Train Loss: 0.0429, LR: 9.78e-04\n",
      "Epoch 6, Batch 87/141, Train Loss: 0.0224, LR: 9.91e-04\n",
      "Epoch 6, Batch 116/141, Train Loss: 0.0977, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0304\n",
      "Epoch 6 — Val Loss: 0.0745, Val Recall: 0.9750 (Acc: 0.9802, P: 0.9851, R: 0.9750, F1: 0.9801, ROC: 0.9967, PR: 0.9968)\n",
      "Epoch 7, Batch 0/141, Train Loss: 0.0099, LR: 1.00e-03\n",
      "Epoch 7, Batch 29/141, Train Loss: 0.0085, LR: 9.99e-04\n",
      "Epoch 7, Batch 58/141, Train Loss: 0.0735, LR: 9.98e-04\n",
      "Epoch 7, Batch 87/141, Train Loss: 0.0640, LR: 9.95e-04\n",
      "Epoch 7, Batch 116/141, Train Loss: 0.0018, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0199\n",
      "Epoch 7 — Val Loss: 0.0736, Val Recall: 0.9797 (Acc: 0.9811, P: 0.9823, R: 0.9797, F1: 0.9810, ROC: 0.9967, PR: 0.9969)\n",
      "  New best model (by val_loss) saved at epoch 7.\n",
      "Epoch 8, Batch 0/141, Train Loss: 0.0030, LR: 9.87e-04\n",
      "Epoch 8, Batch 29/141, Train Loss: 0.0042, LR: 9.81e-04\n",
      "Epoch 8, Batch 58/141, Train Loss: 0.0056, LR: 9.75e-04\n",
      "Epoch 8, Batch 87/141, Train Loss: 0.0030, LR: 9.67e-04\n",
      "Epoch 8, Batch 116/141, Train Loss: 0.0039, LR: 9.58e-04\n",
      "Epoch 8/20 — Train Loss: 0.0123\n",
      "Epoch 8 — Val Loss: 0.0854, Val Recall: 0.9855 (Acc: 0.9800, P: 0.9747, R: 0.9855, F1: 0.9801, ROC: 0.9965, PR: 0.9967)\n",
      "Epoch 9, Batch 0/141, Train Loss: 0.0269, LR: 9.50e-04\n",
      "Epoch 9, Batch 29/141, Train Loss: 0.0007, LR: 9.39e-04\n",
      "Epoch 9, Batch 58/141, Train Loss: 0.0009, LR: 9.28e-04\n",
      "Epoch 9, Batch 87/141, Train Loss: 0.0002, LR: 9.15e-04\n",
      "Epoch 9, Batch 116/141, Train Loss: 0.0067, LR: 9.02e-04\n",
      "Epoch 9/20 — Train Loss: 0.0071\n",
      "Epoch 9 — Val Loss: 0.0881, Val Recall: 0.9788 (Acc: 0.9806, P: 0.9823, R: 0.9788, F1: 0.9806, ROC: 0.9966, PR: 0.9967)\n",
      "Epoch 10, Batch 0/141, Train Loss: 0.0021, LR: 8.90e-04\n",
      "Epoch 10, Batch 29/141, Train Loss: 0.0055, LR: 8.75e-04\n",
      "Epoch 10, Batch 58/141, Train Loss: 0.0011, LR: 8.59e-04\n",
      "Epoch 10, Batch 87/141, Train Loss: 0.0091, LR: 8.43e-04\n",
      "Epoch 10, Batch 116/141, Train Loss: 0.0058, LR: 8.26e-04\n",
      "Epoch 10/20 — Train Loss: 0.0051\n",
      "Epoch 10 — Val Loss: 0.0952, Val Recall: 0.9791 (Acc: 0.9812, P: 0.9832, R: 0.9791, F1: 0.9811, ROC: 0.9966, PR: 0.9968)\n",
      "Epoch 11, Batch 0/141, Train Loss: 0.0004, LR: 8.10e-04\n",
      "Epoch 11, Batch 29/141, Train Loss: 0.0003, LR: 7.92e-04\n",
      "Epoch 11, Batch 58/141, Train Loss: 0.0034, LR: 7.73e-04\n",
      "Epoch 11, Batch 87/141, Train Loss: 0.0033, LR: 7.53e-04\n",
      "Epoch 11, Batch 116/141, Train Loss: 0.0010, LR: 7.33e-04\n",
      "Epoch 11/20 — Train Loss: 0.0025\n",
      "Epoch 11 — Val Loss: 0.1004, Val Recall: 0.9813 (Acc: 0.9802, P: 0.9791, R: 0.9813, F1: 0.9802, ROC: 0.9963, PR: 0.9965)\n",
      "Epoch 12, Batch 0/141, Train Loss: 0.0119, LR: 7.16e-04\n",
      "Epoch 12, Batch 29/141, Train Loss: 0.0011, LR: 6.94e-04\n",
      "Epoch 12, Batch 58/141, Train Loss: 0.0003, LR: 6.73e-04\n",
      "Epoch 12, Batch 87/141, Train Loss: 0.0004, LR: 6.51e-04\n",
      "Epoch 12, Batch 116/141, Train Loss: 0.0187, LR: 6.29e-04\n",
      "Epoch 12/20 — Train Loss: 0.0019\n",
      "Epoch 12 — Val Loss: 0.1090, Val Recall: 0.9808 (Acc: 0.9796, P: 0.9784, R: 0.9808, F1: 0.9796, ROC: 0.9959, PR: 0.9961)\n",
      "Early stopping triggered after 12 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 7 with Val Loss: 0.0736\n",
      "\n",
      "Training finished. Total time: 157.65s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 7:\n",
      "  - Accuracy: 0.9811\n",
      "  - Precision: 0.9823\n",
      "  - Recall: 0.9797\n",
      "  - F1: 0.9810\n",
      "  - Roc_auc: 0.9967\n",
      "  - Pr_auc: 0.9969\n",
      "  - Val_loss: 0.0736\n",
      "Fold 9 finished. Val Recall: 0.9797 (from best model by val_loss within fold)\n",
      "\n",
      "--- Repetition 5/5, Fold 2/2 (Overall Fold 10) ---\n",
      "Using device: cuda\n",
      "Using recall as the primary scoring metric for validation.\n",
      "Using explicitly provided training and validation DataFrames.\n",
      "Training set size: 8987, Validation set size: 8987\n",
      "Training set class distribution: {0: 4500, 1: 4487}\n",
      "Validation set class distribution: {0: 4500, 1: 4487}\n",
      "Using class weights: [0.99855556 1.00144863]\n",
      "Starting training...\n",
      "Epoch 1, Batch 0/141, Train Loss: 0.6959, LR: 4.00e-05\n",
      "Epoch 1, Batch 29/141, Train Loss: 0.6699, LR: 4.30e-05\n",
      "Epoch 1, Batch 58/141, Train Loss: 0.6395, LR: 5.15e-05\n",
      "Epoch 1, Batch 87/141, Train Loss: 0.5878, LR: 6.55e-05\n",
      "Epoch 1, Batch 116/141, Train Loss: 0.4701, LR: 8.47e-05\n",
      "Epoch 1/20 — Train Loss: 0.5914\n",
      "Epoch 1 — Val Loss: 0.3829, Val Recall: 0.9010 (Acc: 0.8668, P: 0.8430, R: 0.9010, F1: 0.8711, ROC: 0.9380, PR: 0.9313)\n",
      "  New best model (by val_loss) saved at epoch 1.\n",
      "Epoch 2, Batch 0/141, Train Loss: 0.3334, LR: 1.05e-04\n",
      "Epoch 2, Batch 29/141, Train Loss: 0.3797, LR: 1.34e-04\n",
      "Epoch 2, Batch 58/141, Train Loss: 0.2227, LR: 1.67e-04\n",
      "Epoch 2, Batch 87/141, Train Loss: 0.2073, LR: 2.04e-04\n",
      "Epoch 2, Batch 116/141, Train Loss: 0.1616, LR: 2.44e-04\n",
      "Epoch 2/20 — Train Loss: 0.2404\n",
      "Epoch 2 — Val Loss: 0.1454, Val Recall: 0.9668 (Acc: 0.9624, P: 0.9583, R: 0.9668, F1: 0.9625, ROC: 0.9902, PR: 0.9853)\n",
      "  New best model (by val_loss) saved at epoch 2.\n",
      "Epoch 3, Batch 0/141, Train Loss: 0.0733, LR: 2.82e-04\n",
      "Epoch 3, Batch 29/141, Train Loss: 0.1444, LR: 3.28e-04\n",
      "Epoch 3, Batch 58/141, Train Loss: 0.0510, LR: 3.77e-04\n",
      "Epoch 3, Batch 87/141, Train Loss: 0.1163, LR: 4.27e-04\n",
      "Epoch 3, Batch 116/141, Train Loss: 0.0348, LR: 4.78e-04\n",
      "Epoch 3/20 — Train Loss: 0.1070\n",
      "Epoch 3 — Val Loss: 0.1133, Val Recall: 0.9710 (Acc: 0.9742, P: 0.9771, R: 0.9710, F1: 0.9741, ROC: 0.9946, PR: 0.9920)\n",
      "  New best model (by val_loss) saved at epoch 3.\n",
      "Epoch 4, Batch 0/141, Train Loss: 0.0346, LR: 5.23e-04\n",
      "Epoch 4, Batch 29/141, Train Loss: 0.0740, LR: 5.74e-04\n",
      "Epoch 4, Batch 58/141, Train Loss: 0.0706, LR: 6.25e-04\n",
      "Epoch 4, Batch 87/141, Train Loss: 0.0494, LR: 6.75e-04\n",
      "Epoch 4, Batch 116/141, Train Loss: 0.0654, LR: 7.23e-04\n",
      "Epoch 4/20 — Train Loss: 0.0754\n",
      "Epoch 4 — Val Loss: 0.1002, Val Recall: 0.9768 (Acc: 0.9793, P: 0.9816, R: 0.9768, F1: 0.9792, ROC: 0.9956, PR: 0.9938)\n",
      "  New best model (by val_loss) saved at epoch 4.\n",
      "Epoch 5, Batch 0/141, Train Loss: 0.0795, LR: 7.63e-04\n",
      "Epoch 5, Batch 29/141, Train Loss: 0.0462, LR: 8.06e-04\n",
      "Epoch 5, Batch 58/141, Train Loss: 0.0612, LR: 8.46e-04\n",
      "Epoch 5, Batch 87/141, Train Loss: 0.0663, LR: 8.82e-04\n",
      "Epoch 5, Batch 116/141, Train Loss: 0.0223, LR: 9.14e-04\n",
      "Epoch 5/20 — Train Loss: 0.0532\n",
      "Epoch 5 — Val Loss: 0.0988, Val Recall: 0.9733 (Acc: 0.9775, P: 0.9816, R: 0.9733, F1: 0.9774, ROC: 0.9961, PR: 0.9941)\n",
      "  New best model (by val_loss) saved at epoch 5.\n",
      "Epoch 6, Batch 0/141, Train Loss: 0.0398, LR: 9.37e-04\n",
      "Epoch 6, Batch 29/141, Train Loss: 0.0661, LR: 9.60e-04\n",
      "Epoch 6, Batch 58/141, Train Loss: 0.0938, LR: 9.78e-04\n",
      "Epoch 6, Batch 87/141, Train Loss: 0.0451, LR: 9.91e-04\n",
      "Epoch 6, Batch 116/141, Train Loss: 0.0552, LR: 9.98e-04\n",
      "Epoch 6/20 — Train Loss: 0.0366\n",
      "Epoch 6 — Val Loss: 0.0859, Val Recall: 0.9860 (Acc: 0.9836, P: 0.9814, R: 0.9860, F1: 0.9837, ROC: 0.9970, PR: 0.9961)\n",
      "  New best model (by val_loss) saved at epoch 6.\n",
      "Epoch 7, Batch 0/141, Train Loss: 0.0083, LR: 1.00e-03\n",
      "Epoch 7, Batch 29/141, Train Loss: 0.0059, LR: 9.99e-04\n",
      "Epoch 7, Batch 58/141, Train Loss: 0.0591, LR: 9.98e-04\n",
      "Epoch 7, Batch 87/141, Train Loss: 0.0162, LR: 9.95e-04\n",
      "Epoch 7, Batch 116/141, Train Loss: 0.0127, LR: 9.91e-04\n",
      "Epoch 7/20 — Train Loss: 0.0225\n",
      "Epoch 7 — Val Loss: 0.0915, Val Recall: 0.9811 (Acc: 0.9815, P: 0.9819, R: 0.9811, F1: 0.9815, ROC: 0.9977, PR: 0.9975)\n",
      "Epoch 8, Batch 0/141, Train Loss: 0.0032, LR: 9.87e-04\n",
      "Epoch 8, Batch 29/141, Train Loss: 0.0505, LR: 9.81e-04\n",
      "Epoch 8, Batch 58/141, Train Loss: 0.0045, LR: 9.75e-04\n",
      "Epoch 8, Batch 87/141, Train Loss: 0.0028, LR: 9.67e-04\n",
      "Epoch 8, Batch 116/141, Train Loss: 0.0734, LR: 9.58e-04\n",
      "Epoch 8/20 — Train Loss: 0.0188\n",
      "Epoch 8 — Val Loss: 0.0893, Val Recall: 0.9853 (Acc: 0.9838, P: 0.9822, R: 0.9853, F1: 0.9838, ROC: 0.9974, PR: 0.9971)\n",
      "Epoch 9, Batch 0/141, Train Loss: 0.0037, LR: 9.50e-04\n",
      "Epoch 9, Batch 29/141, Train Loss: 0.0054, LR: 9.39e-04\n",
      "Epoch 9, Batch 58/141, Train Loss: 0.0038, LR: 9.28e-04\n",
      "Epoch 9, Batch 87/141, Train Loss: 0.0038, LR: 9.15e-04\n",
      "Epoch 9, Batch 116/141, Train Loss: 0.0005, LR: 9.02e-04\n",
      "Epoch 9/20 — Train Loss: 0.0108\n",
      "Epoch 9 — Val Loss: 0.0992, Val Recall: 0.9833 (Acc: 0.9832, P: 0.9831, R: 0.9833, F1: 0.9832, ROC: 0.9974, PR: 0.9972)\n",
      "Epoch 10, Batch 0/141, Train Loss: 0.0297, LR: 8.90e-04\n",
      "Epoch 10, Batch 29/141, Train Loss: 0.0022, LR: 8.75e-04\n",
      "Epoch 10, Batch 58/141, Train Loss: 0.0114, LR: 8.59e-04\n",
      "Epoch 10, Batch 87/141, Train Loss: 0.0046, LR: 8.43e-04\n",
      "Epoch 10, Batch 116/141, Train Loss: 0.0011, LR: 8.26e-04\n",
      "Epoch 10/20 — Train Loss: 0.0066\n",
      "Epoch 10 — Val Loss: 0.1095, Val Recall: 0.9889 (Acc: 0.9831, P: 0.9775, R: 0.9889, F1: 0.9832, ROC: 0.9972, PR: 0.9971)\n",
      "Epoch 11, Batch 0/141, Train Loss: 0.0052, LR: 8.10e-04\n",
      "Epoch 11, Batch 29/141, Train Loss: 0.0053, LR: 7.92e-04\n",
      "Epoch 11, Batch 58/141, Train Loss: 0.0050, LR: 7.73e-04\n",
      "Epoch 11, Batch 87/141, Train Loss: 0.0029, LR: 7.53e-04\n",
      "Epoch 11, Batch 116/141, Train Loss: 0.0008, LR: 7.33e-04\n",
      "Epoch 11/20 — Train Loss: 0.0038\n",
      "Epoch 11 — Val Loss: 0.1163, Val Recall: 0.9880 (Acc: 0.9824, P: 0.9771, R: 0.9880, F1: 0.9825, ROC: 0.9971, PR: 0.9970)\n",
      "Early stopping triggered after 11 epochs due to no improvement in validation loss.\n",
      "Loaded best model from epoch 6 with Val Loss: 0.0859\n",
      "\n",
      "Training finished. Total time: 150.12s. Model size: 1034778.51 KB\n",
      "Best model (by val_loss) obtained at epoch 6:\n",
      "  - Accuracy: 0.9836\n",
      "  - Precision: 0.9814\n",
      "  - Recall: 0.9860\n",
      "  - F1: 0.9837\n",
      "  - Roc_auc: 0.9970\n",
      "  - Pr_auc: 0.9961\n",
      "  - Val_loss: 0.0859\n",
      "Fold 10 finished. Val Recall: 0.9860 (from best model by val_loss within fold)\n",
      "  *** New overall best model found based on RECALL across folds! Score: 0.9860 ***\n",
      "\n",
      "--- Overall Cross-Validation Summary ---\n",
      "Mean accuracy: 0.9809\n",
      "Std accuracy: 0.0018\n",
      "Mean precision: 0.9823\n",
      "Std precision: 0.0030\n",
      "Mean recall: 0.9794\n",
      "Std recall: 0.0039\n",
      "Mean f1: 0.9809\n",
      "Std f1: 0.0019\n",
      "Mean roc auc: 0.9973\n",
      "Std roc auc: 0.0005\n",
      "Mean pr auc: 0.9971\n",
      "Std pr auc: 0.0007\n",
      "Mean val loss: 0.0705\n",
      "Std val loss: 0.0122\n",
      "Mean model size: 1034778.5098\n",
      "Std model size: 0.0000\n",
      "Mean training time: 148.3310\n",
      "Std training time: 8.3610\n",
      "Best model across all folds (based on RECALL on validation set of its fold): Rep 5, Fold 2 (Overall Fold 10) with score 0.9860\n",
      "Loading the overall best model state...\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_results, fitted_scalers = cross_val_train_nn_model(\n",
    "    df=df,\n",
    "    vocab_dict=vocab_dict,\n",
    "    sequence_cols=SEQUENCE_COLS,\n",
    "    scalar_cols=SCALAR_COLS,\n",
    "    char_cols=CHAR_COLS,\n",
    "    vector_cols=VECTOR_COLS,\n",
    "    vector_dims=VECTOR_DIMS,\n",
    "    hyperparams=nn_hyperparams,\n",
    "    n_folds=2,\n",
    "    n_repetitions=5,\n",
    "    scoring_metric=\"recall\",\n",
    "    device=device,\n",
    "    random_seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83184c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalers saved to c:\\Users\\david\\Desktop\\Clase\\TFG\\tfg_24_25\\model_artifacts\\nn_models\\20250702_162411\\scalers.joblib\n",
      "Model and artifacts saved to c:\\Users\\david\\Desktop\\Clase\\TFG\\tfg_24_25\\model_artifacts\\nn_models\\20250702_162411\n"
     ]
    }
   ],
   "source": [
    "save_paths = save_model_with_metadata(\n",
    "    model=nn_model,\n",
    "    vocab_dict=vocab_dict,\n",
    "    hyperparams=nn_hyperparams,\n",
    "    results=nn_results,\n",
    "    scalers=fitted_scalers,\n",
    "    save_dir=PATH_TO_SAVE_NN_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5421c88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 4070 SUPER\n",
      "Loading latest model version: 20250702_162411\n",
      "Scalers loaded from c:\\Users\\david\\Desktop\\Clase\\TFG\\tfg_24_25\\model_artifacts\\nn_models\\20250702_162411\\scalers.joblib\n",
      "Model loaded from c:\\Users\\david\\Desktop\\Clase\\TFG\\tfg_24_25\\model_artifacts\\nn_models\\20250702_162411\n",
      "Using CUDA device: NVIDIA GeForce RTX 4070 SUPER\n",
      "--- Evaluating on Test Set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\Desktop\\Clase\\TFG\\tfg_24_25\\src\\prototypes\\torch_apk_analysis_model.py:1702: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(device.type == \"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Test Set Evaluation Metrics ---\n",
      "  Inference Time: 4.68 seconds\n",
      "  Accuracy: 0.9810\n",
      "  Precision binary: 0.9762\n",
      "  Recall binary: 0.9860\n",
      "  F1 binary: 0.9811\n",
      "  Precision weighted: 0.9810\n",
      "  Recall weighted: 0.9810\n",
      "  F1 weighted: 0.9810\n",
      "  Confusion Matrix:\n",
      "[[976  24]\n",
      " [ 14 984]]\n",
      "  Inference time: 4.6806\n",
      "  Roc auc: 0.9966\n",
      "  Pr auc: 0.9955\n",
      "---------------------------------\n",
      "Test set evaluation results:\n",
      "{'accuracy': 0.980980980980981,\n",
      " 'classification_report': {'0': {'f1-score': 0.9809045226130654,\n",
      "                                 'precision': 0.9858585858585859,\n",
      "                                 'recall': 0.976,\n",
      "                                 'support': 1000.0},\n",
      "                           '1': {'f1-score': 0.9810568295114656,\n",
      "                                 'precision': 0.9761904761904762,\n",
      "                                 'recall': 0.9859719438877755,\n",
      "                                 'support': 998.0},\n",
      "                           'accuracy': 0.980980980980981,\n",
      "                           'macro avg': {'f1-score': 0.9809806760622655,\n",
      "                                         'precision': 0.981024531024531,\n",
      "                                         'recall': 0.9809859719438878,\n",
      "                                         'support': 1998.0},\n",
      "                           'weighted avg': {'f1-score': 0.9809805998325866,\n",
      "                                            'precision': 0.9810293699182588,\n",
      "                                            'recall': 0.980980980980981,\n",
      "                                            'support': 1998.0}},\n",
      " 'confusion_matrix': [[976, 24], [14, 984]],\n",
      " 'f1_binary': 0.9810568295114656,\n",
      " 'f1_weighted': 0.9809805998325866,\n",
      " 'inference_time': 4.680608749389648,\n",
      " 'pr_auc': 0.9955327256131843,\n",
      " 'precision_binary': 0.9761904761904762,\n",
      " 'precision_weighted': 0.9810293699182588,\n",
      " 'recall_binary': 0.9859719438877755,\n",
      " 'recall_weighted': 0.980980980980981,\n",
      " 'roc_auc': 0.9965966933867736}\n"
     ]
    }
   ],
   "source": [
    "nn_model, _, fitted_scalers, _ = load_apk_analysis_model_from_version(\n",
    "    base_dir=PATH_TO_SAVE_NN_MODEL,\n",
    ")\n",
    "\n",
    "results = evaluate_model_on_test_set(\n",
    "    model=nn_model,\n",
    "    df_test=df_test,\n",
    "    scalers=fitted_scalers,\n",
    "    sequence_cols=SEQUENCE_COLS,\n",
    "    scalar_cols=SCALAR_COLS,\n",
    "    char_cols=CHAR_COLS,\n",
    "    vector_cols=VECTOR_COLS,\n",
    "    hyperparams=nn_hyperparams,\n",
    ")\n",
    "\n",
    "print(\"Test set evaluation results:\")\n",
    "pp.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64783c0a",
   "metadata": {},
   "source": [
    "# ML MODELS TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1ddf73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 4070 SUPER\n",
      "Loading latest model version: 20250702_162411\n",
      "Scalers loaded from c:\\Users\\david\\Desktop\\Clase\\TFG\\tfg_24_25\\model_artifacts\\nn_models\\20250702_162411\\scalers.joblib (for context, though embedder doesn't use them directly)\n",
      "Embedder loaded from c:\\Users\\david\\Desktop\\Clase\\TFG\\tfg_24_25\\model_artifacts\\nn_models\\20250702_162411\n",
      "Loading latest model version: 20250702_162411\n",
      "Recall Score: 0.9794071762870514\n"
     ]
    }
   ],
   "source": [
    "# Load the latest model version\n",
    "# model, vocab_dict, used_scalers, metadata = load_apk_analysis_model_from_version(\n",
    "#     base_dir=PATH_TO_SAVE_NN_MODEL,\n",
    "# )\n",
    "\n",
    "# Load just the embedder from the latest version\n",
    "embedder, vocab_dict, used_scalers, metadata = load_apk_feature_embedder_from_version(\n",
    "    base_dir=PATH_TO_SAVE_NN_MODEL,\n",
    ")\n",
    "\n",
    "# Load just metadata to check performance metrics\n",
    "metadata = load_apk_analysis_model_metadata(base_dir=PATH_TO_SAVE_NN_MODEL)\n",
    "print(f\"Recall Score: {metadata['summary_metrics'].get('mean_recall', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b149bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 5 models with 5 x 2-fold cross-validation...\n",
      "\n",
      "=== Repetition 1/5, Fold 1/2 ===\n",
      "Class distribution:\n",
      "  Class 0: 4500 train, 4500 test\n",
      "  Class 1: 4487 train, 4487 test\n",
      "Training RandomForest...\n",
      "  RandomForest: Acc=0.9876, P: 0.9877, R: 0.9876, F1: 0.9876, ROC: 0.9993, PR: 0.9993, Size=1356.3KB, Time=8.14s\n",
      "  ★ New best RandomForest model: recall=0.9876\n",
      "Training XGBoost...\n",
      "  XGBoost: Acc=0.9909, P: 0.9909, R: 0.9909, F1: 0.9909, ROC: 0.9994, PR: 0.9994, Size=228.2KB, Time=1.01s\n",
      "  ★ New best XGBoost model: recall=0.9909\n",
      "Training KNN...\n",
      "  KNN: Acc=0.9734, P: 0.9735, R: 0.9734, F1: 0.9734, ROC: 0.9904, PR: 0.9908, Size=15833.2KB, Time=0.00s\n",
      "  ★ New best KNN model: recall=0.9734\n",
      "Training SVM...\n",
      "[LibSVM]  SVM: Acc=0.9846, P: 0.9847, R: 0.9846, F1: 0.9846, ROC: 0.9978, PR: 0.9947, Size=1204.1KB, Time=0.44s\n",
      "  ★ New best SVM model: recall=0.9846\n",
      "Training LogisticRegression...\n",
      "  LogisticRegression: Acc=0.9878, P: 0.9878, R: 0.9878, F1: 0.9878, ROC: 0.9985, PR: 0.9956, Size=4.2KB, Time=0.30s\n",
      "  ★ New best LogisticRegression model: recall=0.9878\n",
      "\n",
      "=== Repetition 1/5, Fold 2/2 ===\n",
      "Class distribution:\n",
      "  Class 0: 4500 train, 4500 test\n",
      "  Class 1: 4487 train, 4487 test\n",
      "Training RandomForest...\n",
      "  RandomForest: Acc=0.9844, P: 0.9844, R: 0.9844, F1: 0.9844, ROC: 0.9980, PR: 0.9982, Size=1229.2KB, Time=8.01s\n",
      "Training XGBoost...\n",
      "  XGBoost: Acc=0.9866, P: 0.9866, R: 0.9866, F1: 0.9866, ROC: 0.9989, PR: 0.9989, Size=207.4KB, Time=0.97s\n",
      "Training KNN...\n",
      "  KNN: Acc=0.9720, P: 0.9722, R: 0.9720, F1: 0.9720, ROC: 0.9872, PR: 0.9885, Size=15833.2KB, Time=0.00s\n",
      "Training SVM...\n",
      "[LibSVM]  SVM: Acc=0.9839, P: 0.9839, R: 0.9839, F1: 0.9839, ROC: 0.9980, PR: 0.9978, Size=1101.8KB, Time=0.45s\n",
      "Training LogisticRegression...\n",
      "  LogisticRegression: Acc=0.9878, P: 0.9878, R: 0.9878, F1: 0.9878, ROC: 0.9987, PR: 0.9986, Size=4.2KB, Time=0.46s\n",
      "\n",
      "=== Repetition 2/5, Fold 1/2 ===\n",
      "Class distribution:\n",
      "  Class 0: 4500 train, 4500 test\n",
      "  Class 1: 4487 train, 4487 test\n",
      "Training RandomForest...\n",
      "  RandomForest: Acc=0.9862, P: 0.9862, R: 0.9862, F1: 0.9862, ROC: 0.9988, PR: 0.9988, Size=1272.2KB, Time=8.06s\n",
      "Training XGBoost...\n",
      "  XGBoost: Acc=0.9875, P: 0.9875, R: 0.9875, F1: 0.9875, ROC: 0.9992, PR: 0.9992, Size=218.1KB, Time=0.97s\n",
      "Training KNN...\n",
      "  KNN: Acc=0.9730, P: 0.9732, R: 0.9730, F1: 0.9730, ROC: 0.9889, PR: 0.9896, Size=15833.2KB, Time=0.00s\n",
      "Training SVM...\n",
      "[LibSVM]  SVM: Acc=0.9835, P: 0.9835, R: 0.9835, F1: 0.9835, ROC: 0.9981, PR: 0.9964, Size=1119.4KB, Time=0.45s\n",
      "Training LogisticRegression...\n",
      "  LogisticRegression: Acc=0.9873, P: 0.9873, R: 0.9873, F1: 0.9873, ROC: 0.9986, PR: 0.9973, Size=4.2KB, Time=0.36s\n",
      "\n",
      "=== Repetition 2/5, Fold 2/2 ===\n",
      "Class distribution:\n",
      "  Class 0: 4500 train, 4500 test\n",
      "  Class 1: 4487 train, 4487 test\n",
      "Training RandomForest...\n",
      "  RandomForest: Acc=0.9873, P: 0.9873, R: 0.9873, F1: 0.9873, ROC: 0.9986, PR: 0.9987, Size=1301.8KB, Time=8.19s\n",
      "Training XGBoost...\n",
      "  XGBoost: Acc=0.9892, P: 0.9892, R: 0.9892, F1: 0.9892, ROC: 0.9992, PR: 0.9992, Size=219.2KB, Time=0.98s\n",
      "Training KNN...\n",
      "  KNN: Acc=0.9732, P: 0.9733, R: 0.9732, F1: 0.9732, ROC: 0.9892, PR: 0.9899, Size=15833.2KB, Time=0.00s\n",
      "Training SVM...\n",
      "[LibSVM]  SVM: Acc=0.9823, P: 0.9823, R: 0.9823, F1: 0.9823, ROC: 0.9976, PR: 0.9974, Size=1101.8KB, Time=0.40s\n",
      "Training LogisticRegression...\n",
      "  LogisticRegression: Acc=0.9871, P: 0.9871, R: 0.9871, F1: 0.9871, ROC: 0.9985, PR: 0.9984, Size=4.2KB, Time=0.51s\n",
      "\n",
      "=== Repetition 3/5, Fold 1/2 ===\n",
      "Class distribution:\n",
      "  Class 0: 4500 train, 4500 test\n",
      "  Class 1: 4487 train, 4487 test\n",
      "Training RandomForest...\n",
      "  RandomForest: Acc=0.9859, P: 0.9859, R: 0.9859, F1: 0.9859, ROC: 0.9987, PR: 0.9987, Size=1295.3KB, Time=8.24s\n",
      "Training XGBoost...\n",
      "  XGBoost: Acc=0.9884, P: 0.9884, R: 0.9884, F1: 0.9884, ROC: 0.9993, PR: 0.9993, Size=215.3KB, Time=0.96s\n",
      "Training KNN...\n",
      "  KNN: Acc=0.9742, P: 0.9744, R: 0.9742, F1: 0.9742, ROC: 0.9892, PR: 0.9898, Size=15833.2KB, Time=0.00s\n",
      "  ★ New best KNN model: recall=0.9742\n",
      "Training SVM...\n",
      "[LibSVM]  SVM: Acc=0.9830, P: 0.9830, R: 0.9830, F1: 0.9830, ROC: 0.9977, PR: 0.9977, Size=1140.6KB, Time=0.48s\n",
      "Training LogisticRegression...\n",
      "  LogisticRegression: Acc=0.9868, P: 0.9868, R: 0.9868, F1: 0.9868, ROC: 0.9985, PR: 0.9986, Size=4.2KB, Time=0.46s\n",
      "\n",
      "=== Repetition 3/5, Fold 2/2 ===\n",
      "Class distribution:\n",
      "  Class 0: 4500 train, 4500 test\n",
      "  Class 1: 4487 train, 4487 test\n",
      "Training RandomForest...\n",
      "  RandomForest: Acc=0.9852, P: 0.9852, R: 0.9852, F1: 0.9852, ROC: 0.9987, PR: 0.9988, Size=1266.6KB, Time=8.14s\n",
      "Training XGBoost...\n",
      "  XGBoost: Acc=0.9882, P: 0.9882, R: 0.9882, F1: 0.9882, ROC: 0.9993, PR: 0.9993, Size=220.8KB, Time=1.00s\n",
      "Training KNN...\n",
      "  KNN: Acc=0.9708, P: 0.9710, R: 0.9708, F1: 0.9708, ROC: 0.9875, PR: 0.9887, Size=15833.2KB, Time=0.00s\n",
      "Training SVM...\n",
      "[LibSVM]  SVM: Acc=0.9839, P: 0.9839, R: 0.9839, F1: 0.9839, ROC: 0.9979, PR: 0.9960, Size=1080.6KB, Time=0.36s\n",
      "Training LogisticRegression...\n",
      "  LogisticRegression: Acc=0.9880, P: 0.9880, R: 0.9880, F1: 0.9880, ROC: 0.9984, PR: 0.9970, Size=4.2KB, Time=0.30s\n",
      "  ★ New best LogisticRegression model: recall=0.9880\n",
      "\n",
      "=== Repetition 4/5, Fold 1/2 ===\n",
      "Class distribution:\n",
      "  Class 0: 4500 train, 4500 test\n",
      "  Class 1: 4487 train, 4487 test\n",
      "Training RandomForest...\n",
      "  RandomForest: Acc=0.9871, P: 0.9871, R: 0.9871, F1: 0.9871, ROC: 0.9988, PR: 0.9988, Size=1288.3KB, Time=8.10s\n",
      "Training XGBoost...\n",
      "  XGBoost: Acc=0.9887, P: 0.9887, R: 0.9887, F1: 0.9887, ROC: 0.9991, PR: 0.9992, Size=218.9KB, Time=0.97s\n",
      "Training KNN...\n",
      "  KNN: Acc=0.9742, P: 0.9742, R: 0.9742, F1: 0.9742, ROC: 0.9896, PR: 0.9905, Size=15833.2KB, Time=0.00s\n",
      "Training SVM...\n",
      "[LibSVM]  SVM: Acc=0.9846, P: 0.9846, R: 0.9846, F1: 0.9846, ROC: 0.9978, PR: 0.9962, Size=1112.4KB, Time=0.39s\n",
      "Training LogisticRegression...\n",
      "  LogisticRegression: Acc=0.9872, P: 0.9872, R: 0.9872, F1: 0.9872, ROC: 0.9984, PR: 0.9971, Size=4.2KB, Time=0.35s\n",
      "\n",
      "=== Repetition 4/5, Fold 2/2 ===\n",
      "Class distribution:\n",
      "  Class 0: 4500 train, 4500 test\n",
      "  Class 1: 4487 train, 4487 test\n",
      "Training RandomForest...\n",
      "  RandomForest: Acc=0.9856, P: 0.9857, R: 0.9856, F1: 0.9856, ROC: 0.9990, PR: 0.9990, Size=1311.3KB, Time=8.14s\n",
      "Training XGBoost...\n",
      "  XGBoost: Acc=0.9888, P: 0.9888, R: 0.9888, F1: 0.9888, ROC: 0.9993, PR: 0.9993, Size=221.1KB, Time=0.99s\n",
      "Training KNN...\n",
      "  KNN: Acc=0.9676, P: 0.9683, R: 0.9676, F1: 0.9676, ROC: 0.9871, PR: 0.9878, Size=15833.2KB, Time=0.00s\n",
      "Training SVM...\n",
      "[LibSVM]  SVM: Acc=0.9848, P: 0.9848, R: 0.9848, F1: 0.9848, ROC: 0.9983, PR: 0.9983, Size=1172.3KB, Time=0.50s\n",
      "  ★ New best SVM model: recall=0.9848\n",
      "Training LogisticRegression...\n",
      "  LogisticRegression: Acc=0.9873, P: 0.9873, R: 0.9873, F1: 0.9873, ROC: 0.9989, PR: 0.9989, Size=4.2KB, Time=0.57s\n",
      "\n",
      "=== Repetition 5/5, Fold 1/2 ===\n",
      "Class distribution:\n",
      "  Class 0: 4500 train, 4500 test\n",
      "  Class 1: 4487 train, 4487 test\n",
      "Training RandomForest...\n",
      "  RandomForest: Acc=0.9871, P: 0.9871, R: 0.9871, F1: 0.9871, ROC: 0.9992, PR: 0.9993, Size=1393.2KB, Time=8.09s\n",
      "Training XGBoost...\n",
      "  XGBoost: Acc=0.9908, P: 0.9908, R: 0.9908, F1: 0.9908, ROC: 0.9995, PR: 0.9995, Size=237.7KB, Time=1.02s\n",
      "Training KNN...\n",
      "  KNN: Acc=0.9745, P: 0.9746, R: 0.9745, F1: 0.9745, ROC: 0.9891, PR: 0.9898, Size=15833.2KB, Time=0.00s\n",
      "  ★ New best KNN model: recall=0.9745\n",
      "Training SVM...\n",
      "[LibSVM]  SVM: Acc=0.9873, P: 0.9873, R: 0.9873, F1: 0.9873, ROC: 0.9990, PR: 0.9990, Size=1405.1KB, Time=0.66s\n",
      "  ★ New best SVM model: recall=0.9873\n",
      "Training LogisticRegression...\n",
      "  LogisticRegression: Acc=0.9893, P: 0.9893, R: 0.9893, F1: 0.9893, ROC: 0.9995, PR: 0.9995, Size=4.2KB, Time=0.90s\n",
      "  ★ New best LogisticRegression model: recall=0.9893\n",
      "\n",
      "=== Repetition 5/5, Fold 2/2 ===\n",
      "Class distribution:\n",
      "  Class 0: 4500 train, 4500 test\n",
      "  Class 1: 4487 train, 4487 test\n",
      "Training RandomForest...\n",
      "  RandomForest: Acc=0.9845, P: 0.9846, R: 0.9845, F1: 0.9845, ROC: 0.9981, PR: 0.9979, Size=1186.0KB, Time=7.94s\n",
      "Training XGBoost...\n",
      "  XGBoost: Acc=0.9861, P: 0.9861, R: 0.9861, F1: 0.9861, ROC: 0.9983, PR: 0.9983, Size=194.4KB, Time=0.88s\n",
      "Training KNN...\n",
      "  KNN: Acc=0.9703, P: 0.9707, R: 0.9703, F1: 0.9703, ROC: 0.9867, PR: 0.9882, Size=15833.2KB, Time=0.00s\n",
      "Training SVM...\n",
      "[LibSVM]  SVM: Acc=0.9723, P: 0.9723, R: 0.9723, F1: 0.9723, ROC: 0.9958, PR: 0.9938, Size=678.5KB, Time=0.20s\n",
      "Training LogisticRegression...\n",
      "  LogisticRegression: Acc=0.9779, P: 0.9779, R: 0.9779, F1: 0.9779, ROC: 0.9964, PR: 0.9948, Size=4.2KB, Time=0.29s\n",
      "\n",
      "=== RandomForest Summary ===\n",
      "Mean RECALL: 0.9861 ± 0.0011\n",
      "Best RECALL: 0.9876\n",
      "Mean Size: 1290.0KB\n",
      "Mean Training Time: 8.11s\n",
      "\n",
      "=== XGBoost Summary ===\n",
      "Mean RECALL: 0.9885 ± 0.0015\n",
      "Best RECALL: 0.9909\n",
      "Mean Size: 218.1KB\n",
      "Mean Training Time: 0.98s\n",
      "\n",
      "=== KNN Summary ===\n",
      "Mean RECALL: 0.9723 ± 0.0021\n",
      "Best RECALL: 0.9745\n",
      "Mean Size: 15833.2KB\n",
      "Mean Training Time: 0.00s\n",
      "\n",
      "=== SVM Summary ===\n",
      "Mean RECALL: 0.9830 ± 0.0038\n",
      "Best RECALL: 0.9873\n",
      "Mean Size: 1111.7KB\n",
      "Mean Training Time: 0.43s\n",
      "\n",
      "=== LogisticRegression Summary ===\n",
      "Mean RECALL: 0.9866 ± 0.0030\n",
      "Best RECALL: 0.9893\n",
      "Mean Size: 4.2KB\n",
      "Mean Training Time: 0.45s\n",
      "\n",
      "Best models saved for each model type.\n",
      "Saved 5 model(s) to c:\\Users\\david\\Desktop\\Clase\\TFG\\tfg_24_25\\model_artifacts\\ml_models\\20250702_162653\n",
      "  - RandomForest\n",
      "  - XGBoost\n",
      "  - KNN\n",
      "  - SVM\n",
      "  - LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "# Train classical ML models on embeddings from the nn model\n",
    "X, y = extract_embeddings(\n",
    "    model=embedder,\n",
    "    df=df,\n",
    "    scalers=used_scalers,\n",
    "    sequence_cols=SEQUENCE_COLS,\n",
    "    scalar_cols=SCALAR_COLS,\n",
    "    char_cols=CHAR_COLS,\n",
    "    vector_cols=VECTOR_COLS,\n",
    "    device=device,\n",
    "    label_col=\"is_malware\",\n",
    ")\n",
    "\n",
    "ml_hyperparams_dict = {\n",
    "    \"RandomForest\": MLHyperparams(\n",
    "        model_type=\"random_forest\",\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    \"XGBoost\": MLHyperparams(\n",
    "        model_type=\"xgboost\",\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    \"KNN\": MLHyperparams(\n",
    "        model_type=\"knn\",\n",
    "        n_neighbors=5,\n",
    "        weights=\"uniform\",\n",
    "    ),\n",
    "    \"SVM\": MLHyperparams(\n",
    "        model_type=\"svm\",\n",
    "        C=1.0,\n",
    "        kernel=\"linear\",\n",
    "        probability=False,\n",
    "        random_state=42,\n",
    "        verbose=True,\n",
    "    ),\n",
    "    \"LogisticRegression\": MLHyperparams(\n",
    "        model_type=\"logistic_regression\",\n",
    "        C=1.0,\n",
    "        solver=\"liblinear\",\n",
    "        random_state=42,\n",
    "    ),\n",
    "}\n",
    "\n",
    "ml_results, ml_best_models = train_classical_models_cv(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    hyperparams_dict=ml_hyperparams_dict,\n",
    "    n_folds=2,\n",
    "    n_repetitions=5,\n",
    "    scoring_metric=\"recall\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Save the ML models\n",
    "ml_saved_paths = save_ml_models_with_metadata(\n",
    "    models=ml_best_models, results=ml_results, save_dir=PATH_TO_SAVE_ML_MODEL\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
