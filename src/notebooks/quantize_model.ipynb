{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd34ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65189333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "from utils.preprocessing_utils import load_dataset, apply_scalers_to_dataframe\n",
    "\n",
    "from prototypes.torch_apk_analysis_model import (\n",
    "    get_best_available_device,\n",
    "    evaluate_model_on_test_set,\n",
    "    collate_fn,\n",
    "    NNHyperparams,\n",
    "    ApkAnalysisDataset,\n",
    ")\n",
    "\n",
    "from prototypes.torch_apk_analysis_model_io import (\n",
    "    load_apk_analysis_model_from_version,\n",
    ")\n",
    "\n",
    "torchtext.disable_torchtext_deprecation_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd808a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading last preprocessed dataset...\n",
      "Using CUDA device: NVIDIA GeForce RTX 4070 SUPER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3100, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3155, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3367, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3612, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3672, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_26172\\3687523129.py\", line 29, in <module>\n",
      "    device = get_best_available_device()\n",
      "  File \"c:\\Users\\david\\Desktop\\Clase\\TFG\\tfg_24_25\\prototypes\\torch_nn_model_2.py\", line 644, in get_best_available_device\n",
      "    device = torch.device(\"cuda\")\n",
      "c:\\Users\\david\\Desktop\\Clase\\TFG\\tfg_24_25\\prototypes\\torch_nn_model_2.py:644: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device = torch.device(\"cuda\")\n"
     ]
    }
   ],
   "source": [
    "SEQUENCE_COLS = [\n",
    "    \"activities_list\",\n",
    "    \"services_list\",\n",
    "    \"receivers_list\",\n",
    "    \"permissions_list\",\n",
    "    \"api_calls_list\",\n",
    "]\n",
    "\n",
    "CHAR_COLS = [\"fuzzy_hash\"]\n",
    "VECTOR_COLS = [\"opcode_counts\"]\n",
    "SCALAR_COLS = [\"file_size\"]\n",
    "VECTOR_DIMS = {\"opcode_counts\": 768}\n",
    "\n",
    "PROJECT_ROOT = Path(__file__).parent.parent.parent\n",
    "PATH_TO_DATASET_DIR = PROJECT_ROOT / \"dataset\"\n",
    "PATH_TO_SAVE_NN_MODEL = PROJECT_ROOT / \"model_artifacts\" / \"nn_models\"\n",
    "PATH_TO_SAVE_ML_MODEL = PROJECT_ROOT / \"model_artifacts\" / \"ml_models\"\n",
    "\n",
    "# Load dataset\n",
    "df, vocab_dict = load_dataset(\n",
    "    PATH_TO_DATASET_DIR,\n",
    "    SEQUENCE_COLS,\n",
    "    CHAR_COLS,\n",
    "    VECTOR_COLS,\n",
    "    SCALAR_COLS,\n",
    "    VECTOR_DIMS,\n",
    "    load_fresh=False,\n",
    "    sample_size=None,\n",
    ")\n",
    "\n",
    "df, df_test = train_test_split(\n",
    "    df, test_size=0.1, random_state=42, stratify=df[\"is_malware\"]\n",
    ")\n",
    "\n",
    "device = get_best_available_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA GeForce RTX 4070 SUPER\n",
      "Loading latest model version: 20250622_175541\n",
      "Scalers loaded from ./model_artifacts/nn_models\\20250622_175541\\scalers.joblib\n",
      "Model loaded from ./model_artifacts/nn_models\\20250622_175541\n"
     ]
    }
   ],
   "source": [
    "nn_hyperparams = NNHyperparams(\n",
    "    batch_size=64,\n",
    "    max_learning_rate=6e-3,\n",
    "    epochs=20,\n",
    "    early_stopping=True,\n",
    "    patience=5,\n",
    "    optimizer=\"adamw\",\n",
    "    weight_decay=8e-4,\n",
    "    embedding_dim=64,\n",
    "    hidden_dims=[128],\n",
    "    dropout=0.5,\n",
    "    seq_pooling=\"mean\",\n",
    "    n_classes=2,\n",
    "    label_col=\"is_malware\",\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_persistent_workers=True,\n",
    "    grad_scaler_max_norm=1.0,\n",
    ")\n",
    "\n",
    "model, vocab_dict, used_scalers, metadata = load_apk_analysis_model_from_version(base_dir=PATH_TO_SAVE_NN_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e34a5340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic quantization complete.\n"
     ]
    }
   ],
   "source": [
    "# Dynamic quantization\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "modules_to_quantize = {torch.nn.Linear, torch.nn.GRU}\n",
    "\n",
    "quantized_model_dynamic = torch.quantization.quantize_dynamic(\n",
    "    model,\n",
    "    qconfig_spec=modules_to_quantize,\n",
    "    dtype=torch.qint8,\n",
    ")\n",
    "\n",
    "print(\"Dynamic quantization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e51e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up quantization configurations for each layer type...\n",
      "Quantization configurations assigned.\n",
      "Preparing model for static quantization...\n",
      "\n",
      "Calibrating the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\miniconda3\\envs\\python311\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calibration batch 1/2\n",
      "  Calibration batch 2/2\n",
      "Calibration complete.\n",
      "\n",
      "Converting the model...\n",
      "Static quantization complete.\n"
     ]
    }
   ],
   "source": [
    "# Static quantization\n",
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "model_to_quantize = copy.deepcopy(model)\n",
    "model_to_quantize.eval()\n",
    "\n",
    "print(\"Setting up quantization configurations for each layer type...\")\n",
    "default_qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "\n",
    "model_to_quantize.classifier.qconfig = default_qconfig\n",
    "model_to_quantize.quant.qconfig = default_qconfig\n",
    "model_to_quantize.dequant.qconfig = default_qconfig\n",
    "\n",
    "print(\"Quantization configurations assigned.\")\n",
    "print(\"Preparing model for static quantization...\")\n",
    "\n",
    "quantized_model_static = torch.quantization.prepare(model_to_quantize, inplace=False)\n",
    "quantized_model_static.eval()\n",
    "\n",
    "print(\"\\nCalibrating the model...\")\n",
    "\n",
    "df_calibration = df.sample(n=128, random_state=42)\n",
    "apply_scalers_to_dataframe(\n",
    "    df_calibration, SCALAR_COLS, VECTOR_COLS, used_scalers, fit_scalers=False\n",
    ")\n",
    "\n",
    "calibration_dataset = ApkAnalysisDataset(\n",
    "    df=df_calibration,\n",
    "    sequence_cols=SEQUENCE_COLS,\n",
    "    scalar_cols=SCALAR_COLS,\n",
    "    char_cols=CHAR_COLS,\n",
    "    vector_cols=VECTOR_COLS,\n",
    "    label_col=\"is_malware\",\n",
    ")\n",
    "\n",
    "calibration_loader = DataLoader(\n",
    "    calibration_dataset,\n",
    "    batch_size=nn_hyperparams.batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (\n",
    "        seq_feats,\n",
    "        char_feats,\n",
    "        vector_feats,\n",
    "        scalars,\n",
    "        labels,\n",
    "    ) in enumerate(calibration_loader):\n",
    "        seq_feats = {k: v.to(\"cpu\") for k, v in seq_feats.items()}\n",
    "        char_feats = {k: v.to(\"cpu\") for k, v in char_feats.items()}\n",
    "        vector_feats = {k: v.to(\"cpu\") for k, v in vector_feats.items()}\n",
    "        scalars = {k: v.to(\"cpu\") for k, v in scalars.items()}\n",
    "        labels = labels.to(\"cpu\")\n",
    "        \n",
    "        quantized_model_static(seq_feats, char_feats, vector_feats, scalars)\n",
    "        print(f\"  Calibration batch {i + 1}/{len(calibration_loader)}\")\n",
    "print(\"Calibration complete.\")\n",
    "\n",
    "print(\"\\nConverting the model...\")\n",
    "quantized_model_static = torch.quantization.convert(\n",
    "    quantized_model_static, inplace=False\n",
    ")\n",
    "quantized_model_static.eval()\n",
    "print(\"Static quantization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bbfac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating base model on CPU...\n",
      "--- Evaluating on Test Set ---\n",
      "\n",
      "--- Test Set Evaluation Metrics ---\n",
      "  Inference Time: 8.07 seconds\n",
      "  Accuracy: 0.9760\n",
      "  Precision binary: 0.9817\n",
      "  Recall binary: 0.9699\n",
      "  F1 binary: 0.9758\n",
      "  Precision weighted: 0.9760\n",
      "  Recall weighted: 0.9760\n",
      "  F1 weighted: 0.9760\n",
      "  Confusion Matrix:\n",
      "[[982  18]\n",
      " [ 30 968]]\n",
      "  Inference time: 8.0653\n",
      "  Roc auc: 0.9962\n",
      "  Pr auc: 0.9967\n",
      "---------------------------------\n",
      "\n",
      "Evaluating base model on GPU...\n",
      "--- Evaluating on Test Set ---\n",
      "\n",
      "--- Test Set Evaluation Metrics ---\n",
      "  Inference Time: 6.96 seconds\n",
      "  Accuracy: 0.9760\n",
      "  Precision binary: 0.9817\n",
      "  Recall binary: 0.9699\n",
      "  F1 binary: 0.9758\n",
      "  Precision weighted: 0.9760\n",
      "  Recall weighted: 0.9760\n",
      "  F1 weighted: 0.9760\n",
      "  Confusion Matrix:\n",
      "[[982  18]\n",
      " [ 30 968]]\n",
      "  Inference time: 6.9587\n",
      "  Roc auc: 0.9952\n",
      "  Pr auc: 0.9939\n",
      "---------------------------------\n",
      "\n",
      "Evaluating dynamic quantized model on CPU...\n",
      "--- Evaluating on Test Set ---\n",
      "\n",
      "--- Test Set Evaluation Metrics ---\n",
      "  Inference Time: 12.21 seconds\n",
      "  Accuracy: 0.9750\n",
      "  Precision binary: 0.9837\n",
      "  Recall binary: 0.9659\n",
      "  F1 binary: 0.9747\n",
      "  Precision weighted: 0.9751\n",
      "  Recall weighted: 0.9750\n",
      "  F1 weighted: 0.9750\n",
      "  Confusion Matrix:\n",
      "[[984  16]\n",
      " [ 34 964]]\n",
      "  Inference time: 12.2140\n",
      "  Roc auc: 0.9962\n",
      "  Pr auc: 0.9967\n",
      "---------------------------------\n",
      "\n",
      "Evaluating static quantized model on CPU...\n",
      "--- Evaluating on Test Set ---\n",
      "\n",
      "--- Test Set Evaluation Metrics ---\n",
      "  Inference Time: 12.24 seconds\n",
      "  Accuracy: 0.5005\n",
      "  Precision binary: 0.0000\n",
      "  Recall binary: 0.0000\n",
      "  F1 binary: 0.0000\n",
      "  Precision weighted: 0.2505\n",
      "  Recall weighted: 0.5005\n",
      "  F1 weighted: 0.3339\n",
      "  Confusion Matrix:\n",
      "[[1000    0]\n",
      " [ 998    0]]\n",
      "  Inference time: 12.2435\n",
      "  Roc auc: 0.5000\n",
      "  Pr auc: 0.4995\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating base model on CPU...\")\n",
    "base_results = evaluate_model_on_test_set(\n",
    "    model=model,\n",
    "    df_test=df_test,\n",
    "    scalers=used_scalers,\n",
    "    sequence_cols=SEQUENCE_COLS,\n",
    "    scalar_cols=SCALAR_COLS,\n",
    "    char_cols=CHAR_COLS,\n",
    "    vector_cols=VECTOR_COLS,\n",
    "    hyperparams=nn_hyperparams,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating base model on GPU...\")\n",
    "gpu_base_results = evaluate_model_on_test_set(\n",
    "    model=model,\n",
    "    df_test=df_test,\n",
    "    scalers=used_scalers,\n",
    "    sequence_cols=SEQUENCE_COLS,\n",
    "    scalar_cols=SCALAR_COLS,\n",
    "    char_cols=CHAR_COLS,\n",
    "    vector_cols=VECTOR_COLS,\n",
    "    hyperparams=nn_hyperparams,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating dynamic quantized model on CPU...\")\n",
    "dynamic_q_results = evaluate_model_on_test_set(\n",
    "    model=quantized_model_dynamic,\n",
    "    df_test=df_test,\n",
    "    scalers=used_scalers,\n",
    "    sequence_cols=SEQUENCE_COLS,\n",
    "    scalar_cols=SCALAR_COLS,\n",
    "    char_cols=CHAR_COLS,\n",
    "    vector_cols=VECTOR_COLS,\n",
    "    hyperparams=nn_hyperparams,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating static quantized model on CPU...\")\n",
    "static_q_results = evaluate_model_on_test_set(\n",
    "    model=quantized_model_static,\n",
    "    df_test=df_test,\n",
    "    scalers=used_scalers,\n",
    "    sequence_cols=SEQUENCE_COLS,\n",
    "    scalar_cols=SCALAR_COLS,\n",
    "    char_cols=CHAR_COLS,\n",
    "    vector_cols=VECTOR_COLS,\n",
    "    hyperparams=nn_hyperparams,\n",
    "    device=torch.device(\"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151aad4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (FP32) Model Size: 1034891.11 KB\n",
      "Statically Quantized (INT8) Model Size: 1034727.38 KB\n",
      "Dynamic Quantized (INT8) Model Size: 1034364.68 KB\n",
      "Size Reduction (Dynamic Quantization): 0.05%\n",
      "Size Reduction (Static Quantization): 0.02%\n"
     ]
    }
   ],
   "source": [
    "# To get model size (example):\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "try:\n",
    "    # Save the original (unquantized) model for comparison\n",
    "    fd, original_path = tempfile.mkstemp(suffix=\".pth\")\n",
    "    os.close(fd)\n",
    "    torch.save(model.state_dict(), original_path)\n",
    "    original_model_size_kb = os.path.getsize(original_path) / 1024\n",
    "    os.remove(original_path)\n",
    "    print(f\"Original (FP32) Model Size: {original_model_size_kb:.2f} KB\")\n",
    "\n",
    "    # Save the staticly quantized model object\n",
    "    fd, original_path = tempfile.mkstemp(suffix=\".pth\")\n",
    "    os.close(fd)\n",
    "    torch.save(quantized_model_static.state_dict(), original_path)\n",
    "    static_model_size_kb = os.path.getsize(original_path) / 1024\n",
    "    os.remove(original_path)\n",
    "    print(f\"Statically Quantized (INT8) Model Size: {static_model_size_kb:.2f} KB\")\n",
    "\n",
    "    # Save the quantized model object\n",
    "    fd, quantized_path = tempfile.mkstemp(suffix=\".pth\")\n",
    "    os.close(fd)\n",
    "    torch.save(quantized_model_dynamic, quantized_path)  # Save the whole model object\n",
    "    dynamic_model_size_kb = os.path.getsize(quantized_path) / 1024\n",
    "    os.remove(quantized_path)\n",
    "    print(f\"Dynamic Quantized (INT8) Model Size: {dynamic_model_size_kb:.2f} KB\")\n",
    "\n",
    "    # Calculate reduction\n",
    "    if original_model_size_kb > 0:\n",
    "        dynamic_reduction = (1 - (dynamic_model_size_kb / original_model_size_kb)) * 100\n",
    "        static_reduction = (1 - (static_model_size_kb/ original_model_size_kb)) * 100\n",
    "        print(f\"Size Reduction (Dynamic Quantization): {dynamic_reduction:.2f}%\")\n",
    "        print(f\"Size Reduction (Static Quantization): {static_reduction:.2f}%\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not get model size: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
