import os
import re
import sys
import tempfile
from collections import defaultdict
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import shap
import streamlit as st
import torch
import umap
from matplotlib.patches import Patch
from sklearn.model_selection import train_test_split

# --- Path Setup ---
try:
    project_root = Path(__file__).resolve().parents[2]
except NameError:
    project_root = Path.cwd()

if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

# --- Project Imports ---
from src.prototypes.ml_model_io import load_ml_models_from_version
from src.prototypes.torch_apk_analysis_model import (
    extract_embeddings,
    get_best_available_device,
    predict,
)
from src.prototypes.torch_apk_analysis_model_io import (
    load_apk_analysis_model_from_version,
)
from src.utils.feature_extraction import analyze_apk
from src.utils.preprocessing_utils import (
    apply_scalers_to_dataframe,
    preprocess_data_for_nn,
)

# --- SHAP & Interpretability Functions ---


def analyze_embeddings_with_shap(
    embeddings_to_explain: np.ndarray,
    background_embeddings: np.ndarray,
    classifier_prediction_fn,
) -> shap.Explanation:
    """Analyzes a classifier's behavior on an embedding space using SHAP.

    Parameters
    ----------
    embeddings_to_explain : np.ndarray
        The embedding vectors for the instances to be explained.
    background_embeddings : np.ndarray
        A representative set of embeddings for the background dataset.
    classifier_prediction_fn : callable
        A function that accepts a NumPy array of embeddings and returns a
        NumPy array of model prediction probabilities.

    Returns
    -------
    shap.Explanation
        The explanation object containing SHAP values.
    """
    explainer = shap.KernelExplainer(classifier_prediction_fn, background_embeddings)
    explainer_obj = explainer(embeddings_to_explain)
    return explainer_obj


def aggregate_embedding_shap(
    embedding_explanation: shap.Explanation,
    metadata: dict,
) -> shap.Explanation:
    """Aggregates SHAP values from the embedding space back to original features.

    Parameters
    ----------
    embedding_explanation : shap.Explanation
        The SHAP explanation object for the embedding dimensions.
    metadata : dict
        The model metadata containing architecture details.

    Returns
    -------
    shap.Explanation
        A new explanation object with values aggregated to the original feature level.
    """
    arch = metadata.get("model_architecture", {})
    seq_cols = arch.get("sequence_cols", [])
    char_cols = arch.get("char_cols", [])
    vector_cols = arch.get("vector_cols", [])
    scalar_cols = arch.get("scalar_features", [])
    embedding_dim = metadata.get("hyperparameters", {}).get("embedding_dim", 128)

    original_feature_names = seq_cols + char_cols + vector_cols + scalar_cols
    n_samples, _, n_classes = embedding_explanation.shape
    n_original_features = len(original_feature_names)

    aggregated_shap_matrix = np.zeros((n_samples, n_original_features, n_classes))
    aggregated_data_matrix = np.zeros((n_samples, n_original_features))

    current_index = 0
    feature_map = (
        [(name, embedding_dim) for name in seq_cols]
        + [(name, embedding_dim) for name in char_cols]
        + [(name, embedding_dim) for name in vector_cols]
        + [(name, 1) for name in scalar_cols]
    )

    for i, (name, width) in enumerate(feature_map):
        feature_slice = slice(current_index, current_index + width)
        for c in range(n_classes):
            aggregated_shap_matrix[:, i, c] = embedding_explanation.values[
                :, feature_slice, c
            ].sum(axis=1)
        aggregated_data_matrix[:, i] = embedding_explanation.data[
            :, feature_slice
        ].mean(axis=1)
        current_index += width

    return shap.Explanation(
        values=aggregated_shap_matrix,
        base_values=embedding_explanation.base_values,
        data=aggregated_data_matrix,
        feature_names=original_feature_names,
    )


@st.cache_data
def calculate_shap_explanations(
    model_name: str,
    _apk_filename: str,
    _instance_embedding: np.ndarray,
    _background_embeddings: np.ndarray,
    _model_to_explain,
    _nn_metadata: dict,
    _device,
    _df_background_is_malware: pd.Series,
):
    """
    Calculates and caches SHAP explanations for a given model.
    The leading underscores in args are hints for st.cache_data.
    """
    all_embeddings = np.vstack([_instance_embedding, _background_embeddings])

    if model_name == "Neural Network":

        def nn_prediction_function(embeddings_numpy):
            embeddings_tensor = torch.from_numpy(embeddings_numpy).to(
                _device, dtype=torch.float32
            )
            with torch.no_grad():
                logits = _model_to_explain(embeddings_tensor)
            return torch.softmax(logits, dim=1).cpu().numpy()

        prediction_fn = nn_prediction_function
    else:
        if hasattr(_model_to_explain, "predict_proba"):
            prediction_fn = _model_to_explain.predict_proba
        else:

            def predict_proba_from_predict(X):
                preds = _model_to_explain.predict(X)
                return np.array([[1.0 - p, p] for p in preds], dtype=np.float32)

            prediction_fn = predict_proba_from_predict

    _, sampled_background_embeddings = train_test_split(
        _background_embeddings,
        test_size=0.5,
        random_state=42,
        stratify=_df_background_is_malware.values,
    )

    embedding_explanation = analyze_embeddings_with_shap(
        all_embeddings, sampled_background_embeddings, prediction_fn
    )
    agg_explanation = aggregate_embedding_shap(embedding_explanation, _nn_metadata)
    agg_expl_malware = agg_explanation[:, :, 1]

    expl_instance = agg_expl_malware[0]
    expl_global = agg_expl_malware[1:]

    return expl_instance, expl_global


def st_shap(plot, height=None):
    """Helper function to display SHAP plots in Streamlit."""

    # Ensure SHAP JavaScript is loaded
    shap.initjs()

    # Convert the SHAP plot to HTML and display it in Streamlit
    shap_html = f'<head>{shap.getjs()}</head><body><div style="background-color: white">{plot.html()}</div></body>'
    st.components.v1.html(shap_html, height=height)


def display_list_feature(feature_name, features_dict, container):
    with container:
        title = feature_name.replace("_", " ").title()
        st.markdown(f"**{title}**")
        value = features_dict.get(feature_name, [])
        if len(value) > 0:
            st.dataframe(pd.DataFrame(value, columns=[title]), height=300)
        else:
            st.info("No data found.")


# --- Model & Data Loading ---


@st.cache_resource
def load_nn_model_from_disk(version=None, base_dir="model_artifacts/nn_models"):
    with st.spinner("Loading Neural Network model and artifacts..."):
        device = get_best_available_device()
        model, vocab_dict, scalers, metadata = load_apk_analysis_model_from_version(
            version=version, base_dir=base_dir, device=device
        )
        model.to(device)
        model.eval()
        st.success("Neural Network model loaded.")
        return model, vocab_dict, scalers, metadata, device


@st.cache_resource
def load_ml_models_from_disk(version=None, base_dir="model_artifacts/ml_models"):
    with st.spinner("Loading classical ML models..."):
        ml_models, ml_metadata = load_ml_models_from_version(
            version=version, base_dir=base_dir
        )
        st.success("Classical ML models loaded.")
        return ml_models, ml_metadata


@st.cache_data
def load_background_df(path=project_root / "src" / "app" / "df_background.pic"):
    """Loads the pre-computed background dataframe for SHAP analysis."""
    if not path.exists():
        st.error(f"Background data file not found at: {path}")
        st.warning(
            "Please run the `model_interpretability.ipynb` notebook to generate `df_background.pic`."
        )
        return None

    df = pd.read_pickle(path)

    # Convert list columns to tuples to make them hashable for Streamlit's cache
    for col in df.columns:
        if df[col].apply(type).eq(list).any():
            df[col] = df[col].apply(lambda x: tuple(x) if isinstance(x, list) else x)

    return df


@st.cache_data
def get_background_embeddings(
    _nn_model, _vocab_dict, _scalers, _metadata, _device, df_background
):
    """Processes the background dataframe and extracts embeddings."""
    with st.spinner("Processing background data for explainer context..."):
        _, _, background_embeddings, df_background_processed = (
            full_preprocess_and_predict(
                df_background,
                _nn_model,
                _vocab_dict,
                _scalers,
                _metadata,
                _device,
                preprocess=False,
            )
        )
        return background_embeddings, df_background_processed


# --- Data Processing & Prediction ---


def full_preprocess_and_predict(
    df_raw,
    nn_model,
    vocab_dict,
    scalers,
    metadata,
    device,
    preprocess: bool = True,
):
    """Full pipeline: Preprocesses raw data, gets NN prediction, and extracts embeddings."""
    arch = metadata.get("model_architecture", {})
    sequence_cols = arch.get("sequence_cols", [])
    char_cols = arch.get("char_cols", [])
    vector_cols = arch.get("vector_cols", [])
    scalar_cols = arch.get("scalar_features", [])
    vector_dims = arch.get("vector_dims", {})
    max_lengths = metadata.get("max_lengths")

    if preprocess:
        df_pre_processed = df_raw.copy()
        for col in sequence_cols:
            if col in df_pre_processed.columns:
                df_pre_processed[col] = df_pre_processed[col].apply(
                    lambda x: str(list(x))
                    if isinstance(x, (list, np.ndarray))
                    else str(x)
                )

        df_tokenized, _ = preprocess_data_for_nn(
            df_pre_processed,
            sequence_cols,
            char_cols,
            vector_cols,
            scalar_cols,
            vector_dims,
            vocab_dict=vocab_dict,
            max_lengths=max_lengths,
        )

        df_scaled, _ = apply_scalers_to_dataframe(
            df_tokenized,
            scalar_cols=scalar_cols,
            vector_cols=vector_cols,
            scalers=scalers,
            fit_scalers=False,
        )
    else:
        df_scaled = df_raw

    embeddings, _ = extract_embeddings(
        model=nn_model,
        df=df_scaled,
        scalers=scalers,
        sequence_cols=sequence_cols,
        scalar_cols=scalar_cols,
        char_cols=char_cols,
        vector_cols=vector_cols,
        device=device,
        batch_size=df_scaled.shape[0],
    )

    predictions_nn, probabilities_nn = predict(
        model=nn_model,
        df=df_scaled,
        scalers=scalers,
        sequence_cols=sequence_cols,
        scalar_cols=scalar_cols,
        char_cols=char_cols,
        vector_cols=vector_cols,
        device=device,
        batch_size=df_scaled.shape[0],
    )

    return predictions_nn, probabilities_nn, embeddings, df_scaled


def get_ml_predictions(ml_models, embeddings):
    """Gets predictions from classical ML models."""
    predictions = {}
    for name, model in ml_models.items():
        try:
            pred = model.predict(embeddings)
            if hasattr(model, "predict_proba"):
                proba = model.predict_proba(embeddings)
            else:  # Handle models like SVM without predict_proba
                proba = np.array([[1.0 - p, p] for p in pred])
            predictions[name] = (pred[0], proba[0])
        except Exception as e:
            st.warning(f"Could not get prediction for {name}: {e}")
    return predictions


# --- Plotting ---


def plot_umap_projection(
    current_embedding: np.ndarray,
    background_embeddings: np.ndarray,
    background_labels: np.ndarray,
):
    """Visualizes embedding space with UMAP, highlighting the current sample.

    Parameters
    ----------
    current_embedding : np.ndarray
        The embedding vector for the current APK.
    background_embeddings : np.ndarray
        Embeddings for the background dataset.
    background_labels : np.ndarray
        Labels (0 or 1) for the background dataset.
    """
    with st.spinner("Reducing dimensionality with UMAP..."):
        try:
            all_data = np.vstack([current_embedding, background_embeddings])
            reducer = umap.UMAP(
                n_neighbors=15, min_dist=0.1, n_components=2, random_state=42
            )
            embedded = reducer.fit_transform(all_data)

            fig, ax = plt.subplots(figsize=(12, 8))

            colors = [
                "#1a9850" if label == 0 else "#d73027" for label in background_labels
            ]
            ax.scatter(embedded[1:, 0], embedded[1:, 1], c=colors, alpha=0.6, s=50)

            ax.scatter(
                embedded[0, 0],
                embedded[0, 1],
                c="#3366FF",
                s=250,
                marker="*",
                edgecolor="black",
                linewidth=1.5,
                label="Current APK",
                zorder=10,
            )

            legend_elements = [
                Patch(facecolor="#1a9850", label="Benign (Background)"),
                Patch(facecolor="#d73027", label="Malware (Background)"),
                Patch(color="#3366FF", label="Current APK"),
            ]
            ax.legend(handles=legend_elements, loc="upper right")
            ax.set_title("UMAP Projection of APK Embeddings", fontsize=16)
            ax.set_xlabel("UMAP Dimension 1")
            ax.set_ylabel("UMAP Dimension 2")
            ax.grid(alpha=0.3)
            plt.tight_layout()
            st.pyplot(fig)
            plt.close(fig)

        except Exception as e:
            st.error(f"Failed to generate UMAP plot: {e}")


# --- UI Layout & Main Logic ---

st.set_page_config(page_title="APK Malware Interpreter", layout="wide")
st.title("ü§ñ APK Malware Analysis & Interpretation")

if "history" not in st.session_state:
    st.session_state.history = []
if "current_analysis" not in st.session_state:
    st.session_state.current_analysis = None

try:
    nn_model, vocab_dict, scalers, nn_metadata, device = load_nn_model_from_disk()
    ml_models, ml_metadata = load_ml_models_from_disk()
    df_background_raw = load_background_df()

    # Add NN classifier to the list of models to explain
    models_for_explanation = ml_models.copy()
    models_for_explanation["Neural Network"] = nn_model.classifier

    with st.sidebar:
        st.header("Control Panel")
        uploaded_file = st.file_uploader(
            "Upload an APK file for analysis", type=["apk"]
        )

        if st.button("Analyze Uploaded APK"):
            if uploaded_file is not None:
                analysis_data = {}
                with st.spinner("Performing full analysis... This may take a moment."):
                    tmp_path = None
                    try:
                        with tempfile.NamedTemporaryFile(
                            delete=False, suffix=".apk"
                        ) as tmp:
                            tmp.write(uploaded_file.getvalue())
                            tmp_path = tmp.name

                        features, _ = analyze_apk(tmp_path)
                        if features is None:
                            raise ValueError("Feature extraction failed.")

                        df_raw = pd.DataFrame([features])
                        (nn_pred, nn_proba, embeddings, df_processed) = (
                            full_preprocess_and_predict(
                                df_raw,
                                nn_model,
                                vocab_dict,
                                scalers,
                                nn_metadata,
                                device,
                            )
                        )
                        ml_predictions = get_ml_predictions(ml_models, embeddings)

                        verdict = "Malware" if nn_pred[0] == 1 else "Benign"
                        analysis_data = {
                            "filename": uploaded_file.name,
                            "verdict": verdict,
                            "nn_pred": nn_pred[0],
                            "nn_proba": nn_proba[0],
                            "ml_predictions": ml_predictions,
                            "embeddings": embeddings,
                            "df_processed": df_processed,
                            "raw_features": features,
                            "error": None,
                        }
                        st.success("‚úÖ Analysis complete!")

                    except Exception as e:
                        st.error(f"Analysis failed: {e}")
                        analysis_data["error"] = str(e)
                    finally:
                        if tmp_path and os.path.exists(tmp_path):
                            os.unlink(tmp_path)

                st.session_state.current_analysis = analysis_data
                if not analysis_data.get("error"):
                    st.session_state.history.insert(0, analysis_data)
                st.rerun()
            else:
                st.warning("Please upload an APK file first.")

        st.header("Analysis History")
        if st.button("Clear History"):
            st.session_state.history = []
            st.session_state.current_analysis = None
            st.rerun()

        if not st.session_state.history:
            st.info("No analyses yet.")
        else:
            for i, record in enumerate(st.session_state.history):
                verdict_icon = "üü•" if record["verdict"] == "Malware" else "üü©"
                entry = f"{verdict_icon} {record['filename']} ({record['verdict']})"
                if st.button(entry, key=f"history_{i}"):
                    st.session_state.current_analysis = record
                    st.rerun()

    if st.session_state.current_analysis:
        analysis_data = st.session_state.current_analysis
        if analysis_data.get("error"):
            st.error(f"Could not display analysis. Error: {analysis_data['error']}")
        else:
            st.header(f"Analysis for: `{analysis_data['filename']}`")
            tab_summary, tab_preds, tab_features, tab_exp, tab_umap = st.tabs(
                [
                    "üìä Summary",
                    "üìÑ Predictions",
                    "‚öôÔ∏è Extracted Features",
                    "ü§ñ Explanations",
                    "üó∫Ô∏è UMAP",
                ]
            )

            with tab_summary:
                verdict = analysis_data["verdict"]
                nn_proba = analysis_data["nn_proba"]
                ml_predictions = analysis_data["ml_predictions"]
                nn_col, ml_col = st.columns(2)
                with nn_col:
                    st.metric(
                        "Neural Network Verdict",
                        verdict,
                        f"{nn_proba[1] * 100:.1f}% Malware"
                        if verdict == "Malware"
                        else f"{nn_proba[0] * 100:.1f}% Benign",
                        delta_color="inverse",
                    )
                with ml_col:
                    ml_verdicts = [
                        1 for pred, _ in ml_predictions.values() if pred == 1
                    ]
                    malware_count = len(ml_verdicts)
                    benign_count = len(ml_predictions) - malware_count
                    st.metric(
                        "Classical Models Consensus",
                        "Malware" if malware_count > benign_count else "Benign",
                        f"{malware_count} Malware vs. {benign_count} Benign",
                        delta_color="off",
                    )

            with tab_preds:
                st.subheader("Neural Network (APKAnalysisModel)")
                st.dataframe(
                    pd.DataFrame(
                        {
                            "Model": ["Neural Network"],
                            "Prediction": [
                                "Malware" if analysis_data["nn_pred"] == 1 else "Benign"
                            ],
                            "Benign Confidence": [f"{nn_proba[0] * 100:.2f}%"],
                            "Malware Confidence": [f"{nn_proba[1] * 100:.2f}%"],
                        }
                    )
                )
                st.subheader("Classical ML Models (on NN Embeddings)")
                ml_results = [
                    {
                        "Model": name,
                        "Prediction": "Malware" if pred == 1 else "Benign",
                        "Benign Confidence": f"{proba[0] * 100:.2f}%",
                        "Malware Confidence": f"{proba[1] * 100:.2f}%",
                    }
                    for name, (pred, proba) in ml_predictions.items()
                ]
                st.dataframe(pd.DataFrame(ml_results))

            with tab_features:
                st.header("Raw Features Extracted from APK")
                raw_features = analysis_data.get("raw_features")

                if raw_features:
                    # Use tabs for a more organized and user-friendly display
                    tab_manifest, tab_code, tab_metadata = st.tabs(
                        ["AndroidManifest", "Code Analysis", "File Properties"]
                    )

                    with tab_manifest:
                        col1, col2 = st.columns(2)
                        display_list_feature("permissions_list", raw_features, col1)
                        display_list_feature("activities_list", raw_features, col1)
                        display_list_feature("services_list", raw_features, col2)
                        display_list_feature("receivers_list", raw_features, col2)

                    with tab_code:
                        st.markdown("**Detected API Calls**")
                        api_calls = raw_features.get("api_calls_list", [])
                        if len(api_calls) > 0:
                            st.dataframe(
                                pd.DataFrame(api_calls, columns=["API Call"]),
                                height=400,
                            )
                        else:
                            st.info("No API calls found.")

                        st.markdown("**Opcode Counts**")
                        opcode_counts = raw_features.get("opcode_counts", {})
                        if opcode_counts:
                            st.json(opcode_counts, expanded=False)
                        else:
                            st.info("No opcode counts found.")

                    with tab_metadata:
                        file_size = raw_features.get("file_size")
                        if file_size is not None:
                            st.metric("File Size (Bytes)", f"{file_size:,}")

                        fuzzy_hash = raw_features.get("fuzzy_hash")
                        if fuzzy_hash:
                            st.markdown("**Fuzzy Hash (ssdeep)**")
                            st.code(fuzzy_hash, language=None)

                else:
                    st.info("No raw feature data available.")

                with st.expander("Advanced: Processed Model Input"):
                    st.subheader("Processed Data (Scaled & Tokenized)")
                    st.dataframe(analysis_data["df_processed"])
                    st.subheader("Final Embeddings Fed to Classifier")
                    st.dataframe(
                        pd.DataFrame(
                            analysis_data["embeddings"],
                            columns=[
                                f"emb_{i}"
                                for i in range(analysis_data["embeddings"].shape[1])
                            ],
                        )
                    )

            with tab_exp:
                st.header("SHAP Model Explanations")
                model_to_explain_name = st.selectbox(
                    "Choose a model to inspect:", list(models_for_explanation.keys())
                )

                if model_to_explain_name and df_background_raw is not None:
                    model_to_explain = models_for_explanation[model_to_explain_name]

                    with st.spinner(
                        f"Calculating SHAP explanations for {model_to_explain_name}..."
                    ):
                        background_embeddings, _ = get_background_embeddings(
                            nn_model,
                            vocab_dict,
                            scalers,
                            nn_metadata,
                            device,
                            df_background_raw,
                        )

                        expl_instance, expl_global = calculate_shap_explanations(
                            model_name=model_to_explain_name,
                            _apk_filename=analysis_data["filename"],
                            _instance_embedding=analysis_data["embeddings"],
                            _background_embeddings=background_embeddings,
                            _model_to_explain=model_to_explain,
                            _nn_metadata=nn_metadata,
                            _device=device,
                            _df_background_is_malware=df_background_raw["is_malware"],
                        )

                    st.subheader("Local Explanation (This APK)")
                    st_shap(shap.force_plot(expl_instance), height=200)

                    fig, ax = plt.subplots()
                    shap.plots.waterfall(expl_instance, max_display=15, show=False)
                    st.pyplot(fig, use_container_width=True)
                    plt.close(fig)

                    st.subheader("Global Explanation (Dataset)")
                    col1, col2 = st.columns(2)
                    with col1:
                        fig, ax = plt.subplots(figsize=(6, 9.5))
                        shap.plots.bar(expl_global, max_display=15, ax=ax, show=False)
                        ax.set_title("Global Feature Importance")
                        st.pyplot(fig, use_container_width=False)
                        plt.close(fig)
                    with col2:
                        fig, ax = plt.subplots()
                        shap.plots.beeswarm(expl_global, max_display=15, show=False)
                        ax.set_title("Feature Importance vs. Impact")
                        st.pyplot(fig, use_container_width=True)
                        plt.close(fig)

                        fig, ax = plt.subplots()
                        shap.plots.heatmap(expl_global, max_display=15, show=False)
                        ax.set_title("Feature Importance Heatmap")
                        st.pyplot(fig, use_container_width=True)
                        plt.close(fig)

                    st_shap(shap.force_plot(expl_global), height=400)

                    st.subheader("Feature Dependency Plots")
                    mean_abs_shap = np.abs(expl_global.values).mean(axis=0)
                    top_feature_indices = np.argsort(mean_abs_shap)[::-1]

                    dep_col1, dep_col2 = st.columns(2)
                    cols = [dep_col1, dep_col2, dep_col1, dep_col2]
                    for i, col in zip(top_feature_indices[:4], cols):
                        with col:
                            feature_name = expl_global.feature_names[i]
                            fig, ax = plt.subplots(figsize=(6, 4))
                            shap.plots.scatter(
                                expl_global[:, feature_name],
                                color=expl_global[:, top_feature_indices[0]],
                                ax=ax,
                                show=False,
                            )
                            ax.set_title(
                                f"Dependency plot for '{feature_name}' with interaction from '{expl_global.feature_names[top_feature_indices[0]]}'"
                            )
                            st.pyplot(fig, use_container_width=True)
                            plt.close(fig)

            with tab_umap:
                st.header("Embedding Space Visualization (UMAP)")
                if df_background_raw is not None:
                    background_embeddings, df_background_processed = (
                        get_background_embeddings(
                            nn_model,
                            vocab_dict,
                            scalers,
                            nn_metadata,
                            device,
                            df_background_raw,
                        )
                    )
                    background_labels = df_background_processed["is_malware"].values
                    plot_umap_projection(
                        analysis_data["embeddings"],
                        background_embeddings,
                        background_labels,
                    )

    else:
        st.info("Upload an APK file and click 'Analyze Uploaded APK' to begin.")

except FileNotFoundError as e:
    st.error(f"‚ùå Error loading model artifacts: {e}")
    st.warning(
        "Please ensure trained models exist in 'model_artifacts' and you have run the interpretability notebook."
    )
except Exception as e:
    st.error(f"An unexpected error occurred: {e}")
    st.exception(e)
