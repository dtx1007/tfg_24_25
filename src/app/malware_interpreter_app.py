import os
import sys
import tempfile
from pathlib import Path

import pandas as pd
import streamlit as st
import shap
import numpy as np
import matplotlib.pyplot as plt

# --- Path Setup ---
try:
    project_root = Path(__file__).resolve().parents[2]
except NameError:
    project_root = Path.cwd()

if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

# --- Project Imports ---
from src.utils.feature_extraction import analyze_apk
from src.app.model_utils import (
    load_nn_model_from_disk,
    load_ml_models_from_disk,
    full_preprocess_and_predict,
    get_ml_predictions,
)
from src.app.data_utils import load_background_df, get_background_embeddings
from src.app.shap_utils import (
    get_background_explainer_and_explanations,
    explain_single_instance,
)
from src.app.ui_utils import (
    st_shap,
    display_list_feature,
    plot_umap_projection,
)

# --- UI Layout & Main Logic ---
st.set_page_config(page_title="APK Malware Interpreter", layout="wide")
st.title("🤖 APK Malware Analysis & Interpretation")

if "history" not in st.session_state:
    st.session_state.history = []
if "current_analysis" not in st.session_state:
    st.session_state.current_analysis = None

try:
    nn_model, vocab_dict, scalers, nn_metadata, device = load_nn_model_from_disk()
    ml_models, ml_metadata = load_ml_models_from_disk()
    df_background_raw = load_background_df(
        project_root / "src" / "app" / "df_background.pic"
    )

    if df_background_raw is not None:
        df_background_raw = shap.sample(df_background_raw, 32)

    # Add NN classifier to the list of models to explain
    models_for_explanation = ml_models.copy()
    models_for_explanation["Neural Network"] = nn_model.classifier

    with st.sidebar:
        st.header("Control Panel")
        uploaded_file = st.file_uploader(
            "Upload an APK file for analysis", type=["apk"]
        )

        if st.button("Analyze Uploaded APK"):
            if uploaded_file is not None:
                analysis_data = {}
                with st.spinner("Performing full analysis... This may take a moment."):
                    tmp_path = None
                    try:
                        with tempfile.NamedTemporaryFile(
                            delete=False, suffix=".apk"
                        ) as tmp:
                            tmp.write(uploaded_file.getvalue())
                            tmp_path = tmp.name

                        features, _ = analyze_apk(tmp_path)
                        if features is None:
                            raise ValueError("Feature extraction failed.")

                        df_raw = pd.DataFrame([features])
                        (nn_pred, nn_proba, embeddings, df_processed) = (
                            full_preprocess_and_predict(
                                df_raw,
                                nn_model,
                                vocab_dict,
                                scalers,
                                nn_metadata,
                                device,
                            )
                        )
                        ml_predictions = get_ml_predictions(ml_models, embeddings)

                        verdict = "Malware" if nn_pred[0] == 1 else "Benign"
                        analysis_data = {
                            "filename": uploaded_file.name,
                            "verdict": verdict,
                            "nn_pred": nn_pred[0],
                            "nn_proba": nn_proba[0],
                            "ml_predictions": ml_predictions,
                            "embeddings": embeddings,
                            "df_processed": df_processed,
                            "raw_features": features,
                            "error": None,
                        }
                        st.success("✅ Analysis complete!")

                    except Exception as e:
                        st.error(f"Analysis failed: {e}")
                        analysis_data["error"] = str(e)
                    finally:
                        if tmp_path and os.path.exists(tmp_path):
                            os.unlink(tmp_path)

                st.session_state.current_analysis = analysis_data
                if not analysis_data.get("error"):
                    st.session_state.history.insert(0, analysis_data)
                st.rerun()
            else:
                st.warning("Please upload an APK file first.")

        st.header("Analysis History")
        if st.button("Clear History"):
            st.session_state.history = []
            st.session_state.current_analysis = None
            st.rerun()

        if not st.session_state.history:
            st.info("No analyses yet.")
        else:
            for i, record in enumerate(st.session_state.history):
                verdict_icon = "🟥" if record["verdict"] == "Malware" else "🟩"
                entry = f"{verdict_icon} {record['filename']} ({record['verdict']})"
                if st.button(entry, key=f"history_{i}"):
                    st.session_state.current_analysis = record
                    st.rerun()

    if st.session_state.current_analysis:
        analysis_data = st.session_state.current_analysis
        if analysis_data.get("error"):
            st.error(f"Could not display analysis. Error: {analysis_data['error']}")
        else:
            st.header(f"Analysis for: `{analysis_data['filename']}`")
            tab_summary, tab_preds, tab_features, tab_exp, tab_umap = st.tabs(
                [
                    "📊 Summary",
                    "📄 Predictions",
                    "⚙️ Extracted Features",
                    "🤖 Explanations",
                    "🗺️ UMAP",
                ]
            )

            with tab_summary:
                verdict = analysis_data["verdict"]
                nn_proba = analysis_data["nn_proba"]
                ml_predictions = analysis_data["ml_predictions"]
                nn_col, ml_col = st.columns(2)
                with nn_col:
                    st.metric(
                        "Neural Network Verdict",
                        verdict,
                        f"{nn_proba[1] * 100:.1f}% Malware"
                        if verdict == "Malware"
                        else f"{nn_proba[0] * 100:.1f}% Benign",
                        delta_color="inverse",
                    )
                with ml_col:
                    ml_verdicts = [
                        1 for pred, _ in ml_predictions.values() if pred == 1
                    ]
                    malware_count = len(ml_verdicts)
                    benign_count = len(ml_predictions) - malware_count
                    st.metric(
                        "Classical Models Consensus",
                        "Malware" if malware_count > benign_count else "Benign",
                        f"{malware_count} Malware vs. {benign_count} Benign",
                        delta_color="off",
                    )

            with tab_preds:
                st.subheader("Neural Network (APKAnalysisModel)")
                st.dataframe(
                    pd.DataFrame(
                        {
                            "Model": ["Neural Network"],
                            "Prediction": [
                                "Malware" if analysis_data["nn_pred"] == 1 else "Benign"
                            ],
                            "Benign Confidence": [f"{nn_proba[0] * 100:.2f}%"],
                            "Malware Confidence": [f"{nn_proba[1] * 100:.2f}%"],
                        }
                    )
                )
                st.subheader("Classical ML Models (on NN Embeddings)")
                ml_results = [
                    {
                        "Model": name,
                        "Prediction": "Malware" if pred == 1 else "Benign",
                        "Benign Confidence": f"{proba[0] * 100:.2f}%",
                        "Malware Confidence": f"{proba[1] * 100:.2f}%",
                    }
                    for name, (pred, proba) in ml_predictions.items()
                ]
                st.dataframe(pd.DataFrame(ml_results))

            with tab_features:
                st.header("Raw Features Extracted from APK")
                raw_features = analysis_data.get("raw_features")

                if raw_features:
                    # Use tabs for a more organized and user-friendly display
                    tab_manifest, tab_code, tab_metadata = st.tabs(
                        ["AndroidManifest", "Code Analysis", "File Properties"]
                    )

                    with tab_manifest:
                        col1, col2 = st.columns(2)
                        display_list_feature("permissions_list", raw_features, col1)
                        display_list_feature("activities_list", raw_features, col1)
                        display_list_feature("services_list", raw_features, col2)
                        display_list_feature("receivers_list", raw_features, col2)

                    with tab_code:
                        st.markdown("**Detected API Calls**")
                        api_calls = raw_features.get("api_calls_list", [])
                        if len(api_calls) > 0:
                            st.dataframe(
                                pd.DataFrame(api_calls, columns=["API Call"]),
                                height=400,
                            )
                        else:
                            st.info("No API calls found.")

                        st.markdown("**Opcode Counts**")
                        opcode_counts = raw_features.get("opcode_counts", {})
                        if opcode_counts:
                            st.json(opcode_counts, expanded=False)
                        else:
                            st.info("No opcode counts found.")

                    with tab_metadata:
                        file_size = raw_features.get("file_size")
                        if file_size is not None:
                            st.metric("File Size (bytes)", f"{file_size:,}")

                        fuzzy_hash = raw_features.get("fuzzy_hash")
                        if fuzzy_hash:
                            st.markdown("**Fuzzy Hash (ssdeep)**")
                            st.code(fuzzy_hash, language=None)

                else:
                    st.info("No raw feature data available.")

                with st.expander("Advanced: Processed Model Input"):
                    st.subheader("Processed Data (Scaled & Tokenized)")
                    st.dataframe(analysis_data["df_processed"])
                    st.subheader("Final Embeddings Fed to Classifier")
                    st.dataframe(
                        pd.DataFrame(
                            analysis_data["embeddings"],
                            columns=[
                                f"emb_{i}"
                                for i in range(analysis_data["embeddings"].shape[1])
                            ],
                        )
                    )

            with tab_exp:
                st.header("SHAP Model Explanations")
                model_to_explain_name = st.selectbox(
                    "Choose a model to inspect:", list(models_for_explanation.keys())
                )

                if model_to_explain_name and df_background_raw is not None:
                    model_to_explain = models_for_explanation[model_to_explain_name]

                    with st.spinner(
                        f"Preparing SHAP explainer for {model_to_explain_name}..."
                    ):
                        background_embeddings, _ = get_background_embeddings(
                            nn_model,
                            vocab_dict,
                            scalers,
                            nn_metadata,
                            device,
                            df_background_raw,
                        )

                        # Get the cached explainer and global explanations
                        explainer, expl_global = (
                            get_background_explainer_and_explanations(
                                model_name=model_to_explain_name,
                                _model_to_explain=model_to_explain,
                                _background_embeddings=background_embeddings,
                                _nn_metadata=nn_metadata,
                                _device=device,
                                _df_background_is_malware=df_background_raw[
                                    "is_malware"
                                ],
                            )
                        )

                    with st.spinner("Calculating local explanation for this APK..."):
                        # Calculate explanation for the new instance on the fly
                        expl_instance = explain_single_instance(
                            explainer, analysis_data["embeddings"], nn_metadata
                        )

                    st.subheader("Local Explanation (This APK)")
                    st_shap(shap.force_plot(expl_instance), height=200)

                    fig, ax = plt.subplots()
                    shap.plots.waterfall(expl_instance, max_display=15, show=False)
                    st.pyplot(fig, use_container_width=True)
                    plt.close(fig)

                    st.subheader("Global Explanation (Dataset)")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("#### Global Feature Importance")
                        # Make the figure taller to occupy more vertical space
                        fig, ax = plt.subplots(figsize=(6, 8))
                        shap.plots.bar(expl_global, max_display=15, show=False)
                        st.pyplot(fig, use_container_width=False)
                        plt.close(fig)

                    with col2:
                        st.write("#### Feature Importance Heatmap")
                        fig, ax = plt.subplots()
                        shap.plots.heatmap(expl_global, max_display=15, show=False)
                        st.pyplot(fig, use_container_width=True)
                        plt.close(fig)

                        st.write("#### Feature Importance vs. Impact")
                        fig, ax = plt.subplots()
                        shap.plots.beeswarm(expl_global, max_display=15, show=False)
                        st.pyplot(fig, use_container_width=True)
                        plt.close(fig)

                    st.write("#### Global Force Plot")
                    st_shap(shap.force_plot(expl_global), height=400)

                    st.subheader("Feature Dependency Plots")
                    mean_abs_shap = np.abs(expl_global.values).mean(axis=0)
                    top_feature_indices = np.argsort(mean_abs_shap)[::-1]

                    dep_col1, dep_col2 = st.columns(2)
                    cols = [dep_col1, dep_col2, dep_col1, dep_col2]
                    for i, col in zip(top_feature_indices[:4], cols):
                        with col:
                            feature_name = expl_global.feature_names[i]
                            fig, ax = plt.subplots()
                            # Pass the full explanation object and specify the feature by name.
                            # Also, pass the axes object to ensure the plot is drawn correctly.
                            shap.plots.scatter(
                                expl_global[:, feature_name],
                                color=expl_global[:, top_feature_indices[0]],
                                ax=ax,
                                show=False,
                            )
                            ax.set_title(f"Dependency plot for '{feature_name}'")
                            st.pyplot(fig, use_container_width=True)
                            plt.close(fig)

            with tab_umap:
                st.header("Embedding Space Visualization (UMAP)")
                if df_background_raw is not None:
                    background_embeddings, df_background_processed = (
                        get_background_embeddings(
                            nn_model,
                            vocab_dict,
                            scalers,
                            nn_metadata,
                            device,
                            df_background_raw,
                        )
                    )
                    background_labels = df_background_processed["is_malware"].values
                    plot_umap_projection(
                        analysis_data["embeddings"],
                        background_embeddings,
                        background_labels,
                    )

    else:
        st.info("Upload an APK file and click 'Analyze Uploaded APK' to begin.")

except FileNotFoundError as e:
    st.error(f"❌ Error loading model artifacts: {e}")
    st.warning(
        "Please ensure trained models exist in 'model_artifacts' and you have run the interpretability notebook."
    )
except Exception as e:
    st.error(f"An unexpected error occurred: {e}")
    st.exception(e)
