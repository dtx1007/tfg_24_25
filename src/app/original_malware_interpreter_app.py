import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import shap
import lime
import lime.lime_tabular
from captum.attr import IntegratedGradients
import matplotlib.pyplot as plt
import umap
import hashlib
import time

# --- CONFIGURACIÓN DE LA PÁGINA DE STREAMLIT ---
st.set_page_config(
    page_title="Análisis de Interpretabilidad de Malware",
    page_icon="🤖",
    layout="wide",
    initial_sidebar_state="expanded",
)

# --- MOCKUP DE FUNCIONES Y MODELOS (Para simular tu entorno) ---
# En un entorno real, importarías tus funciones y modelos aquí.

# Mockup de la arquitectura del modelo de Red Neuronal
class MockNNModel(nn.Module):
    def __init__(self, vocab_sizes, embedding_dims, num_numerical_features, mlp_hidden_dims, output_dim):
        super().__init__()
        self.embedding_layers = nn.ModuleList([
            nn.Embedding(num_embeddings, emb_dim) for num_embeddings, emb_dim in zip(vocab_sizes, embedding_dims)
        ])
        total_embedding_dim = sum(embedding_dims)
        
        # Capa de la que extraeremos los embeddings concatenados
        self.embedder_output_dim = total_embedding_dim + num_numerical_features
        
        # Cabeza MLP
        mlp_layers =
        input_dim = self.embedder_output_dim
        for hidden_dim in mlp_hidden_dims:
            mlp_layers.append(nn.Linear(input_dim, hidden_dim))
            mlp_layers.append(nn.ReLU())
            input_dim = hidden_dim
        mlp_layers.append(nn.Linear(input_dim, output_dim))
        self.mlp_head = nn.Sequential(*mlp_layers)

    def forward(self, categorical_inputs, numerical_input):
        # Proceso de Embedding
        embedded_features = [emb_layer(cat_input) for emb_layer, cat_input in zip(self.embedding_layers, categorical_inputs)]
        embedded_features = torch.cat(embedded_features, dim=1)
        
        # Concatenación
        # Esta es la salida del "embedder" que usaremos para los modelos clásicos
        concatenated_embeddings = torch.cat([embedded_features, numerical_input], dim=1)
        
        # Predicción final
        output = self.mlp_head(concatenated_embeddings)
        return output

    def extract_embeddings(self, categorical_inputs, numerical_input):
        """Función para extraer solo la salida del embedder."""
        with torch.no_grad():
            embedded_features = [emb_layer(cat_input) for emb_layer, cat_input in zip(self.embedding_layers, categorical_inputs)]
            embedded_features = torch.cat(embedded_features, dim=1)
            concatenated_embeddings = torch.cat([embedded_features, numerical_input], dim=1)
        return concatenated_embeddings

# Mockup de funciones de carga
@st.cache_resource
def load_nn_model():
    """Simula la carga de tu modelo de PyTorch, vocabularios y metadatos."""
    # Parámetros de ejemplo
    vocab_sizes = [1, 2]  # Vocabulario para 2 características categóricas
    embedding_dims = [3, 4] # Dimensiones de sus embeddings
    num_numerical_features = 5
    mlp_hidden_dims = [5, 6]
    output_dim = 2 # Clasificación binaria (Malware/Benigno)
    
    modelo_torch = MockNNModel(vocab_sizes, embedding_dims, num_numerical_features, mlp_hidden_dims, output_dim)
    # Cargar pesos pre-entrenados si existieran
    # modelo_torch.load_state_dict(...)
    modelo_torch.eval()
    
    vocab_dict = {'permisos': {f'PERM_{i}': i for i in range(50)}, 'apis': {f'API_{i}': i for i in range(30)}}
    scalers = {'numerical_scaler': 'mock_scaler'} # Simula un scaler de sklearn
    metadata = {'feature_names_cat': ['permisos', 'apis'], 'feature_names_num': [f'num_feat_{i}' for i in range(5)]}
    
    return modelo_torch, vocab_dict, scalers, metadata

@st.cache_resource
def load_ml_models():
    """Simula la carga de tus modelos clásicos de ML."""
    # Usamos clasificadores dummy de sklearn para la simulación
    from sklearn.dummy import DummyClassifier
    dict_modelos_ml = {
        'Random Forest': DummyClassifier(strategy="uniform"),
        'XGBoost': DummyClassifier(strategy="uniform"),
        'SVM': DummyClassifier(strategy="uniform"),
        'k-NN': DummyClassifier(strategy="uniform"),
        'Regresión Logística': DummyClassifier(strategy="uniform"),
    }
    # "Entrenamos" los modelos dummy
    mock_embeddings = np.random.rand(100, 20) # 100 muestras, 20 dims de embedding
    mock_labels = np.random.randint(0, 2, 100)
    for model in dict_modelos_ml.values():
        model.fit(mock_embeddings, mock_labels)
        
    models_metadata = {'info': 'Modelos clásicos entrenados con embeddings de la NN.'}
    return dict_modelos_ml, models_metadata

# Mockup de funciones de preprocesamiento
def extract_features_from_apk(apk_file):
    """Simula la extracción de características de un APK a un DataFrame."""
    st.write(f"📄 Analizando hash: `{hashlib.md5(apk_file.getvalue()).hexdigest()}`")
    # En un caso real, aquí iría tu lógica de análisis estático.
    # Para la demo, generamos datos aleatorios que coincidan con la estructura esperada.
    time.sleep(2) # Simular tiempo de análisis
    data = {
        'permisos':,
        'apis': [f'API_{np.random.randint(0, 30)}'],
        'num_feat_0': [np.random.rand() * 100],
        'num_feat_1': [np.random.rand() * 10],
        'num_feat_2': [np.random.randint(0, 1000)],
        'num_feat_3': [np.random.rand()],
        'num_feat_4': [np.random.rand() * 50],
    }
    return pd.DataFrame(data)

def preprocess_data_for_nn(df, vocab_dict, metadata):
    """Simula el preprocesamiento del DataFrame para la entrada de la NN."""
    # Categóricas: Convertir strings a índices usando el vocabulario
    cat_inputs =
    for col in metadata['feature_names_cat']:
        # Usamos.get con un valor por defecto por si la característica no está en el vocabulario
        idx = vocab_dict[col].get(df[col].iloc, 0)
        cat_inputs.append(torch.tensor([[idx]], dtype=torch.long))

    # Numéricas: Aplicar scaler y convertir a tensor
    num_df = df[metadata['feature_names_num']]
    # Aquí aplicarías tu scaler real: processed_num = scalers['numerical_scaler'].transform(num_df)
    processed_num = num_df.values # Simulación
    num_input = torch.tensor(processed_num, dtype=torch.float32)
    
    return cat_inputs, num_input

# --- FUNCIONES DE INTERPRETABILIDAD ---

@st.cache_data
def get_nn_explanations(_df_features, _nn_input_cat, _nn_input_num):
    """Calcula las explicaciones para el modelo de Red Neuronal."""
    modelo_torch, vocab_dict, _, metadata = load_nn_model()
    
    # --- 1. SHAP (DeepExplainer) ---
    # Necesitamos datos de fondo (background) para el explainer
    # Usamos 100 muestras aleatorias simuladas
    background_cat = [torch.randint(0, 50, (100, 1)) for _ in metadata['feature_names_cat']]
    background_num = torch.randn(100, len(metadata['feature_names_num']))
    
    # Wrapper para el forward pass que SHAP pueda entender
    def model_wrapper(cat_inputs_list, num_input):
        # SHAP pasa una lista de tensores, necesitamos separarlos
        return modelo_torch(cat_inputs_list, num_input)

    explainer_shap = shap.DeepExplainer(modelo_torch, (background_cat, background_num))
    shap_values = explainer_shap.shap_values((_nn_input_cat, _nn_input_num))
    
    # --- 2. LIME (Tabular) ---
    # Wrapper para la función de predicción que LIME espera (numpy in -> numpy out)
    def predict_proba_wrapper(numpy_array):
        modelo_torch.eval()
        # LIME perturba los datos, así que recibimos un batch
        n_samples = numpy_array.shape
        
        # Reconstruir la entrada para el modelo PyTorch desde el array de numpy
        num_cat_features = len(metadata['feature_names_cat'])
        cat_features_np = numpy_array[:, :num_cat_features].astype(int)
        num_features_np = numpy_array[:, num_cat_features:]
        
        cat_inputs = [torch.tensor(cat_features_np[:, i:i+1], dtype=torch.long) for i in range(num_cat_features)]
        num_input = torch.tensor(num_features_np, dtype=torch.float32)
        
        with torch.no_grad():
            logits = modelo_torch(cat_inputs, num_input)
            probabilities = torch.nn.functional.softmax(logits, dim=1)
        return probabilities.cpu().numpy()

    # Combinar características para LIME
    feature_names = metadata['feature_names_cat'] + metadata['feature_names_num']
    training_data_np = np.random.rand(100, len(feature_names)) # Simular datos de entrenamiento
    
    explainer_lime = lime.lime_tabular.LimeTabularExplainer(
        training_data=training_data_np,
        feature_names=feature_names,
        class_names=,
        mode='classification'
    )
    
    # Extraer la fila de datos para LIME
    data_row_np = np.concatenate([
        np.array([c.item() for c in _nn_input_cat]),
        _nn_input_num.numpy().flatten()
    ])
    
    lime_exp = explainer_lime.explain_instance(
        data_row=data_row_np,
        predict_fn=predict_proba_wrapper,
        num_features=len(feature_names)
    )

    # --- 3. Integrated Gradients (Captum) ---
    ig = IntegratedGradients(modelo_torch)
    baseline_cat = [torch.zeros_like(c) for c in _nn_input_cat]
    baseline_num = torch.zeros_like(_nn_input_num)
    
    attributions, _ = ig.attribute(
        inputs=(_nn_input_cat, _nn_input_num),
        baselines=(baseline_cat, baseline_num),
        target=1, # Atribución para la clase "Malware"
        return_convergence_delta=True
    )
    
    return shap_values, explainer_shap.expected_value, lime_exp, attributions

@st.cache_data
def get_ml_explanations(_embeddings_vector):
    """Calcula las explicaciones para los modelos clásicos."""
    dict_modelos_ml, _ = load_ml_models()
    explanations = {}
    
    # Usamos un fondo de embeddings aleatorios para KernelExplainer
    background_embeddings = np.random.rand(50, _embeddings_vector.shape[7])
    
    for name, model in dict_modelos_ml.items():
        if name in:
            # TreeExplainer es rápido y exacto para modelos de árbol
            explainer = shap.TreeExplainer(model, background_embeddings)
        else:
            # KernelExplainer es agnóstico al modelo, pero más lento
            explainer = shap.KernelExplainer(model.predict_proba, background_embeddings)
        
        shap_values = explainer.shap_values(_embeddings_vector)
        explanations[name] = {
            'shap_values': shap_values,
            'expected_value': explainer.expected_value
        }
    return explanations

# --- FUNCIONES DE VISUALIZACIÓN ---

def plot_attributions_bar(attributions, feature_names, title):
    """Crea un gráfico de barras para atribuciones (ej. Integrated Gradients)."""
    attr_scores = attributions.flatten()
    colors = ['green' if s > 0 else 'red' for s in attr_scores]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.barh(feature_names, attr_scores, color=colors)
    ax.set_title(title)
    ax.set_xlabel("Puntuación de Atribución (hacia la clase Malware)")
    plt.tight_layout()
    return fig

def plot_umap_embeddings(apk_embedding, n_points=500):
    """Genera y plotea embeddings con UMAP para dar contexto."""
    modelo_torch, _, _, metadata = load_nn_model()
    
    # Generar embeddings de fondo (50% benignos, 50% malware simulados)
    st.write("Generando mapa de embeddings para contexto visual...")
    
    # Simular datos de entrada
    cat_bg = [torch.randint(0, 50, (n_points, 1)) for _ in metadata['feature_names_cat']]
    num_bg = torch.randn(n_points, len(metadata['feature_names_num']))
    
    # Extraer embeddings
    background_embeddings = modelo_torch.extract_embeddings(cat_bg, num_bg).numpy()
    
    # Simular etiquetas
    labels = np.array( * (n_points // 2) + ['Malware'] * (n_points // 2))
    
    # Combinar con el embedding del APK actual
    all_embeddings = np.vstack([background_embeddings, apk_embedding])
    all_labels = np.append(labels, 'APK Actual')
    
    # Reducción de dimensionalidad con UMAP
    reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)
    embedding_2d = reducer.fit_transform(all_embeddings)
    
    # Crear el gráfico
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # Definir colores
    color_map = {'Benigno': 'blue', 'Malware': 'red', 'APK Actual': 'yellow'}
    
    for label in np.unique(all_labels):
        idx = all_labels == label
        ax.scatter(
            embedding_2d[idx, 0],
            embedding_2d[idx, 1],
            c=color_map[label],
            label=label,
            s=100 if label == 'APK Actual' else 30,
            alpha=1 if label == 'APK Actual' else 0.6,
            edgecolors='k' if label == 'APK Actual' else 'none'
        )
        
    ax.set_title("Visualización del Espacio de Embedding (UMAP)")
    ax.set_xlabel("Componente UMAP 1")
    ax.set_ylabel("Componente UMAP 2")
    ax.legend()
    st.pyplot(fig)


# --- INTERFAZ DE USUARIO (UI) ---

st.title("🤖 Análisis de Interpretabilidad para Detección de Malware")
st.write("""
Sube un archivo `.apk`. La aplicación extraerá sus características, las pasará por una red neuronal (NN) y varios modelos clásicos de Machine Learning (ML) para predecir si es malware. 
Luego, usaremos técnicas de IA Explicable (XAI) como SHAP, LIME e Integrated Gradients para entender el *porqué* de cada decisión.
""")

# Inicializar historial en session_state
if 'history' not in st.session_state:
    st.session_state.history =

# --- Sidebar para subida de archivo e historial ---
with st.sidebar:
    st.header("Panel de Control")
    uploaded_file = st.file_uploader("Sube un archivo APK", type=['apk'])
    
    st.header("Historial de Análisis")
    if st.button("Borrar Historial"):
        st.session_state.history =
        st.success("Historial borrado.")

    if not st.session_state.history:
        st.info("No hay análisis en el historial.")
    else:
        for i, entry in enumerate(st.session_state.history):
            with st.expander(f"APK Hash: `{entry['hash'][:10]}...`"):
                st.write(f"**Predicción NN:** {entry['predictions']['NN']['label']} ({entry['predictions']['NN']['proba']:.2f})")
                st.write("**Predicciones ML Clásico:**")
                for model_name, pred in entry['predictions']['ML'].items():
                    st.write(f"- {model_name}: {pred['label']} ({pred['proba']:.2f})")
                # Se podría añadir un botón para recargar el análisis completo desde el historial

# --- LÓGICA PRINCIPAL DE LA APP ---
if uploaded_file is not None:
    # Mostrar spinner mientras se procesa
    with st.spinner('Analizando APK y generando explicaciones... Esto puede tardar un momento.'):
        # 1. Cargar modelos y metadatos
        modelo_torch, vocab_dict, _, nn_metadata = load_nn_model()
        dict_modelos_ml, _ = load_ml_models()
        
        # 2. Extraer y preprocesar características
        df_features = extract_features_from_apk(uploaded_file)
        nn_input_cat, nn_input_num = preprocess_data_for_nn(df_features, vocab_dict, nn_metadata)
        
        # 3. Obtener embeddings para modelos clásicos
        embeddings_vector = modelo_torch.extract_embeddings(nn_input_cat, nn_input_num).numpy()

        # 4. Realizar predicciones
        predictions = {'NN': {}, 'ML': {}}
        
        # Predicción NN
        with torch.no_grad():
            nn_logits = modelo_torch(nn_input_cat, nn_input_num)
            nn_probs = torch.nn.functional.softmax(nn_logits, dim=1).numpy().flatten()
        predictions['NN']['proba'] = nn_probs[7]
        predictions['NN']['label'] = "Malware" if nn_probs[7] > 0.5 else "Benigno"

        # Predicciones ML Clásicos
        for name, model in dict_modelos_ml.items():
            ml_probs = model.predict_proba(embeddings_vector)
            predictions['ML'][name] = {
                'proba': ml_probs[7],
                'label': "Malware" if ml_probs[7] > 0.5 else "Benigno"
            }
            
        # 5. Generar explicaciones
        shap_values_nn, expected_value_nn, lime_exp, ig_attributions = get_nn_explanations(df_features, nn_input_cat, nn_input_num)
        ml_explanations = get_ml_explanations(embeddings_vector)

        # 6. Guardar en historial
        apk_hash = hashlib.md5(uploaded_file.getvalue()).hexdigest()
        history_entry = {
            'hash': apk_hash,
            'predictions': predictions,
            # Podríamos guardar también los datos de explicación para recargarlos
        }
        # Evitar duplicados en el historial
        if not any(h['hash'] == apk_hash for h in st.session_state.history):
            st.session_state.history.insert(0, history_entry)

    # --- MOSTRAR RESULTADOS EN EL DASHBOARD ---
    st.header(f"Resultados del Análisis para `{apk_hash}`")

    # Columna de Resumen de Predicciones
    st.subheader("Resumen de Predicciones")
    col1, col2 = st.columns(2)
    with col1:
        st.metric(
            label="Predicción de la Red Neuronal",
            value=predictions['NN']['label'],
            delta=f"{predictions['NN']['proba']:.2%} de confianza",
            delta_color="inverse" if predictions['NN']['label'] == "Malware" else "off"
        )
    with col2:
        avg_ml_proba = np.mean([p['proba'] for p in predictions['ML'].values()])
        avg_ml_label = "Malware" if avg_ml_proba > 0.5 else "Benigno"
        st.metric(
            label="Predicción Promedio de Modelos Clásicos",
            value=avg_ml_label,
            delta=f"{avg_ml_proba:.2%} de confianza promedio",
            delta_color="inverse" if avg_ml_label == "Malware" else "off"
        )
    
    with st.expander("Ver todas las predicciones de los modelos clásicos"):
        for name, pred in predictions['ML'].items():
            st.text(f"- {name}: {pred['label']} (Confianza: {pred['proba']:.2%})")

    st.markdown("---")

    # Tabs para análisis de interpretabilidad
    tab1, tab2, tab3, tab4 = st.tabs()

    with tab1:
        st.header("Contexto Global: ¿Dónde se sitúa este APK?")
        st.info("""
        Este gráfico UMAP muestra cómo la red neuronal "ve" las aplicaciones. Cada punto es una app, y las que están cerca son similares según el modelo. 
        Esto nos ayuda a entender si el APK analizado (amarillo) cae en una zona densa de malware (rojo) o de software benigno (azul).
        """)
        plot_umap_embeddings(embeddings_vector)

    with tab2:
        st.header("Análisis de la Red Neuronal (PyTorch)")
        st.write("Aquí desglosamos por qué la red neuronal tomó su decisión para este APK específico.")

        st.subheader("Explicación con SHAP")
        st.info("""
        El **Force Plot** de SHAP muestra una "batalla de fuerzas". El *valor base* es la predicción promedio del modelo. Las características en **rojo** empujan la predicción hacia "Malware", mientras que las en **azul** la empujan hacia "Benigno". La suma de todas estas fuerzas da la predicción final.
        """)
        # SHAP para la clase 1 (Malware)
        shap.force_plot(expected_value_nn[7], shap_values_nn[7], df_features, matplotlib=True, show=False)
        plt.tight_layout()
        st.pyplot(plt.gcf(), clear_figure=True)

        st.subheader("Explicación con LIME")
        st.info("""
        LIME (Local Interpretable Model-agnostic Explanations) crea un modelo simple y local para imitar el comportamiento de la red neuronal solo para esta predicción. Las barras **verdes** apoyan la predicción de "Malware", y las **rojas** la contradicen.
        """)
        fig_lime = lime_exp.as_pyplot_figure(label=1) # Explicación para la clase "Malware"
        st.pyplot(fig_lime, clear_figure=True)

        st.subheader("Explicación con Integrated Gradients (Captum)")
        st.info("""
        Esta técnica, específica para redes neuronales, calcula la "atribución" de cada característica a la predicción final. Una puntuación positiva significa que la característica empujó la decisión hacia "Malware".
        """)
        # Combinar atribuciones categóricas y numéricas
        attr_cat = ig_attributions.sum(dim=1).cpu().numpy()
        attr_num = ig_attributions.[7]cpu().numpy()
        all_ig_attributions = np.concatenate([attr_cat, attr_num], axis=1)
        all_feature_names = nn_metadata['feature_names_cat'] + nn_metadata['feature_names_num']
        
        fig_ig = plot_attributions_bar(all_ig_attributions, all_feature_names, "Atribuciones de Integrated Gradients")
        st.pyplot(fig_ig, clear_figure=True)

    with tab3:
        st.header("Análisis de los Modelos Clásicos")
        st.warning("""
        **Nota Importante:** Estos modelos fueron entrenados usando los *embeddings* (una representación numérica abstracta) generados por la red neuronal, no con las características originales. Por lo tanto, las explicaciones de SHAP aquí muestran la influencia de cada **dimensión del embedding**, no de las características originales como "permisos" o "APIs". Esto nos dice qué partes de la representación aprendida son importantes para cada modelo clásico.
        """)
        
        model_choice = st.selectbox("Selecciona un modelo clásico para analizar:", list(dict_modelos_ml.keys()))
        
        if model_choice:
            explanation_data = ml_explanations[model_choice]
            st.subheader(f"Explicación SHAP para {model_choice}")
            
            # SHAP para la clase 1 (Malware)
            shap.force_plot(
                explanation_data['expected_value'][7], 
                explanation_data['shap_values'][7], 
                feature_names=[f'Emb_{i}' for i in range(embeddings_vector.shape[7])],
                matplotlib=True,
                show=False
            )
            plt.tight_layout()
            st.pyplot(plt.gcf(), clear_figure=True)

    with tab4:
        st.header("Sugerencias y Conclusiones del Análisis")
        st.info("""
        Esta sección ofrece una interpretación combinada de los resultados para mejorar la comprensión del comportamiento del modelo.
        """)
        
        # Lógica de sugerencias (ejemplo)
        is_malware_nn = predictions['NN']['label'] == "Malware"
        is_malware_ml = (np.mean([p['proba'] for p in predictions['ML'].values()]) > 0.5)

        if is_malware_nn and is_malware_ml:
            st.success("ALERTA: Alta probabilidad de Malware. Tanto la red neuronal como la mayoría de los modelos clásicos coinciden en la clasificación.")
            st.write("**Recomendación:** Analizar las características en rojo/verde en las pestañas de la NN para identificar los indicadores de riesgo clave (ej. permisos peligrosos, API calls sospechosas).")
        elif not is_malware_nn and not is_malware_ml:
            st.success("INFO: Baja probabilidad de Malware. La mayoría de los modelos coinciden en que la aplicación es benigna.")
        else:
            st.warning("ADVERTENCIA: Hay discrepancias entre los modelos.")
            st.write(f"La red neuronal predice **{predictions['NN']['label']}** mientras que el consenso de los modelos clásicos apunta a **{'Malware' if is_malware_ml else 'Benigno'}**.")
            st.write("""
            **Posibles causas y sugerencias:**
            - La red neuronal puede estar capturando interacciones complejas que los modelos clásicos (entrenados en sus embeddings) no pueden.
            - Revisa las explicaciones de SHAP para la NN y para el modelo clásico más discrepante. ¿Qué características valora cada uno de forma diferente?
            - Este tipo de desacuerdo es una excelente oportunidad para identificar casos límite y mejorar la robustez del sistema de detección.
            """)

else:
    st.info("Esperando la subida de un archivo APK en el panel de la izquierda...")
