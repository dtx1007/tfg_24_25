\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}

% Intro, en este apartado se expone...

\section{Pruebas iniciales y estado del arte}
% Pruebas iniciales con el dataset de drebin
% - Adaptación del dataset a un dataframe / csv
% - Creación del modelo simple, mlp y embedder
% - Proceso de entrenamiento y de entrenamiento de los otros modelos de ml
% - Proceso de carga y guardado de los modelos
% - Proceso de obtención de gráficas y datos

\subsection{Base del proyecto}
% Papaer en el que nos basamos (a lo mejor debería ir en la parte de los papers)

\subsection{El dataset de Drebin}

\subsection{Creación de un modelo prototipo}

\subsection{Entrenamiento del modelo}

\subsection{Análisis de datos preliminar}



\section{Extracción de caracteísticas y \textit{dataset} propio}
% Creación del dataset propio
% - Filtrado y descarga de las apks
% - Pruebas de extración de características con androguard
% - Creación de un proceso automatizado y creación del dataset
% - Características extraidas y explicación de estas

\subsection{Pruebas con Androguard}

\subsection{Automatización del proceso de creación del \textit{dataset}}



\section{Desarrollo y optimización del modelo final}

\subsection{Adaptación del modelo al nuevo set de datos}
% Adaptación modificación del modelo a los datos actuales
% - Añadir el gru para el procesador del fuzzy hash
% - Añadir los opcodes y el procesado de estos (procesado de vectores)
% - Añadir los escalares (tamaño del archivo)
% - Cambiar el proceso de training

\subsection{Problemas imprevistos}
% Problemas que el modelo nuevo trajo a la mesa (complejidad mucho mayor y un mal diseño del sistema inicial)
% - Optimizaciones empleadas
%   - Preprocesar datos fuera del modelo (ya no era factible no hacerlo para este caso) y pasar todos los tipos de forma cuidadosa a tipos más óptimos como los que numpy provee, sobretodo para aprovechar la vectorización de operaciones
%   - Añadir el scheaduler para el learning rate
%   - Añadir auto grad (evitar gradient clipping)
%   - Añadir auto cast (operar con datos a menor precisión cuando sea posible)
% 	- Limpiar memoria innecesaria

\subsection{Búsqueda de hiperparámetros óptimos}
% Búsqueda de hiperparámetros optimos mediante optuna
% - Explicar proceso y resultados

\subsection{Cuantización del modelo}
% (maybe) Quantización del modelo
% - Explicar proceso y si merece la pena comparado con el original

\subsection{Análisis de los resultados}
% Comparativas de rendimiento

\section{Interpretabilidad del modelo}
\subsection{Análisis de las predicciones mediante SHAP}
% Explicar las difernets pruebas realizadas, problemas con el hecho de que las funciones de SHAP requerían del uso de wrappers para evaluar el modelo y que se prefirió el uso de un método que fuera agnóstico al modelo. Exponer problema de las características de entrada y el cómo interpretarlas sería complicado, exponer el problema de la separación y agregación de estas. Llegar a la idea de que, para general, se analiza la predición tras el embedder, lo cual presenta el reto de reinterpretar las características y la péridad del "valor" o "sentido" directo de estas pero sigue dando mucho valor al proyecto. Mostrar gráficas para cada modelo y explicar alguna predicción rara.
\subsection{Análisis del \textit{embedder} y el espacio de características}
% Hablar acerca de UMAP y la evaluación del embedder y el cómo coloca este los datos en dicho espacio de embeddings. Mostrar gráfica.
% Concluir que el embedder hace la gran mayoría del trabajo, aprende muy bien a colocar el malware en una region del espacio de características y a las aplicaciones benignas en otra, comentar el hecho de que, esto simplifica un montón el trabajo de otros modelos para parender, puesto que les deja ya gran parte del trabajo hecho. Además reslata una idea interesante y es que parece ser que las características de entrada están muy fuertemente relacionadas o, puede ser una consecuencia de que el embedder hace su trabajo muy bien pero, por norma general tendría sentido, si algo es malware siempre tiende a tener características malas en muchos aspectos. Esto se ve muy bien en el análisis de muestras individuales donde no hay una especie de "tira y afloja" en las predicciones, si algo es maligno, todo suele apoyar esto y, si no lo es, todo suele apuntar a ello nuevamenete, es raro que haya casos en los que las barras se muevan a ambos lados.

\section{Desarrollo de la aplicación web}
% Desarrollo de la aplicación web
% - Explicar proceso y funcionalidades básicas
% - (maybe) incluir boton de historial en local storage y mini base de datos sqlite, comparativo con múltiples modelos y estadísticas
% - Dockerizar y desplegar

\subsection{Creación de la web}

\subsection{\textit{Dockerización} y despliegue}
 