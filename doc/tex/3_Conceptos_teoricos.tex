\capitulo{3}{Conceptos teóricos}

En esta sección se definen aquellos conceptos que son necesarios conocer para comprender el resto del documento.

\section{Conceptos de Inteligencia Artificial}

Para comprender cómo una máquina puede aprender a identificar amenazas, primero debemos sentar las bases de la inteligencia artificial (IA). Este apartado introduce las ideas fundamentales que hacen posible este proyecto, comenzando por el concepto general de la IA, centrándose en el aprendizaje automático como su motor principal. Se explora cómo los modelos computacionales pueden identificar patrones en los datos y el importante proceso de transformar información cruda, como un archivo, en un formato que la máquina pueda entender. El objetivo es ofrecer una visión clara del camino que se sigue para construir y entrenar un modelo de IA desde cero.

\subsection{Inteligencia artificial}

La inteligencia artificial (IA) es una disciplina de la informática que busca desarrollar sistemas capaces de realizar tareas que tradicionalmente asociamos con la inteligencia humana, como el razonamiento, el aprendizaje o la percepción del entorno. Más que intentar replicar la mente humana, un objetivo todavía lejano, la IA se centra en crear herramientas que puedan procesar información de forma autónoma y tomar decisiones para resolver problemas concretos. La IA nos proporciona un conjunto de técnicas para abordar estos desafíos, abriendo la puerta a nuevas formas de automatización y análisis.

El enorme interés que vemos hoy en día por la IA se debe a una combinación de factores que han coincidido en el tiempo: la disponibilidad de cantidades masivas de datos para entrenar los modelos, el diseño de algoritmos de aprendizaje cada vez más eficaces y, sobre todo, el acceso a una gran capacidad de cómputo gracias al abaratamiento de costes y el desarrollo de mejores Unidades de Procesamiento Gráfico (GPU). Estas circunstancias han permitido que la IA deje de ser un campo puramente académico para convertirse en una tecnología con aplicaciones en prácticamente cualquier sector.

Gracias a ello, la IA ha impulsado avances en un espectro muy amplio de aplicaciones. En el procesamiento del lenguaje (NLP), ha sido la base para mejorar los traductores automáticos~\cite{wu2016google} y ha permitido el desarrollo de asistentes virtuales mediante el desarrollo de la arquitectura de los \textit{transformers}~\cite{vaswani2017attention}. En el campo de la visión por computador, las técnicas de IA son fundamentales en sistemas de reconocimiento facial o en la asistencia al diagnóstico médico mediante imágenes~\cite{razzak2017deep}. Otros campos, como los sistemas de recomendación~\cite{gheewala2025depth} o la conducción autónoma~\cite{grigorescu2020survey}, son desafíos complejos donde la IA es un componente esencial, aunque su desarrollo y perfeccionamiento siguen siendo un trabajo en curso.

\subsection{Aprendizaje automático}

El aprendizaje automático (\textit{machine learning} o ML) es el subcampo de la inteligencia artificial centrado en crear algoritmos que aprenden patrones directamente de los datos, sin necesidad de ser programados con reglas explícitas para cada tarea. La premisa fundamental del aprendizaje automático es que, al exponer a un modelo a una cantidad suficientemente grande de ejemplos, este puede identificar patrones, correlaciones y estructuras subyacentes en los datos para, posteriormente, realizar predicciones o tomar decisiones sobre datos nuevos y nunca antes vistos.

El tipo de datos y el nivel de guía que se le proporciona al modelo definen los principales paradigmas de aprendizaje que existen. El más extendido es el \textbf{aprendizaje supervisado}~\cite{cortes1995support}, donde el modelo se entrena con un conjunto de datos completamente etiquetado, es decir, cada ejemplo de entrada está emparejado con una salida, comúnmente conocida como etiqueta, o clase. El objetivo es que el modelo aprenda a mapear cada entrada con su salida correcta. En el otro extremo se encuentra el \textbf{aprendizaje no supervisado}~\cite{hinton2006reducing}, donde el modelo recibe datos sin ninguna etiqueta y su tarea es encontrar por sí mismo cualquier estructura o agrupación inherente en ellos, como organizar una biblioteca sin conocer los géneros de los libros.

Entre estos dos extremos, surge una solución pragmática: el \textbf{aprendizaje semisupervisado}~\cite{van2020survey}. Este enfoque es ideal cuando etiquetar datos es caro o laborioso, ya que utiliza un pequeño conjunto de datos etiquetados junto con una gran cantidad de datos sin etiquetar. La idea es que la estructura de los datos no etiquetados ayude al modelo a generalizar mejor a partir de las pocas etiquetas que tiene. Finalmente, el \textbf{aprendizaje por refuerzo}~\cite{de2018multi} funciona de manera distinta, ya que no se basa en aprender de un \textit{dataset} estático. En este caso, un agente aprende interactuando con un entorno dinámico, tomando decisiones y recibiendo recompensas o penalizaciones por sus acciones, lo que le permite aprender una estrategia óptima a través de la experiencia.

El aprendizaje automático, en todas sus formas, constituye el motor que impulsa la mayoría de las aplicaciones modernas de IA, permitiéndolas adaptarse y ser útiles en multitud de situaciones.

\subsection{Conjunto de datos (\textit{dataset})}

Un conjunto de datos, o \textit{dataset}, es una colección estructurada de datos que se utiliza como base para el aprendizaje de un modelo de inteligencia artificial. En esencia, es el material de estudio del que dispone el modelo para entrenar, validar su rendimiento y, finalmente, ser puesto a prueba. Los \textit{datasets} pueden ser públicos y estar ya creados, permitiendo a los investigadores experimentar con ellos, o pueden ser construidos a medida para un proyecto específico, como ha sido el caso en este trabajo.

En el ámbito de la seguridad en Android, existen varios \textit{datasets} de referencia. Uno de los más conocidos es el Drebin \textit{dataset}~\cite{arp2014drebin}, una colección que contiene miles de aplicaciones Android, tanto maliciosas como benignas, junto con un gran número de características estáticas extraídas de ellas. Otro ejemplo relevante es el CIC-MalDroid 2020~\cite{mahdavifar2020dynamic}, que ofrece una colección más moderna y diversa de \textit{malware}. Para la fase de prototipado de este proyecto, se utilizó una versión del Drebin dataset para realizar las pruebas iniciales y validar la viabilidad del proyecto.

\subsection{Extracción de características}

Un modelo de aprendizaje automático no <<entiende>> el mundo como nosotros; no ve una imagen, un texto o un archivo, sino que únicamente opera con números. La extracción de características es el proceso fundamental mediante el cual, se intenta traducir la información del mundo real al lenguaje matemático que el modelo puede procesar. Consiste en identificar y cuantificar las propiedades más representativas de los datos en bruto para convertirlas en un formato numérico y estructurado.

Esta <<traducción>> debe capturar la esencia del problema. Por ejemplo, si queremos analizar un programa en busca de \textit{malware}, no le pasamos el archivo binario directamente al modelo. En su lugar, extraemos <<pistas>> medibles como el número de permisos que solicita, la presencia de ciertas funciones sospechosas o si intenta conectarse a direcciones de internet conocidas por ser maliciosas. Cada una de estas pistas es una característica que, en conjunto, forma una especie de perfil numérico de la aplicación que el modelo es capaz de aprender mucho mejor que el binario completo.

Sin embargo, no todas las pistas son igual de importantes. Una parte crucial del proceso es la selección de características, que consiste en filtrar el ruido y quedarse solo con la información que realmente aporta valor predictivo. Esto sería el equivalente a un detective que descarta las pistas falsas para centrarse en solo aquellas que le permiten resolver el caso. Este proceso de selección y filtrado culmina en la creación de un \textit{dataset}, una tabla organizada donde cada fila es una muestra y cada columna es una de las características relevantes, lista para que el modelo pueda aprender de ella.

\subsection{Preprocesamiento de datos}

El preprocesamiento de datos es una etapa crucial en cualquier proyecto de aprendizaje automático la cual consiste en aplicar un conjunto técnicas al \textit{dataset} justo antes del entrenamiento del modelo. Su objetivo principal es limpiar, transformar y adecuar los datos para mejorar la calidad de la información de entrada y, en consecuencia, el rendimiento, la eficiencia y la capacidad de generalización del modelo. Un modelo solo puede ser tan bueno como los datos con los que se entrena, el preprocesamiento es la etapa que intenta que esos datos estén en la mejor forma posible para facilitar el aprendizaje del modelo. A su vez, también es útil para permitir simplificar el modelo internamente y, asegurar que los datos que le llegan a este son siempre de una forma concreta, lo cual permite optimizar enormemente el rendimiento de este.

Las tareas de preprocesamiento son variadas y dependen de la naturaleza de los datos y del modelo que se pretende utilizar. Una de las más fundamentales es la conversión de estos a un formato numérico, ya que, al final, todos los algoritmos no son más que puras matemáticas que operan exclusivamente con números. Esto implica, por ejemplo, transformar texto en una representaciones numéricas mediante técnicas como la tokenización y la creación de vocabularios, o convertir características categóricas (como <<tipo de archivo>>) en valores numéricos mediante métodos como la discretización o el \textit{one-hot encoding}. El resultado final suele ser una matriz numérica.~\cite{luengo2020big}

Otras tareas comunes incluyen:

\begin{itemize}
	\item \textbf{Imputación de valores nulos:} Muchos \textit{datasets} contienen valores faltantes. Estos deben ser tratados, ya sea eliminando las muestras o características afectadas (si son pocas) o, más comúnmente, rellenando los huecos con un valor por defecto o incluso un estadístico como la media, la mediana o la moda de dicha característica.
	
	\item \textbf{Escalado de características:} Cuando las características numéricas tienen rangos muy diferentes (p. ej., una contiene valores entre 0 y 1, mientras que otra de 0 a 1\,000\,000), los algoritmos sensibles a la escala, como las máquinas de soporte vectorial (SVM) o los que usan el descenso de gradiente, pueden verse negativamente afectados. Técnicas como la normalización (escalar a un rango [0, 1]) o la estandarización (centrar en media 0 y desviación estándar 1) resuelven este problema.
	
	\item \textbf{Discretización:} Consiste en convertir características numéricas continuas en categorías o \textit{bins} que cubren un rango concreto de todos los valores posibles.
	
	\item \textbf{Filtrado de características:} Aunque a menudo se considera parte de la extracción, la selección final de características puede realizarse también en esta fase, eliminando aquellas con baja varianza o baja correlación con la variable objetivo.
\end{itemize}

\subsection{\textit{Embedder} (Incrustador)}

En el contexto del aprendizaje automático, un \textit{embedding} es una técnica que permite convertir características discretas o de alta dimensionalidad (como palabras, permisos de una aplicación o identificadores únicos) en vectores de números reales densos~\cite{bengio2003neural}. La idea fundamental es mapear estos elementos a un espacio vectorial de tal manera que las relaciones semánticas o funcionales entre ellos se vean reflejadas en la geometría de dicho espacio. Es decir, los elementos que son similares en el mundo real acabarán <<agrupados>> o cerca unos de otros en este nuevo espacio de características.

Un \textit{embedder} es el componente del modelo, generalmente una capa de la red neuronal, que se encarga de realizar esta transformación. Aprende a generar estos vectores durante el propio proceso de entrenamiento del modelo principal. Por ejemplo, en lugar de decirle al modelo que el permiso \texttt{READ\_CONTACTS} es simplemente el número <<5>>, el \textit{embedder} aprende a representarlo como un vector (p. ej., $[0.12, -0.45, 01.89, ...]$) que lo sitúa cerca de otros permisos de naturaleza similar, como \texttt{WRITE\_CONTACTS}. Este proceso transforma datos complejos y no numéricos en una representación rica y llena de significado que el resto del modelo puede utilizar para encontrar patrones de manera mucho más eficaz.

\subsection{Clasificador}

Un clasificador es un modelo o algoritmo que se utiliza para asignar una categoría o etiqueta a un conjunto de datos, basándose en las características de los mismos. En el contexto de la IA, el proceso de clasificación implica entrenar un modelo para que aprenda a predecir la clase o categoría correcta de nuevas observaciones, basándose en ejemplos previos conocidos como datos de entrenamiento.

\subsection{Clasificadores clásicos (ML)}

Los clasificadores clásicos de aprendizaje automático son aquellos modelos que se establecieron como fundamentales en el campo antes de la popularización masiva de las redes neuronales profundas (\textit{deep learning}). Estos algoritmos siguen siendo extremadamente útiles y relevantes, especialmente cuando se trabaja con datos estructurados o tabulares, o cuando la interpretabilidad del modelo es una prioridad, además, suelen requerir menos datos y recursos computacionales que los modelos de \textit{deep learning}. A continuación se describen algunos de los más representativos:

\begin{itemize}
	\item \textbf{Árbol de decisión (\textit{Decision tree}):} Es uno de los modelos más intuitivos. Funciona creando una estructura similar a un diagrama de flujo, donde cada nodo interno representa una pregunta sobre una característica, cada rama representa la respuesta a esa pregunta, y cada nodo hoja representa una etiqueta de una de las clases. Para clasificar una nueva muestra, se la hace <<descender>> por el árbol desde la raíz, respondiendo a las preguntas en cada nodo hasta llegar a una hoja, la cual proporciona la predicción. Los árboles aprenden a realizar las mejores preguntas (o <<cortes>>) para dividir los datos de la forma más pura posible en cada paso, basándose en métricas como la ganancia de información.~\cite{breiman2017classification}
	
	\item \textbf{$k$-Vecinos más cercanos (\textit{$k$-Nearest Neighbors} o $k$-NN):} A diferencia de otros modelos, k-NN es un algoritmo <<perezoso>> (\textit{lazy learner}), ya que no aprende una función como tal durante su entrenamiento. Simplemente, almacena todo el conjunto de datos de entrenamiento. Para clasificar una nueva muestra, busca las $k$ muestras más similares (sus <<vecinos más cercanos>>) en el conjunto de entrenamiento, basándose en una métrica de distancia como puede ser la distancia euclídea, la distancia de Manhattan o la distancia de Hamming. La clase de la nueva muestra se asigna por votación mayoritaria entre las clases de esos $k$ vecinos.~\cite{cover1967nearest}
	
	\item \textbf{Máquinas de soporte vectorial (\textit{Support Vector Machines} o SVM):} El objetivo de un SVM es encontrar el hiperplano óptimo que separe las distintas clases que se desean predecir en el espacio de características del conjunto de datos con el mayor margen posible. Dicho <<margen>> es la distancia entre el hiperplano de decisión y los puntos de datos más cercanos de cada clase, llamados <<vectores de soporte>>. Al maximizar este margen, el modelo logra una mejor capacidad de generalización. Para problemas que no son linealmente separables, los SVM pueden utilizar lo que se conoce como el \textit{kernel trick}, el cual consiste en mapear los datos a un espacio de mayor dimensión donde sí se pueda encontrar un hiperplano que separe las clases correctamente~\cite{cortes1995support}.

	\item \textbf{Regresión logística:} A pesar de su nombre, la regresión logística es un modelo de clasificación lineal estadístico bastante usado y que funciona sorprendentemente bien pese a su simplicidad. Se basa en estimar la probabilidad de que una muestra pertenezca a una clase determinada. Lo hace aplicando la función logística (o sigmoide) a una combinación lineal de las características de entrada. El resultado es un valor entre 0 y 1, que puede interpretarse como una probabilidad. Un umbral de decisión (típicamente $0.5$) se utiliza para asignar la muestra a una de las dos clases.
\end{itemize}

\subsection{Ensembles}

En el aprendizaje automático, a menudo se sigue una filosofía similar a la de un comité de expertos: en lugar de confiar en la opinión de un único especialista, se combinan las decisiones de varios para obtener un veredicto final más robusto y fiable. Los ensembles o conjuntos de modelos se basan precisamente en esta idea. La lógica subyacente es que los errores o sesgos individuales de cada modelo tienden a ser compensados por los aciertos de los demás, resultando en una predicción colectiva más sólida que la de cualquiera de sus miembros por separado.

Existen dos estrategias principales para crear estos <<comités>> de modelos. El \textit{bagging}~\cite{breiman1996bagging} consiste en entrenar a varios modelos en paralelo, cada uno con una muestra ligeramente diferente de los datos, y luego agregar sus predicciones (por ejemplo, mediante una votación). Uno de los modelos más usados que siguen este enfoque es el RandomForest~\cite{breiman2001random}, el cual se basa en aplicar este método de entrenamiento a varios árboles de decisión, debida su naturaleza <<inestable>> (pequeños cambios en su entrenamiento cambian drásticamente el árbol final) esto funciona muy bien con ellos, porque los diferentes árboles que se obtienen son muy distintos entre sí, lo cual permite que algunos sean mejores en ciertos aspectos que otros, complementándose y mejorando su rendimiento al ser evaluados en conjunto. Por otro lado, el \textit{boosting}~\cite{friedman2002stochastic} es un proceso secuencial: se entrena un primer modelo y, a continuación, se entrena un segundo modelo que se centra específicamente en corregir los errores del primero, y así sucesivamente. Cada nuevo modelo se especializa aquellos ejemplos difíciles que el modelo anterior no supo clasificar correctamente, generando así una cadena de expertos cada vez mejor entrenados. El mejor modelo que representa esta técnica de entrenamiento es el XGBoost (eXtreme Gradient Boosting)~\cite{chen2016xgboost}, el cual, se basa nuevamente en entrenar árboles de decisión pero de manera secuencial y siguiendo el proceso descrito anteriormente.

\subsection{Neurona artificial y perceptrón multicapa (MLP)}

La neurona artificial~\cite{mcculloch1943logical} es la piedra fundamental que sirve como predecesor a las redes neuronales. Su diseño está inspirado en una neurona biológica, la cual recibe señales a través de sus dendritas, las procesa en el soma y emite una señal a través de su axón si el estímulo acumulado supera un cierto umbral. De forma análoga, una neurona artificial recibe una o más entradas numéricas ($x_1, x_2, ..., x_n$), a cada una de las cuales se le asigna un peso ajustable ($w_1, w_2, ..., w_n$). Estos pesos determinan la importancia de cada entrada. La neurona calcula la suma ponderada de sus entradas, a la que se le añade un término de sesgo (\textit{bias}, $b$). Este resultado agregado pasa a través de una función de activación no lineal, que determina la salida final de la neurona.

El primer modelo formal de una neurona artificial fue el perceptrón~\cite{rosenblatt1958perceptron}, desarrollado por Frank Rosenblatt en 1957. Un único perceptrón podía aprender a resolver problemas de clasificación binaria que fueran linealmente separables. Sin embargo, se demostró que un perceptrón simple era incapaz de aprender funciones no lineales tan básicas como la puerta XOR~\cite{minsky2017perceptrons}, lo que limitó gravemente su aplicabilidad y condujo a un período de menor interés en la investigación de redes neuronales.

La solución a esta limitación llegó con el desarrollo del perceptrón multicapa (\textit{Multilayer Perceptron} o MLP). Un MLP consiste en apilar las neuronas artificiales en múltiples capas: una capa de entrada (\textit{input layer}), que recibe los datos iniciales; una o más capas intermedias u ocultas (\textit{hidden layers}), que realizan la mayor parte del procesamiento y permiten aprender patrones complejos; y una capa de salida (\textit{output layer}), que produce el resultado final (p. ej., una clasificación o un valor de regresión). Al interconectar las neuronas de esta manera, donde la salida de las neuronas de una capa se convierte en la entrada de las neuronas de la siguiente, la red en su conjunto es capaz de aprender representaciones jerárquicas y aproximar cualquier función continua, superando las limitaciones del perceptrón simple. El MLP es el modelo base de cualquier red neuronal actual y uno de los avances más importantes en el campo.

\subsection{Redes Neuronales Profundas (\textit{Deep Learning})}

El aprendizaje profundo o \textit{deep learning} es un subcampo del aprendizaje automático que ha revolucionado la inteligencia artificial en la última década. Su seña de identidad es el uso de redes neuronales profundas, que no son más que redes neuronales con una arquitectura que incluye múltiples capas ocultas apiladas entre la capa de entrada y la de salida. Esta <<profundidad>> es lo que les otorga su extraordinario poder, ya que les permite aprender representaciones de los datos de forma jerárquica y con distintos niveles de abstracción.

La idea es que cada capa aprende a reconocer patrones basándose en la salida de la capa anterior. Las primeras capas aprenden a detectar características muy simples y de bajo nivel. A medida que la información fluye a través de la red, las capas posteriores combinan estas características simples para construir conceptos cada vez más complejos y abstractos. Por ejemplo, en el análisis de imágenes, las primeras capas podrían detectar bordes y colores, las siguientes podrían combinarlos para reconocer formas como ojos o narices, y las capas finales podrían combinar toda esta información para identificar una cara.

El éxito arrollador del \textit{deep learning} no se debe únicamente a la elegancia de su arquitectura, sino también a la unión de dos factores externos: la disponibilidad de enormes volúmenes de datos (\textit{big data}) para entrenar estos modelos tan complejos, y el desarrollo de \textit{hardware} de computación paralela, principalmente las Unidades de Procesamiento Gráfico (GPU), que proporcionan la potencia de cálculo necesaria.

\subsection{Métricas de evaluación de un modelo}

Para saber si un modelo de clasificación funciona bien, no basta con mirar su porcentaje de aciertos. Necesitamos un análisis más profundo que nos revele cómo acierta y cómo se equivoca. Esto es especialmente crítico cuando las clases están desbalanceadas, es decir, cuando tenemos muchos más ejemplos de una categoría que de otra. Para ello, primero se definen los cuatro posibles resultados que puede tomar una predicción:

\begin{itemize}
	\item \textbf{Verdadero positivo (TP - \textit{True Positive}):} El modelo predice correctamente que una muestra es positiva (p. ej., predice <<\textit{malware}>> y la aplicación es \textit{malware}).
	
	\item \textbf{Falso positivo (FP - \textit{False Positive}):} El modelo predice incorrectamente que una muestra es positiva (p. ej., predice <<\textit{malware}>> y la aplicación es benigna). También se conoce como \textbf{error de tipo I}.
	
	\item \textbf{Verdadero negativo (TN - \textit{True Negative}):} El modelo predice correctamente que una muestra es negativa (p. ej., predice <<benigno>> y la aplicación es benigna).
	
	\item \textbf{Falso negativo (FN - \textit{False Negative}):} El modelo predice incorrectamente que una muestra es negativa (p. ej., predice <<benigno>> y la aplicación es \textit{malware}). También se conoce como \textbf{error de tipo II}.
\end{itemize}

A partir de estos valores, se calculan las siguientes métricas:

\begin{itemize}
	\item \textbf{Exactitud (\textit{Accuracy}):} Es la métrica más intuitiva, representa el porcentaje total de aciertos. Aunque útil, puede ser muy engañosa en escenarios con clases desbalanceadas. Un 99\% de exactitud suena genial, pero si solo el 1\% de las muestras son \textit{malware}, un modelo que siempre predice <<benigno>> lograría esa cifra sin aportar ningún valor.

	\[Accuracy =\frac{TP + TN}{TP + FP + TN + FN}\]
	
	\item \textbf{Precisión (\textit{Precision}):} Responde a la pregunta: de todas las veces que el modelo predijo <<\textit{malware}>>, ¿cuántas veces acertó? Una alta precisión es vital cuando el coste de un falso positivo es alto (por ejemplo, bloquear una aplicación legítima).

	\[Precision = \frac{TP}{TP + FP}\]

	\item \textbf{Sensibilidad (\textit{Recall} o \textit{Sensitivity}):} Responde a una pregunta diferente pero igualmente crucial: de todas los ejemplos de \textit{malware} real que existían, ¿qué porcentaje fue capaz de detectar el modelo? Es la métrica más importante cuando el coste de un falso negativo es alto, como es en este caso, equivaldría a no detectar una aplicación maliciosa que sí lo era.
	
	\[Recall = \frac{TP}{TP + FN}\]
	
	\item \textbf{Especificidad (\textit{Specificity}):} Es el equivalente a la sensibilidad, pero para la clase negativa. De todas las aplicaciones benignas, ¿cuántas fueron correctamente identificadas como tal?
	
	\[Specicity = \frac{TN}{TN + FP}\]
	
	\item \textbf{Puntuación F1 (\textit{$F_1$-Score}):} Dado que la precisión y la sensibilidad suelen estar siempre en conflicto (mejorar una a menudo empeora la otra), la puntuación F1 ofrece un equilibrio entre ambas. Es la media armónica de las dos, lo que significa que penaliza a los modelos que tienen un desequilibrio extremo entre ellas. Es una de las métricas de referencia para evaluar clasificadores más usada.
	
	\[F_1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}\]
	
\end{itemize}

Más allá de las métricas individuales, existen dos curvas de evaluación muy utilizadas que nos ofrecen una visión más detallada del comportamiento de un modelo. Nos permiten ver cómo se comporta un clasificador no en un único punto, sino a través de todos los posibles umbrales de decisión. Un umbral es, sencillamente, el nivel de <<confianza>> que el modelo necesita para clasificar una muestra como positiva. Al variar este umbral (desde ser muy estricto a muy permisivo), obtenemos las siguientes curvas.

\begin{itemize}
	\item \textbf{Curva ROC (\textit{Receiver Operating Characteristic}):} Es una de las dos curvas clásicas usadas para representar el rendimiento de un clasificador binario. Su principal objetivo es mostrar el compromiso entre encontrar los positivos reales y evitar las falsas alarmas. La curva se obtiene de enfrentar las siguientes métricas:
	
	\begin{itemize}
		\item \textbf{Eje Y (Vertical):} La \textbf{Tasa de Verdaderos Positivos (TPR)}, que es exactamente lo mismo que la sensibilidad ($TPR = TP/(TP + FN)$).
		
		\item \textbf{Eje X (Horizontal):} La \textbf{Tasa de Falsos Positivos (FPR)}, similar a la sensibilidad pero para los falsos positivos ($FPR = FP/(TN + FP)$).
	\end{itemize}
	
	% TODO: Añadir figura de una curva ROC
	
	Interpretación:
	
	\begin{itemize}
		\item \textbf{El modelo perfecto:} Una curva que sube verticalmente hasta el punto $(0, 1)$ y luego se desplaza horizontalmente. Este punto en la esquina superior izquierda representa un 100\% de sensibilidad (encuentra todos los positivos) con un 0\% de falsos positivos (cero falsas alarmas). Cuanto más se acerque la curva a esta esquina, mejor será el modelo.
		
		\item \textbf{La línea de azar:} Una línea diagonal de $(0,0)$ a $(1,1)$ representa un modelo sin ninguna capacidad de discriminación, como lanzar una moneda al aire. Cualquier curva por debajo de esta línea indica que el modelo es peor que el azar.
		
		\item \textbf{A medida que la curva crece:} Un crecimiento rápido hacia arriba significa que el modelo está ganando mucha sensibilidad sin un coste alto de falsos positivos, lo cual es ideal. Si la curva se desplaza demasiado hacia la derecha, significa que para encontrar más positivos, el modelo está cometiendo demasiadas falsas alarmas.
	\end{itemize}
	
	\item \textbf{Curva PR (Precisión-Sensibilidad o \textit{Precision-Recall}):} Aunque la curva ROC es muy útil, puede ser engañosa en problemas con un gran desbalance de clases (por ejemplo, cuando la clase positiva es muy rara). En estos casos, la curva PR suele ser más informativa. Cómo su nombre indica, la curva se obtiene de enfrentar la precisión (eje $Y$) con la sensibilidad (eje $X$).
	
	Interpretación:
	
	\begin{itemize}
		\item \textbf{El modelo perfecto:} La curva ideal se sitúa en la esquina superior derecha, en el punto $(1, 1)$. Esto significa que el modelo mantiene una precisión del 100\% incluso cuando logra encontrar a todos los positivos reales (sensibilidad del 100\%).
		
		\item \textbf{La línea de base:} A diferencia de la curva ROC, la línea de base de una curva PR no es la diagonal. Es una línea horizontal que corresponde a la proporción de ejemplos positivos en el \textit{dataset}. Por ejemplo, si solo el 2\% de los datos son positivos, un modelo aleatorio tendrá una precisión en torno al 2\% y esta sería su línea de base. Un modelo solo es útil si su curva PR está significativamente por encima de esta línea de base.
		
		\item \textbf{A medida que la curva crece (hacia la derecha):} Lo que buscamos es que la curva se mantenga lo más alta posible. Una caída brusca en la precisión a medida que aumenta la sensibilidad nos dice que el modelo, para encontrar más positivos, empieza a cometer muchos errores (falsos positivos), <<contaminando>> sus predicciones.
	\end{itemize}
	
	\item \textbf{AUC (\textit{Area Under the Curve} o Área Bajo la Curva):} Ambas de las curvas anteriores son muy útiles si se desea obtener una visión más global del rendimiento del modelo, pero, en muchos casos, es conveniente poder condensar toda la información que estas curvas nos proveen en un único número, para esto existe el AUC. Este no es más que, literalmente, el área debajo de cada una de las curvas y provee un valor numérico que resume el rendimiento general del clasificador a través de todos los umbrales.
	
	La interpretación de esta medida varía en función de la curva que estemos intentado interpretar pero, su significado es equiparable al de las curvas en sí pero condensado en un único valor. Suelen darse en tanto por uno o como probabilidad.
	
	\begin{itemize}
		\item \textbf{ROC-AUC \textit{(Area Under the ROC Curve})}: Resume cuánto de bueno es el modelo a al ahora de diferenciar entre las clases. Si el valor es cercano a uno (100\%) indica que estamos ante un clasificador excelente. Significa que para un ejemplo positivo y negativo elegidos al azar, hay una probabilidad muy alta (cercana al 100\%) de que el modelo asigne una puntuación más alta al positivo. En cambio, si el valor es cercano a $0.5$, implica que el modelo no es capaz de distinguir bien entre las clases y se puede equiparar a lanzar una moneda al aire.
		
		\item \textbf{PR-AUC (\textit{Area Under the Precision-Recall Curve}):} Representa la precisión media del modelo a lo largo de todos los umbrales de sensibilidad. Si el valor es cercano a 1, indica que la precisión se mantiene cerca del 100\% a medida que la sensibilidad aumenta. Por otro lado, si el valor es cercano al de la línea de base, esta métrica no aporta mucha más que, el hecho de que la precisión media es por estadística la proporción de ejemplos positivos en el \textit{dataset} y escala de esa manera a medida que aumenta la sensibilidad.
	\end{itemize}
\end{itemize}

\subsection{Entrenamiento de un modelo}

El entrenamiento de un modelo de IA es el proceso mediante el cual este aprende a realizar una tarea a partir de los datos. Aunque el proceso varía en función del modelo, la idea general es siempre la misma: se le presentan al modelo unos datos de entrada, este genera una predicción y se compara dicha predicción con el resultado correcto para calcular un error. Este error se utiliza posteriormente para ajustar los parámetros internos del modelo, de modo que la próxima vez su predicción sea un poco mejor. Este ciclo se repite durante un número de iteraciones dado o hasta que se cumpla una condición de parada. Esta condición suele ser. que se haya sobrepasado un umbral de rendimiento o convergencia o, que el modelo no mejore de forma significativa tras cierto número de iteraciones.

Los modelos de aprendizaje automático clásicos, como Random Forest, suelen tener un proceso de entrenamiento más directo y estandarizado. En cambio, los modelos de \textit{deep learning}, como las redes neuronales, utilizan un mecanismo más complejo basado en el algoritmo de \textit{backpropagation} para ajustar sus parámetros.

Para asegurar que el modelo se evalúa de forma justa y, para evitar que este simplemente <<memorice>> los datos, se utilizan técnicas como la división del \textit{dataset} en conjuntos de entrenamiento, validación y prueba (\textit{train-validation-test split}). Una técnica aún más robusta es la validación cruzada (\textit{cross-validation}), que consiste en dividir el \textit{dataset} en $k$ partes o \textit{folds}. El modelo se entrena $k$ veces, usando en cada ocasión una de las partes no usadas anteriormente para la validación y el resto para el entrenamiento. Esto garantiza que la evaluación del rendimiento sea mucho más fiable y menos dependiente de la suerte en el reparto inicial de los datos.

\subsection{Interpretabilidad de un modelo}

La interpretabilidad de un modelo se refiere a nuestra capacidad para entender y explicar cómo llega a sus conclusiones. Muchos modelos clásicos de aprendizaje automático, como un árbol de decisión, son relativamente transparentes: podemos seguir su lógica interna para ver por qué han tomado una decisión. Sin embargo, a medida que los modelos se vuelven más complejos, como los ensembles o, especialmente, las redes neuronales profundas, su funcionamiento interno se convierte en una especie de <<caja negra>>. Todo se reduce a millones de operaciones matemáticas y pesos ajustados que, aunque funcionan, no nos dicen explícitamente por qué funcionan.

Aquí es donde nace el campo de la interpretabilidad de modelos (\textit{XAI - Explainable AI}). La confianza en un sistema de IA, sobre todo en campos críticos como la medicina o la ciberseguridad, depende cada vez más de que sus decisiones sean explicables. Por ejemplo, un modelo que predice la probabilidad de que un paciente tenga una enfermedad con un 99\% de acierto es impresionante, pero poco fiable para un médico si no puede explicar en qué se basa. Si, por el contrario, el modelo puede señalar qué factores (características) han influido más en su predicción para ese paciente, gana una enorme credibilidad.

Existen principalmente dos enfoques de interpretabilidad. El análisis local se centra en explicar una predicción individual (p. ej., ¿por qué esta aplicación concreta fue clasificada como \textit{malware}?). El análisis global, en cambio, busca dar una visión más general del comportamiento del modelo, explicando qué características son las más importantes para él en promedio.

\subsection{UMAP (\textit{Uniform Manifold Approximation and Projection})}

UMAP (\textit{Uniform Manifold Approximation and Projection})~\cite{mcinnes2018umap} es un algoritmo moderno de reducción de dimensionalidad basado en principios matemáticos de la topología y el aprendizaje de variedades (\textit{manifold learning}). La idea fundamental de UMAP es que la mayoría de los conjuntos de datos de alta dimensión en realidad viven en una <<variedad>> o superficie de una dimensionalidad mucho más baja que está <<curvada>> dentro de ese espacio. La misión de UMAP es encontrar esa variedad y <<desplegarla>> en un espacio de pocas dimensiones (como una hoja de papel 2D) perdiendo la menor cantidad de información estructural posible, facilitando así su visualización e interpretabilidad.

Para lograrlo, su algoritmo se basa en dos etapas. Primero, construye una representación topológica de los datos en el espacio de alta dimensión. Para ello, crea un grafo ponderado donde los pesos de las aristas se calculan asumiendo que la distancia entre los puntos varía localmente a lo largo de la variedad. En la segunda etapa, busca una <<incrustación>> (\textit{embedding}) de baja dimensión de ese grafo, es decir, una disposición de los puntos en 2D o 3D, que tenga una estructura topológica lo más parecida posible. Para medir esta <<similitud>> entre el grafo de alta y baja dimensión, utiliza una función de coste basada en la entropía cruzada, que optimiza para encontrar la mejor proyección posible.

\subsection{Función de pérdida (\textit{Loss function})}

Para que un modelo pueda aprender, necesita una forma de saber si sus predicciones son correctas o no. La función de pérdida (\textit{loss function}), o función de coste, cumple exactamente este papel, es una fórmula matemática que estima el error que el modelo está cometiendo en sus predicciones. Después de cada predicción que este hace, la función de pérdida la compara con la respuesta correcta y calcula un número que representa cuánto de <<equivocado>> estaba el modelo. Si el número es alto, el error es grande; si es cercano a cero, indica que la predicción fue muy buena.

Existen diferentes tipos de funciones de pérdida y, su elección depende de la tarea específica a realizar por el modelo. Por ejemplo, para problemas de regresión se suele utilizar el error cuadrático medio (\textit{mean squared error}), mientras que para problemas de clasificación se emplea la entropía cruzada (\textit{cross-entropy}).

A su vez, durante el entrenamiento, es fundamental monitorizar dos métricas de pérdida:

\begin{itemize}
	\item \textbf{Pérdida de entrenamiento (\textit{training loss}):} Es el valor de la función de pérdida calculado sobre el conjunto de datos de entrenamiento. Este valor es el que el algoritmo de optimización intenta minimizar directamente ajustando los parámetros del modelo. Una disminución constante de la pérdida de entrenamiento indica que el modelo está aprendiendo los patrones presentes en los datos de entrenamiento.
	
	\item \textbf{Pérdida de validación (\textit{validation loss}):} Es el valor de la función de pérdida calculado sobre un conjunto de datos separado, llamado conjunto de validación, que el modelo no utiliza para aprender. Esta métrica es un indicador mucho más fiable de la capacidad de generalización del modelo, es decir, de su rendimiento en datos nuevos y no vistos. Si la pérdida de entrenamiento sigue disminuyendo pero la de validación comienza a aumentar, es una señal clara de sobreajuste (\textit{overfitting}).
\end{itemize}

\subsection{Sobreajuste (\textit{overfitting}) y subajuste (\textit{underfitting})}

El sobreajuste (\textit{overfitting}) y el subajuste (\textit{underfitting}) son posiblemente los dos problemas más comunes que pueden surgir durante el entrenamiento de modelos de aprendizaje automático, representando extremos opuestos del rendimiento de un modelo. Ambos fenómenos se refieren a la incapacidad del modelo para generalizar adecuadamente a partir de los datos de entrenamiento y, por lo tanto, para realizar predicciones precisas sobre datos nuevos y no vistos.

En concreto, el sobreajuste ocurre cuando un modelo aprende los datos de entrenamiento <<demasiado bien>>, capturando no solo los patrones subyacentes, sino también el ruido y las fluctuaciones aleatorias específicas del conjunto de datos. Dicho de otra forma, el modelo se vuelve excesivamente complejo y <<memoriza>> los ejemplos de entrenamiento en lugar de aprender a generalizar. La manera en la que esto se manifesta durante el entrenamiento es mediante un rendimiento excelente en los datos de entrenamiento (\textit{training loss} muy baja y que cae constantemente) pero un rendimiento pobre en los datos de validación (\textit{validation loss} alta y que se estanca o empieza a subir tras un punto en el entrenamiento).

Por otro lado, el subajuste sucede cuando un modelo es demasiado simple para capturar la complejidad y los patrones inherentes en los datos. En este caso, el modelo no tiene la capacidad suficiente para aprender relaciones significativas, lo que resulta en un mal rendimiento tanto en el conjunto de entrenamiento como en el de validación. Un modelo subajustado no aprende eficazmente, manifestándose en una \textit{loss} alta para ambos conjuntos de datos.

\subsection{Descenso de gradiente, aprendizaje y \textit{backpropagation}}

Tres conceptos intrínsecamente ligados, los cuales forman el núcleo del mecanismo que permite a las redes <<aprender>> de los datos. El proceso de aprendizaje consiste en ajustar iterativamente los parámetros del modelo (pesos y sesgos) para minimizar una función de pérdida.

El descenso de gradiente~\cite{amari2006theory} (\textit{gradient descent}) es el algoritmo de optimización que permite realizar este ajuste. La función de pérdida puede visualizarse como una superficie con valles y colinas en un espacio multidimensional, donde cada punto de la superficie representa un valor de pérdida para una configuración específica de los pesos del modelo. El objetivo es encontrar el punto más bajo de esta superficie (el mínimo global de la pérdida). El <<gradiente>> es un vector que apunta en la dirección del máximo crecimiento de la función en un punto dado. Por lo tanto, para <<descender>> y minimizar la pérdida, el algoritmo da pequeños pasos en la dirección opuesta al gradiente. El tamaño de estos pasos está controlado por un hiperparámetro llamado tasa de aprendizaje (\textit{learning rate}). Una tasa adecuada es crucial: si es demasiado pequeña, el aprendizaje será muy lento; si es demasiado grande, el algoritmo podría sobrepasar el mínimo y no converger nunca.

Para ahora poder realizar dicho ajuste de los pesos es necesario usar un famoso algoritmo conocido como \textit{backpropagation}~\cite{rumelhart1986learning}. Tras realizar una pasada hacia adelante (\textit{forward pass}) para obtener una predicción y calcular la función de pérdida, \textit{backpropagation} aplica la regla de la cadena del cálculo para propagar el error hacia atrás, desde la capa de salida hasta la capa de entrada. De esta manera, calcula la contribución de cada peso y sesgo al error total, es decir, el gradiente de la pérdida con respecto a cada parámetro.

El proceso de aprendizaje completo se resume a continuación:

\begin{itemize}
	\item \textbf{Pasada hacia adelante (\textit{forward pass}):} Se introducen los datos en la red y se propagan a través de las capas para generar una predicción.
	
	\item \textbf{Cálculo de la pérdida:} Se compara la predicción con el valor real usando la función de pérdida para cuantificar el error.

	\item \textbf{Pasada hacia atrás (\textit{backward pass} / \textit{backpropagation}):} Se calcula el gradiente de la función de pérdida con respecto a cada parámetro de la red.

	\item \textbf{Actualización de parámetros:} Se utiliza el descenso de gradiente para ajustar los pesos y sesgos en la dirección opuesta al gradiente, reduciendo así la pérdida.
\end{itemize}

Este ciclo se repite de forma iterativa con diferentes lotes (\textit{batches}) de datos, permitiendo que la red refine gradualmente sus parámetros y mejore su rendimiento.

\subsection{Tokenización}

Los modelos de inteligencia artificial, en su más puro estado, son modelos matemáticos complejos los cuales consiguen aprender patrones de sus datos de entrada y permiten realizar posteriormente predicciones en base a dichos patrones.

El problema fundamental que esto presenta a la hora de procesar diferentes tipos de datos es principalmente que los modelos matemáticos se basan en el uso de números y la búsqueda de patrones en datos numéricos. Esto implica que si usamos datos que no sean numéricos, como puede ser el texto, caemos en un grave problema puesto que no podemos simplemente aplicar el modelo a este tipo de dato directamente puesto no está preparado para funcionar con el.

Una solución a este problema es emplear el proceso de tokenización, el cual consiste en romper el texto que se obtiene como entrada en unidades más sencillas llamadas tokens que el modelo pueda llegar a entender posteriormente. Este proceso puede realizarse de diferentes maneras, ya sea partiendo cada palabra en un token individual, partiendo las palabras en subpalabras o incluso procesando cada carácter de manera independiente.

El proceso de tokenización parece bastante trivial a primera vista pero presenta muchos retos como pueden ser los siguientes. Procesar diferentes idiomas se vuelve rápidamente un problema en función del método que se utilice para obtener los tokens, diferentes idiomas permiten contracciones o expresiones especiales las cuales pueden ser interpretadas de diferentes maneras y han de poder ser divididas en tokens que mantengan dicho significado. También es posible que un texto presente errores gramaticales, símbolos especiales o jerga específica de un campo que ha de ser entendida y tokenizada correctamente para mantener su significado.

\subsection{Vocabulario}

Para que un modelo pueda entender los tokens que se obtienen en el paso de la tokenización, es necesario convertir dichos tokens en números de alguna forma. Esto puede realizarse de forma sencilla mediante el uso de un diccionario o vocabulario para el modelo.

El vocabulario del modelo no es más que la colección de todos los tokens que se pretende que el modelo pueda comprender asociados a un valor numérico que siempre será el mismo. De esta forma, el texto de entrada puede ser dividido en tokens, los cuales, a su vez, pueden ser convertidos siempre en el mismo valor numérico mediante el uso de un diccionario. Permitiendo así que el modelo procese texto y pueda encontrar patrones en este.

De forma similar al proceso de la tokenización, la creación y selección de un vocabulario presenta muchos problemas que pueden no ser tan evidentes a primera vista, algunos de ellos son, la extensión del diccionario y el número de tokens que son necesarios en este para poder procesar diferentes idiomas o, la necesidad de poder incluir valores de control que puedan ser usados en casos en los que un token no exista o se le quiera dar una información especial a ciertos tokens en algunas circunstancias.

\section{Conceptos de ciberseguridad}

Antes de poder construir una buena defensa, es imprescindible conocer a fondo la amenaza que se pretende combatir. Este capítulo se dedica a explorar el mundo del software malicioso o \textit{malware}, el principal adversario que nuestro modelo de inteligencia artificial está diseñado para combatir. Se comenzará por definir qué es el \textit{malware} y se describirán sus diferentes manifestaciones, desde los virus clásicos hasta el \textit{ransomware} moderno. A continuación, se explicarán las técnicas que se utilizan para inspeccionar programas sospechosos, sentando así las bases para comprender de dónde extraerá nuestro modelo la información para tomar sus decisiones.

\subsection{\textit{Malware}}

% citar yadav2022review

El término \textit{malware}, (proveniente de \textit{malicious software}) o software malicioso, se refiere a cualquier programa, código o script diseñado con el propósito explícito de causar daño, comprometer la seguridad, o realizar actividades no autorizadas en un sistema informático, una red o un dispositivo. Coloquialmente, se utiliza el término \textit{malware} para referirse a una amplia gama de diferentes subcategorías de programas que puedan considerarse dañinos, cada una con características, formas de ataque y objetivos específicos, pero todas compartiendo la intención de perjudicar al usuario, robar información, o tomar el control de aquellos sistemas afectados.

Una clasificación bastante común del \textit{malware} es la siguiente:

\subsubsection{Virus}
Un virus es considerado el comodín de los programas maliciosos, al menos desde un punto de vista coloquial, ambas palabras son intercambiable. En cambio, desde un punto de vista técnico, la definición de un virus informático es equiparable a su equivalente biológico. Un virus es un tipo de \textit{malware} que requiere de un huésped para poder realizar su función, es decir, suele ir incrustado o adjunto a otros archivos, generalmente, ejecutables o documentos los cuales, una vez abiertos, permiten al virus infectar el sistema y realizar acciones no deseadas. Una característica importante de los virus, es el hecho de que pueden auto replicarse, es decir, una vez infectada una máquina, pueden emplear diferentes métodos y técnicas para propagarse a otras, comúnmente, estas suelen ser mediante el uso de la red o mediante el uso de medios físicos extraibles. Los virus suelen ser detectados mediante firmas específicas, aunque las técnicas de ofuscación y polimorfismo pueden dificultar su identificación.

\subsubsection{Gusano (\textit{worm})}
Un gusano es un tipo de \textit{malware} diseñado para propagarse automáticamente a través de las redes informáticas, explotando las vulnerabilidades de los diferentes sistemas o utilizando técnicas de ingeniería social para engañar a los usuarios. A diferencia de los virus, los gusanos no necesitan de un huésped al cual adjuntarse para poder desempeñar su función y para auto replicarse, puesto que pueden propagarse de manera independiente. Su principal objetivo es infectar tantos dispositivos como sea posible, lo que puede resultar en la saturación de las redes, la degradación del rendimiento del sistema, o la creación de puertas traseras para permitir el paso a otros tipos de \textit{malware}. Los gusanos son particularmente peligrosos en entornos corporativos, donde pueden propagarse rápidamente a través de las redes internas de una empresa o una organización.

\subsubsection{Troyano (\textit{trojan})}
Un troyano es un tipo de \textit{malware} que se hace pasar por un \textit{software} legítimo o útil para engañar a los usuarios y lograr su ejecución. Una vez instalado, un troyano permite a un atacante acceder o controlar el sistema infectado de manera remota, sin el conocimiento del usuario. Los troyanos no se replican por sí mismos, pero pueden realizar gran variedad de acciones maliciosas, principalmente, robar información confidencial, instalar otros tipos de \textit{malware}, o convertir el sistema en parte de una \textit{botnet}. Su nombre proviene del mito del Caballo de Troya, perteneciente a la mitología griega, ya que, al igual que sucede en la historia, el troyano parece inofensivo en un principio pero oculta una gran amenaza en su interior.

\subsubsection{\textit{Ransomware}}
El \textit{ransomware} es un tipo de \textit{malware} diseñado para cifrar los archivos del usuario o bloquear el acceso al sistema, exigiendo un rescate, \textit{ransom}, generalmente en criptomonedas, a cambio de restaurar el acceso. Este tipo de \textit{malware} ha ganado mucha popularidad en los últimos años debido a su impacto devastador en individuos, empresas e incluso instituciones gubernamentales. El \textit{ransomware} suele propagarse a través de correos electrónicos de \textit{phishing}, descargas maliciosas o \textit{exploits} de vulnerabilidades. Una vez este es activado, el comportamiento más típico consiste en mostrar al usuario un mensaje (\textit{ransom note}) con las instrucciones para pagar el rescate. Es importante destacar que, aunque se pague la tasa correspondiente al rescate dentro del tiempo establecido, no hay ninguna garantía de que los atacantes cumplan con su promesa de desbloquear los archivos. Es por ello, por lo que, este tipo de \textit{malware} es extremadamente peligroso puesto que juega con el factor de perder un recurso muy preciado, como puede ser la información, además de utilizar la desesperación de los usuarios en su contra.

\subsubsection{\textit{Spyware} o \textit{Info stealer}}
El \textit{spyware} es un tipo de \textit{malware} diseñado para recopilar información del usuario sin su consentimiento. Esta información puede incluir contraseñas, datos bancarios, historiales de navegación, métodos de entrada (como pueden ser las pulsaciones de las teclas de un teclado) o cualquier otro dato sensible que pueda posteriormente ser vendido o utilizado en contra de los usuarios. El \textit{spyware} suele operar de manera sigilosa, sin mostrar signos evidentes de su presencia, lo que dificulta su detección. 

En términos generales, existen dos variantes de \textit{spyware}, la primera de ellas se instala en el sistema de forma permanente y siempre está activa, monitorizando la actividad del usuario de manera constante, mandando cualquier información sensible que este use, vea o teclee a un servidor externo para que los atacantes la puedan utilizar o vender, la única ventaja que presenta esta variante es que deja rastro y es más sencilla de detectar. Por otro lado, la segunda variante simplemente se ejecuta una vez, recopila toda la información que pueda y luego se borra a si misma para no dejar ni el más mínimo rastro de su ejecución. Para muchos, esta segunda variante es considerada incluso más peligrosa que la primera puesto que la víctima puede tardar semanas, meses o incluso años en darse cuenta de que su información a sido comprometida. 

Además de robar información, algunos tipos de \textit{spyware} pueden modificar la configuración del sistema, instalando \textit{software} adicional o redirigiendo el tráfico de red, generalmente, para evitar su detección. Este tipo de \textit{malware} es comúnmente distribuido a través de descargas no autorizadas, correos electrónicos de \textit{phishing}, o \textit{software} gratuito (\textit{freeware}) que incluye componentes ocultos.

\subsubsection{\textit{Adware}}
El \textit{adware} es un tipo de \textit{software} que muestra publicidad no deseada, a menudo de manera intrusiva, en el dispositivo del usuario. Aunque no siempre es malicioso, el \textit{adware} puede ser molesto y afectar negativamente a la experiencia del usuario. En algunos casos, el \textit{adware} incluye funcionalidades adicionales para rastrear el comportamiento del usuario y mostrar anuncios personalizados, lo que puede considerarse una violación de la privacidad. El \textit{adware} suele distribuirse junto con \textit{software} gratuito, y los usuarios pueden instalarlo sin darse cuenta al aceptar los términos y condiciones sin leerlos detenidamente.

\subsubsection{PUP (\textit{Potentially Unwanted Program} o Programa Potencialmente no Deseado)}
Un PUP (\textit{Potentially Unwanted Program}, por sus siglas en inglés) es un tipo de \textit{software} que, aunque no es necesariamente malicioso, puede ser considerado no deseado por el usuario. Los PUPs incluyen aplicaciones como barras de herramientas (\textit{toolbars}), optimizadores de sistema, o \textit{software} de publicidad que se instalan sin el consentimiento explícito del usuario. Aunque no siempre son dañinos, los PUPs pueden ralentizar el sistema, mostrar anuncios no deseados, o recopilar información acerca del usuario. Muchos antivirus y soluciones de seguridad clasifican los PUPs como una categoría separada de \textit{malware}, ya que su impacto puede variar desde simplemente molesto hasta potencialmente peligroso.

\subsubsection{\textit{Rootkit}}
Un \textit{rootkit} es un conjunto de herramientas o \textit{software} diseñado para otorgar a un atacante acceso privilegiado y persistente a un sistema, mientras oculta su presencia tanto del usuario como de cualquier \textit{software} de seguridad que pueda estar presente en el sistema. Los \textit{rootkits} suelen operar a nivel de \textit{kernel} (conocido como Ring 0 en muchos casos) o del sistema operativo, lo que les permite manipular cualquier funcionalidad del sistema, incluso aquellas de las que el usuario posiblemente desconoce puesto que están ocultas por el sistema operativo para facilitar su uso. Esto permite a los \textit{rootkits} ser uno de los \textit{malware} más peligrosos debido a que pueden evadir la gran mayoría de soluciones de seguridad y antivirus puesto que operan con los mismos privilegios que estos o incluso más altos. Una vez instalado, un \textit{rootkit} puede ser utilizado para instalar otros tipos de \textit{malware}, robar información, o convertir el sistema en parte de una \textit{botnet}. Debido a su capacidad para ocultarse, los \textit{rootkits} son particularmente difíciles de detectar y eliminar, y a menudo requieren herramientas especializadas o en muchos casos, la reinstalación completa del sistema para poder deshacerse de ellos.

\subsubsection{\textit{Botnet}}
Una \textit{botnet} es una red de dispositivos infectados (llamados \textit{bots} o \textit{zombies}) controlados de manera remota por un atacante, conocido como \textit{botmaster}. Los dispositivos infectados pueden incluir computadoras, servidores, dispositivos IoT, y otros equipos conectados a internet. Las \textit{botnets} son utilizadas para realizar una variedad de tareas maliciosas, como pueden ser, ataques de denegación de servicio distribuido (DDoS), envío masivo de correos no deseados (\textit{spam}), minería de criptomonedas, o robo de información masivo. Los dispositivos infectados suelen ser controlados a través de un servidor externo, y los usuarios generalmente no son conscientes de que su dispositivo forma parte de una \textit{botnet}. La creación y gestión de \textit{botnets} es una de las actividades que más dinero genera para los ciberdelincuentes, ya que les permite llevar a cabo ataques a gran escala con un impacto muy significativo.

\subsection{Análisis estático}

Técnica de detección de \textit{malware} que se realiza sin la necesidad de ejecutar el programa en cuestión. Este método se basa en la obtención, inspección y evaluación de las características que se pueden extraer de un archivo binario, tales como su estructura, código fuente (si está disponible), indicios de obfuscación u otras técnicas de ocultación, cadenas de texto incrustadas en este, firmas digitales, huella digital \textit{hash o signature} del archivo, secuencias de bytes concretas, cabeceras del programa, metadatos incrustados, desensamblado del ejecutable y otras propiedades que pueden ser extraídas directamente del archivo. Las ventajas que este enfoque presenta son: su simplicidad, rapidez y bajo coste computacional, ya que no requiere de entornos de ejecución específicos ni de hardware especializado para probar el comportamineto del programa. Sin embargo, el mayor problema de este tipo de análisis es su dificultad para detectar malware que utiliza técnicas avanzadas de ofuscación o cifrado, ya que estas prácticas dificultan la extracción de información útil del binario.

\subsection{Análisis dinámico}

Técnica de detección de \textit{malware} que consiste en evaluar el comportamiento de un programa mediante su ejecución en un entorno controlado, con el objetivo de observar sus interacciones con el sistema operativo, los recursos de este y otros programas. En este enfoque, se monitorizan actividades como la modificación de archivos, el tráfico de red generado, la creación de procesos o la inyección de código en estos, lo cual permite identificar patrones de comportamiento asociados con programas maliciosos. A diferencia del análisis estático, el análisis dinámico ofrece una mayor precisión, ya que puede detectar comportamientos maliciosos que no son evidentes simplemente escaneando el archivo de manera estática, como el uso de técnicas de ofuscación. Sin embargo, este tipo de análisis sigue teniendo sus inconvenientes, por un lado, es más complejo, requiere de más recursos computacionales y es más costoso de implementar, dado que involucra la ejecución real del código en un entorno controlado, generalmente una máquina virtual (\textit{sandbox}). Por otro lado, también es poco eficiente contra casos en los que el \textit{malware} detecta el hecho de que está siendo analizado y oculta su comportamiento malicioso. Además, puede no ser adecuado para dispositivos con recursos limitados, como dispositivos IoT (\textit{Internet Of Things}) o móviles, debido a sus altos requerimientos de hardware y tiempo.

\subsection{Análisis híbrido}

Metodo de detección de \textit{malware} el cual combina las fortalezas tanto del análisis estático como del dinámico. En este método, el programa se ejecuta en un entorno controlado, y durante su ejecución, se realizan \textit{dumps} de memoria de manera periódica o en respuesta a comportamientos sospechosos. Estos volcados de memoria son posteriormente analizados utilizando técnicas de análisis estático para identificar posibles  patrones maliciosos, tales como la inyección de código en procesos ajenos, manipulación de memoria que no le pertenece al programa o modificaciones en partes protegidas de la memoria pertenecientes al sistema oeprativo. Este enfoque permite una detección más precisa de \textit{malware} que utiliza técnicas avanzadas de ocultamiento, ya que combina la observación del comportamiento en tiempo real con la inspección detallada del estado de la memoria. Sin embargo, el análisis híbrido es el más complejo y costoso de implementar, ya que requiere tanto de infraestructura de virtualización como de herramientas para realizar un buen análisis de memoria. A pesar de todo, suele ofrecer los mejores resultados en términos de detección.

\subsection{Huella digital (\textit{fingerprinting})}

El fingerprinting o, la generación de huellas digitales de archivos, es una técnica utilizada para identificar de manera única un archivo mediante la aplicación de funciones criptográficas de \textit{hashing}. Este proceso consiste en calcular un \textit{hash} a partir del contenido completo del archivo utilizando algoritmos como MD5, SHA-1, SHA-256 u otros. El resultado es una cadena de longitud fija que actúa como un identificador único para ese archivo. Cualquier modificación, por mínima que sea, en el contenido del archivo resultará en un \textit{hash} completamente diferente, lo que permite detectar alteraciones o corrupciones en estos.

Esta técnica es muy utilizada en la verificación de la integridad de archivos, la detección de duplicados, y la identificación de malware conocido al comparar el \textit{hash} que este genera con una base de datos de muestras previamente catalogadas. Sin embargo, una limitación importante del \textit{fingerprinting} es su sensibilidad extrema a cambios mínimos, lo que dificulta la identificación de archivos que han sido ligeramente modificados pero que conservan una estructura o funcionalidad. Esto implica que incluso cambiar un bit en el \textit{padding} del archivo, hace que este ya no se detecte como malware al tener una huella digital diferente.

\subsection{Huella digital difusa (\textit{fuzzy hashing})}

El \textit{fuzzy hashing}, o \textit{hashing} difuso, es una técnica que extiende el concepto del \textit{hashing} tradicional al permitir la comparación de archivos basada en similitudes parciales en lugar de una coincidencia exacta. A diferencia del \textit{hashing} convencional, que opera sobre el archivo completo, el \textit{fuzzy hashing} divide el archivo en bloques o segmentos y calcula un \textit{hash} para cada uno de ellos. Este enfoque por bloques permite identificar similitudes entre archivos incluso cuando solo una porción de su contenido ha sido modificada.

El \textit{fuzzy hashing} es particularmente útil en el análisis forense digital y la detección de \textit{malware}, ya que permite identificar variantes de archivos maliciosos que han sido modificaods para evadir su detección, pero que conservan partes significativas de su código original. Al comparar dos \textit{hashes} difusos, es posible calcular un grado de similitud basado en la cantidad de bloques que coinciden entre ambos. Esto se logra mediante algoritmos especializados como SSDeep o TLSH, que están diseñados para generar \textit{hashes} difusos y medir la similitud entre ellos.

\subsection{Android y su formato ejecutable (\textit{Android Package Kit} o APK)}

Android es el sistema operativo móvil más utilizado del mundo. Originalmente, sus aplicaciones se programaban principalmente en Java, un lenguaje que se ejecuta sobre una máquina virtual conocida como la \textit{Java Virtual Machine} (JVM). Android adoptó este enfoque pero con su propia máquina virtual optimizada para dispositivos móviles, primero llamada Dalvik y ahora ART (\textit{Android Runtime}). Las aplicaciones modernas suelen usar Kotlin, un lenguaje más actual pero que sigue compilando al mismo formato compatible con ART.

El formato ejecutable de una aplicación de Android es un APK (\textit{Android Package Kit}). En esencia, un APK no es más que un archivo comprimido en formato ZIP que contiene todo lo necesario para que la aplicación se pueda instalar y funcionar correctamente. Su estructura incluye elementos clave como el archivo \texttt{AndroidManifest.xml}, que sería similar al <<DNI>> de la aplicación: declara sus componentes, permisos necesarios y características. También contiene los archivos \texttt{classes.dex}, que incluyen el código de la aplicación compilado, y carpetas con recursos como imágenes o textos.

Desde el punto de vista de la seguridad, esta estructura tiene una doble cara. Al ser básicamente un ZIP, es relativamente sencillo de descomprimir e inspeccionar, lo que facilita enormemente el análisis estático. Sin embargo, esta misma simplicidad también facilita que un atacante pueda modificarlo (haciendo ingeniería inversa de su contenido). Para dificultar este proceso, los desarrolladores pueden emplear técnicas de ofuscación de código, que lo hacen más difícil de leer y entender, aunque no imposible para alguien que tenga el conocimiento necesario.

\section{Otros conceptos}

Dadas las bases anteriores necesarias para entender el proyecto, se dejan a continuación aquellos conceptos adicionales que pueden ser de gran ayuda para comprender mejor otros aspectos relevantes del proyecto.

\subsection{\textit{Framework} web}

Un \textit{framework} web es un conjunto de herramientas, bibliotecas y componentes predefinidos que facilitan el desarrollo de aplicaciones y páginas web. Su propósito es ofrecer una estructura básica que los desarrolladores pueden utilizar para crear aplicaciones de manera más eficiente, sin tener que comenzar desde cero. Los \textit{frameworks} web incluyen funcionalidades que resuelven tareas comunes, como el manejo de rutas (URLs), la conexión a bases de datos, la autenticación de usuarios y la seguridad, lo que ahorra tiempo y esfuerzo durante el desarrollo.

Además, proporcionan una organización estructurada para el código, lo que facilita la colaboración entre desarrolladores y simplifica el mantenimiento de los proyectos a largo plazo. Muchos \textit{frameworks} también incluyen mecanismos de seguridad incorporados para proteger las aplicaciones contra amenazas comunes y herramientas que automatizan tareas repetitivas, como la gestión de dependencias y la ejecución de pruebas. Todo esto permite construir aplicaciones web escalables, seguras y fáciles de mantener.

Algunos \textit{frameworks} web populares son: Django (para Python), Ruby on Rails (para Ruby), Angular y React (para JavaScript), y Laravel (para PHP).
