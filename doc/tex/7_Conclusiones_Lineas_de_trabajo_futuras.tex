\capitulo{7}{Conclusiones y Líneas de trabajo futuras}

Este capítulo final tiene como objetivo recapitular los hallazgos y resultados obtenidos a lo largo del desarrollo del proyecto, ofreciendo una reflexión crítica sobre el cumplimiento de los objetivos y las lecciones aprendidas. Asimismo, se propondrán diversas líneas de trabajo futuras que podrían servir como continuación de esta investigación, explorando nuevas vías para mejorar, optimizar y ampliar la solución desarrollada.

\section{Conclusiones}

El desarrollo de este proyecto ha culminado en la creación de un sistema completo y funcional para la detección de \textit{malware} en Android mediante análisis estático e inteligencia artificial. A lo largo de este recorrido, se han obtenido una serie de conclusiones técnicas y conceptuales de gran relevancia.

La primera y más evidente conclusión es que \textbf{el enfoque propuesto es altamente eficaz}. Todos los modelos entrenados, tanto la red neuronal como los clasificadores clásicos, han demostrado una capacidad de detección muy buena, alcanzando métricas de \textit{recall}, precisión y AUC superiores al 98\% en la mayoría de los casos. Esto confirma de manera rotunda la hipótesis central del proyecto: es perfectamente viable construir un clasificador de alto rendimiento basándose únicamente en las características estáticas extraídas de un archivo APK, sin necesidad de recurrir a técnicas de análisis dinámico, que son más lentas y costosas. El objetivo de obtener un modelo útil y con buen rendimiento se ha cumplido con creces.

Sin embargo, la conclusión más importante y reveladora de este trabajo es el \textbf{papel fundamental del \textit{embedder} como motor de ingeniería de características}. Si bien la red neuronal por sí sola es un clasificador muy competente, su verdadero valor radica en su capacidad para aprender una representación numérica densa y significativa a partir de datos de entrada complejos y heterogéneos. La visualización del espacio de \textit{embeddings} mediante UMAP demostró de forma sorprendente que este componente era capaz de separar casi perfectamente las aplicaciones benignas de las maliciosas en un espacio de alta dimensionalidad. Este preprocesamiento inteligente es la razón por la cual modelos mucho más simples, como la Regresión Logística o XGBoost, pudieron alcanzar un rendimiento tan espectacular. El proyecto demuestra en la práctica un principio fundamental del aprendizaje automático: la calidad de los datos y de sus representaciones es, a menudo, más importante que la complejidad del modelo clasificador final. Esto valida la estrategia híbrida como un enfoque extremadamente poderoso.

En tercer lugar, el análisis de interpretabilidad mediante SHAP ha permitido \textbf{abrir la <<caja negra>> de los modelos y validar su proceso de razonamiento}. Los resultados mostraron que todos los clasificadores, de manera consistente, basaban sus decisiones en las características más lógicas: los permisos solicitados, las llamadas a la API y los componentes declarados en la aplicación (actividades, servicios y receptores). Esto no solo aumenta la confianza en las predicciones, sino que también cumple el objetivo de crear un modelo interpretable, demostrando que no estaba aprendiendo de artefactos o sesgos extraños en los datos, sino de indicadores de comportamiento reales.

Desde una perspectiva de ingeniería, otra conclusión importante es que la \textbf{optimización y el diseño del \textit{pipeline} de datos son tan críticos como el propio modelo}. La transición del prototipo al modelo final reveló graves cuellos de botella relacionados con el manejo de la memoria y el preprocesamiento de datos, cuya solución requirió bastante trabajo de optimización. Esto subraya la importancia de entender no solo los algoritmos, sino también las herramientas de software (Pandas, NumPy, PyTorch) a un nivel más bajo para construir sistemas eficientes y escalables.

Finalmente, la exploración de la \textbf{cuantización del modelo arrojó resultados limitados para esta arquitectura específica}. Se concluyó que las técnicas de cuantización post-entrenamiento no son efectivas cuando la mayor parte del peso de un modelo reside en las capas de \textit{embedding}. Esto supone una conclusión técnica valiosa: para llevar un modelo de estas características a entornos con recursos limitados, como un dispositivo móvil, se requerirían enfoques más avanzados, como el entrenamiento consciente de la cuantización y con ello, el posible rediseño de la arquitectura.

En resumen, se han cumplido todos los objetivos principales del proyecto. Se ha realizado una investigación del estado del arte, se ha construido un \textit{dataset} propio con un \textit{pipeline} replicable, se ha entrenado y optimizado una red neuronal de alto rendimiento y se ha realizado un análisis comparativo e interpretativo exhaustivo, culminando en una aplicación web funcional. A nivel personal, el proyecto ha permitido adquirir una profunda experiencia práctica en todo el ciclo de vida de un proyecto de IA, desde la investigación y la gestión de datos hasta el despliegue.

\section{Líneas de trabajo futuras}

Todo proyecto de investigación y desarrollo abre nuevas puertas y plantea nuevas preguntas. Este trabajo no es una excepción. A continuación, se detallan varias líneas de trabajo futuras que podrían dar continuidad y expandir las ideas y soluciones aquí presentadas.

La primera y más obvia línea de futuro sería el \textbf{despliegue y la evaluación del modelo en un dispositivo de Android real}. Aunque la aplicación web es una excelente herramienta de demostración, el objetivo final de un sistema de este tipo sería poder analizar las aplicaciones directamente en el teléfono del usuario. Esto presenta importantes desafíos de optimización, puesto que, como se concluyó en el análisis de cuantización, el modelo actual es demasiado grande (cerca de 1 GB). Una línea de investigación futura muy interesante sería explorar técnicas avanzadas para reducir su tamaño, como el \textit{pruning} (poda de conexiones neuronales), el entrenamiento consciente de la cuantización (\textit{Quantization-Aware Training}) o incluso el rediseño del modelo utilizando arquitecturas específicamente pensadas para dispositivos móviles, como las inspiradas en MobileNet, adaptándolas para procesar las características estáticas en lugar de imágenes.

Una segunda vía de expansión natural sería la de \textbf{escalar el modelo y el \textit{dataset}}. Durante la búsqueda de hiperparámetros, se encontró una configuración de modelo mucho más grande y con mayor capacidad teórica de aprendizaje, pero que no ofrecía una mejora de rendimiento significativa con el \textit{dataset} actual de 20\,000 aplicaciones. Una línea de trabajo futura consistiría en ampliar de forma masiva el \textit{dataset}, utilizando el \textit{pipeline} ya creado para procesar muchas más APKs de AndroZoo. Con un volumen de datos mucho mayor, sería posible que el modelo más grande sí fuera capaz de aprender patrones más sutiles y complejos, superando el rendimiento del modelo actual y de los clasificadores clásicos.

En tercer lugar, se podría \textbf{enriquecer el conjunto de características extraídas}. Aunque el análisis estático ha demostrado ser muy potente, la incorporación de otros tipos de información podría mejorar aún más la precisión. Se podría explorar la inclusión de características basadas en grafos, como los grafos de llamadas a la API o los grafos de flujo de control, y utilizar modelos de Redes Neuronales de Grafos (GNN) para analizarlos, siguiendo la línea de trabajos como el de MAPAS~\cite{kim2022mapas}. Otra opción aún más ambiciosa sería evolucionar hacia un <<modelo híbrido>>, incorporando un número limitado de características dinámicas obtenidas de la ejecución de la aplicación en un \textit{sandbox}, para complementar la visión estática con información acerca del comportamiento real de las aplicaciones.

Finalmente, la \textbf{aplicación web de demostración podría ser mejorada y expandida}. Se podrían añadir funcionalidades como un historial de análisis persistente para cada usuario, al igual que un sistema de usuarios y de gestión que permitiera añadir y modificar los modelos que estarían disponibles en la aplicación. Otro posible enfoque sería reestructurar la aplicación y usar un \textit{framework} web más completo como puede ser Django. Esto convertiría la herramienta de una simple demostración a una plataforma de análisis educativo y de diagnóstico mucho más completa.

